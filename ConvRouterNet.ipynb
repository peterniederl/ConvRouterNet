{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee411d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 12:51:03.260921: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-10-22 12:51:03.261012: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-10-22 12:51:03.261027: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-10-22 12:51:03.261067: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-10-22 12:51:03.261095: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from utils import (\n",
    "    PoolingLayer, ResidualBlock, ResidualBlock3x3, ResidualBlock5x5, ResidualBlock7x7, SpatialSE, ChannelSE, \n",
    "    ResidualBlockDepthwise3x3, ResidualBlockDepthwise5x5, ResidualBlockDepthwise7x7, ResidualBlockDepthwise9x9, \n",
    "    DummyBlock, Conv3x3PoolingLayer, Depthwise3x3ConvPoolingLayer, MaxPoolingLayer, AvgPoolingLayer,\n",
    "    Conv5x5PoolingLayer, Depthwise5x5ConvPoolingLayer, Depthwise7x7ConvPoolingLayer, AdaptiveRouter,\n",
    "    ConvStem, DepthwiseConvStem, SpatialAttention\n",
    ")\n",
    "from InitConvRouterNet import (\n",
    "    get_stem, get_pooling, get_block, get_init_stem, get_init_block, get_init_pooling\n",
    ")\n",
    "from callbacks import (\n",
    "    CosineAnnealingScheduler, TempScheduler, RouterStatsCallback, TopNScheduler, linear_topn_schedule, milestone_topn_schedule, print_router_stats\n",
    ")\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42 \n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "\n",
    "# simple CIFAR-10 aug\n",
    "def cifar_preprocess(x, y):\n",
    "    x = tf.image.resize_with_crop_or_pad(x, 36, 36)\n",
    "    x = tf.image.random_crop(x, [tf.shape(x)[0], 32, 32, 3])\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    return x, y\n",
    "\n",
    "def make_dataset(x, y, batch=128, train=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    if train:\n",
    "        ds = ds.shuffle(5000).batch(batch).map(cifar_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        ds = ds.batch(batch)\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "def build_conv_router_net(net_layers, num_classes, input_shape=(32,32,3)):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    for layer in net_layers:\n",
    "        x = layer(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # keep fp32 logits\n",
    "    return keras.Model(inputs, outputs, name=\"adaptive_model\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype(\"float32\")/255.0; x_test = x_test.astype(\"float32\")/255.0\n",
    "y_train = y_train.flatten(); y_test = y_test.flatten()\n",
    "\n",
    "\n",
    "ds_train = make_dataset(x_train, y_train, batch=128, train=True)\n",
    "ds_val   = make_dataset(x_test, y_test, batch=256, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd6fea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20 \n",
    "ROUTER_TEMP = 1.5\n",
    "DIVERSITY_TAU = 1e-2\n",
    "TOP_N = 7\n",
    "\n",
    "layers = []\n",
    "layers.append(get_init_stem(16, \"01_stem\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_init_pooling(32, \"02_pool\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_init_block(32, \"03_block\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_init_block(32, \"04_block\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_init_pooling(64, \"05_pool\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_init_block(64, \"06_block\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_init_block(64, \"07_block\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_init_pooling(128, \"08_pool\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_init_block(128, \"09_block\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_init_block(128, \"10_block\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_init_pooling(256, \"11_pool\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "\n",
    "all_layers = []\n",
    "\n",
    "\n",
    "router_model = build_conv_router_net(layers, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26b22510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"adaptive_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " 01_stem (AdaptiveRouter)    (None, 32, 32, 16)        12097     \n",
      "                                                                 \n",
      " 02_pool (AdaptiveRouter)    (None, 16, 16, 32)        28481     \n",
      "                                                                 \n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " 01_stem (AdaptiveRouter)    (None, 32, 32, 16)        12097     \n",
      "                                                                 \n",
      " 02_pool (AdaptiveRouter)    (None, 16, 16, 32)        28481     \n",
      "                                                                 \n",
      " 03_block (AdaptiveRouter)   (None, 16, 16, 32)        53043     \n",
      "                                                                 \n",
      " 04_block (AdaptiveRouter)   (None, 16, 16, 32)        53043     \n",
      "                                                                 \n",
      " 05_pool (AdaptiveRouter)    (None, 8, 8, 64)          75329     \n",
      "                                                                 \n",
      " 06_block (AdaptiveRouter)   (None, 8, 8, 64)          170435    \n",
      "                                                                 \n",
      " 07_block (AdaptiveRouter)   (None, 8, 8, 64)          170435    \n",
      "                                                                 \n",
      " 08_pool (AdaptiveRouter)    (None, 4, 4, 128)         250433    \n",
      " 03_block (AdaptiveRouter)   (None, 16, 16, 32)        53043     \n",
      "                                                                 \n",
      " 04_block (AdaptiveRouter)   (None, 16, 16, 32)        53043     \n",
      "                                                                 \n",
      " 05_pool (AdaptiveRouter)    (None, 8, 8, 64)          75329     \n",
      "                                                                 \n",
      " 06_block (AdaptiveRouter)   (None, 8, 8, 64)          170435    \n",
      "                                                                 \n",
      " 07_block (AdaptiveRouter)   (None, 8, 8, 64)          170435    \n",
      "                                                                 \n",
      " 08_pool (AdaptiveRouter)    (None, 4, 4, 128)         250433    \n",
      "                                                                 \n",
      " 09_block (AdaptiveRouter)   (None, 4, 4, 128)         612579    \n",
      "                                                                 \n",
      " 09_block (AdaptiveRouter)   (None, 4, 4, 128)         612579    \n",
      "                                                                 \n",
      " 10_block (AdaptiveRouter)   (None, 4, 4, 128)         612579    \n",
      "                                                                 \n",
      " 10_block (AdaptiveRouter)   (None, 4, 4, 128)         612579    \n",
      "                                                                 \n",
      " 11_pool (AdaptiveRouter)    (None, 2, 2, 256)         926273    \n",
      "                                                                 \n",
      " 11_pool (AdaptiveRouter)    (None, 2, 2, 256)         926273    \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 256)               0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 256)               0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2967297 (11.32 MB)\n",
      "Trainable params: 2967297 (11.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "=================================================================\n",
      "Total params: 2967297 (11.32 MB)\n",
      "Trainable params: 2967297 (11.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    CosineAnnealingScheduler(base_lr=1e-3, min_lr=1e-5, epochs=EPOCHS),\n",
    "]\n",
    "for layer in layers:\n",
    "    callbacks.append(RouterStatsCallback(x_test, y_test, layer_name=layer.name, verbose_every=10))\n",
    "    callbacks.append(TempScheduler(layer_name=layer.name, epochs=EPOCHS, route=(ROUTER_TEMP, 0.5), eps=(0, 0.00), ent=(0, 0.0), lb=(0, 0.0), verbose=1, log=False))\n",
    "    callbacks.append(TopNScheduler(schedule=linear_topn_schedule(n_start=TOP_N, n_end=1, start_epoch=0, end_epoch=int(EPOCHS * 0.8)),verbose=1))\n",
    "\n",
    "router_model.build(input_shape=(None, 32, 32, 3))\n",
    "router_model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "#AdamW(lr=3e-3, weight_decay=0.05)\n",
    "\n",
    "router_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71937a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [LR Scheduler] epoch 1: lr=0.001000\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 13:37:37.361125: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - ETA: 0s - loss: 1.9105 - accuracy: 0.2906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 13:43:56.275581: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [01_stem] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 0, 0, 10000, 0, 0]]\n",
      "> [02_pool] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 9991, 0, 0, 0, 9, 0]]\n",
      "> [02_pool] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 9991, 0, 0, 0, 9, 0]]\n",
      "> [03_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 0, 0, 0, 0, 10000, 0]]\n",
      "> [03_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 0, 0, 0, 0, 10000, 0]]\n",
      "> [04_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 10000, 0, 0, 0, 0, 0]]\n",
      "> [04_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 10000, 0, 0, 0, 0, 0]]\n",
      "> [05_pool] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 0, 0, 0, 6175, 3825]]\n",
      "> [05_pool] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 0, 0, 0, 6175, 3825]]\n",
      "> [06_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 9295, 0, 0, 0, 705, 0]]\n",
      "> [06_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 9295, 0, 0, 0, 705, 0]]\n",
      "> [07_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 0, 0, 0, 0, 9802, 198]]\n",
      "> [07_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 0, 0, 0, 0, 9802, 198]]\n",
      "> [08_pool] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[1746, 0, 0, 0, 0, 0, 8254]]\n",
      "> [08_pool] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[1746, 0, 0, 0, 0, 0, 8254]]\n",
      "> [09_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 4893, 5107, 0, 0, 0, 0]]\n",
      "> [09_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 4893, 5107, 0, 0, 0, 0]]\n",
      "> [10_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 0, 0, 0, 0, 10000, 0]]\n",
      "> [10_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 0, 0, 0, 0, 10000, 0]]\n",
      "> [11_pool] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 9750, 0, 235, 3, 12, 0]]\n",
      "391/391 [==============================] - 634s 2s/step - loss: 1.9105 - accuracy: 0.2906 - val_loss: 1.4965 - val_accuracy: 0.4510\n",
      "> [11_pool] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 9750, 0, 235, 3, 12, 0]]\n",
      "391/391 [==============================] - 634s 2s/step - loss: 1.9105 - accuracy: 0.2906 - val_loss: 1.4965 - val_accuracy: 0.4510\n",
      "Epoch 2/20\n",
      "Epoch 2/20\n",
      "391/391 [==============================] - 550s 1s/step - loss: 1.3545 - accuracy: 0.5113 - val_loss: 1.1165 - val_accuracy: 0.5985\n",
      "391/391 [==============================] - 550s 1s/step - loss: 1.3545 - accuracy: 0.5113 - val_loss: 1.1165 - val_accuracy: 0.5985\n",
      "[TopNScheduler] epoch 2: 01_stem.top_n=6/7; 02_pool.top_n=6/7; 03_block.top_n=6/8; 04_block.top_n=6/8; 05_pool.top_n=6/7; 06_block.top_n=6/8; 07_block.top_n=6/8; 08_pool.top_n=6/7; 09_block.top_n=6/8; 10_block.top_n=6/8; 11_pool.top_n=6/7\n",
      "[TopNScheduler] epoch 2: 01_stem.top_n=6/7; 02_pool.top_n=6/7; 03_block.top_n=6/8; 04_block.top_n=6/8; 05_pool.top_n=6/7; 06_block.top_n=6/8; 07_block.top_n=6/8; 08_pool.top_n=6/7; 09_block.top_n=6/8; 10_block.top_n=6/8; 11_pool.top_n=6/7\n",
      "Epoch 3/20\n",
      "Epoch 3/20\n",
      "391/391 [==============================] - 549s 1s/step - loss: 1.0599 - accuracy: 0.6229 - val_loss: 0.9326 - val_accuracy: 0.6638\n",
      "391/391 [==============================] - 549s 1s/step - loss: 1.0599 - accuracy: 0.6229 - val_loss: 0.9326 - val_accuracy: 0.6638\n",
      "Epoch 4/20\n",
      "Epoch 4/20\n",
      "391/391 [==============================] - 623s 2s/step - loss: 0.8831 - accuracy: 0.6876 - val_loss: 0.8147 - val_accuracy: 0.7109\n",
      "391/391 [==============================] - 623s 2s/step - loss: 0.8831 - accuracy: 0.6876 - val_loss: 0.8147 - val_accuracy: 0.7109\n",
      "> [LR Scheduler] epoch 5: lr=0.000896\n",
      "> [LR Scheduler] epoch 5: lr=0.000896\n",
      "Epoch 5/20\n",
      "Epoch 5/20\n",
      "391/391 [==============================] - 615s 2s/step - loss: 0.7699 - accuracy: 0.7290 - val_loss: 0.7653 - val_accuracy: 0.7303\n",
      "391/391 [==============================] - 615s 2s/step - loss: 0.7699 - accuracy: 0.7290 - val_loss: 0.7653 - val_accuracy: 0.7303\n",
      "[TopNScheduler] epoch 5: 01_stem.top_n=5/7; 02_pool.top_n=5/7; 03_block.top_n=5/8; 04_block.top_n=5/8; 05_pool.top_n=5/7; 06_block.top_n=5/8; 07_block.top_n=5/8; 08_pool.top_n=5/7; 09_block.top_n=5/8; 10_block.top_n=5/8; 11_pool.top_n=5/7\n",
      "[TopNScheduler] epoch 5: 01_stem.top_n=5/7; 02_pool.top_n=5/7; 03_block.top_n=5/8; 04_block.top_n=5/8; 05_pool.top_n=5/7; 06_block.top_n=5/8; 07_block.top_n=5/8; 08_pool.top_n=5/7; 09_block.top_n=5/8; 10_block.top_n=5/8; 11_pool.top_n=5/7\n",
      "Epoch 6/20\n",
      "Epoch 6/20\n",
      "391/391 [==============================] - 590s 2s/step - loss: 0.6924 - accuracy: 0.7590 - val_loss: 0.6916 - val_accuracy: 0.7591\n",
      "391/391 [==============================] - 590s 2s/step - loss: 0.6924 - accuracy: 0.7590 - val_loss: 0.6916 - val_accuracy: 0.7591\n",
      "Epoch 7/20\n",
      "Epoch 7/20\n",
      "391/391 [==============================] - 617s 2s/step - loss: 0.6198 - accuracy: 0.7809 - val_loss: 0.6441 - val_accuracy: 0.7735\n",
      "391/391 [==============================] - 617s 2s/step - loss: 0.6198 - accuracy: 0.7809 - val_loss: 0.6441 - val_accuracy: 0.7735\n",
      "[TopNScheduler] epoch 7: 01_stem.top_n=4/7; 02_pool.top_n=4/7; 03_block.top_n=4/8; 04_block.top_n=4/8; 05_pool.top_n=4/7; 06_block.top_n=4/8; 07_block.top_n=4/8; 08_pool.top_n=4/7; 09_block.top_n=4/8; 10_block.top_n=4/8; 11_pool.top_n=4/7\n",
      "[TopNScheduler] epoch 7: 01_stem.top_n=4/7; 02_pool.top_n=4/7; 03_block.top_n=4/8; 04_block.top_n=4/8; 05_pool.top_n=4/7; 06_block.top_n=4/8; 07_block.top_n=4/8; 08_pool.top_n=4/7; 09_block.top_n=4/8; 10_block.top_n=4/8; 11_pool.top_n=4/7\n",
      "Epoch 8/20\n",
      "Epoch 8/20\n",
      "391/391 [==============================] - 639s 2s/step - loss: 0.5588 - accuracy: 0.8062 - val_loss: 0.6741 - val_accuracy: 0.7666\n",
      "391/391 [==============================] - 639s 2s/step - loss: 0.5588 - accuracy: 0.8062 - val_loss: 0.6741 - val_accuracy: 0.7666\n",
      "Epoch 9/20\n",
      "Epoch 9/20\n",
      "391/391 [==============================] - 550s 1s/step - loss: 0.5104 - accuracy: 0.8223 - val_loss: 0.6187 - val_accuracy: 0.7875\n",
      "391/391 [==============================] - 550s 1s/step - loss: 0.5104 - accuracy: 0.8223 - val_loss: 0.6187 - val_accuracy: 0.7875\n",
      "> [LR Scheduler] epoch 10: lr=0.000546\n",
      "> [LR Scheduler] epoch 10: lr=0.000546\n",
      "Epoch 10/20\n",
      "Epoch 10/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4682 - accuracy: 0.8362> [01_stem] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 0, 0, 10000, 0, 0]]\n",
      "> [01_stem] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 0, 0, 10000, 0, 0]]\n",
      "> [02_pool] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[8113, 1887, 0, 0, 0, 0, 0]]\n",
      "> [02_pool] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[8113, 1887, 0, 0, 0, 0, 0]]\n",
      "> [03_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 0, 0, 0, 0, 10000, 0]]\n",
      "> [03_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 0, 0, 0, 0, 10000, 0]]\n",
      "> [04_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 10000, 0, 0, 0, 0, 0]]\n",
      "> [04_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 10000, 0, 0, 0, 0, 0]]\n",
      "> [05_pool] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 174, 0, 0, 0, 7043, 2783]]\n",
      "> [05_pool] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 174, 0, 0, 0, 7043, 2783]]\n",
      "> [06_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[1, 0, 6631, 0, 0, 0, 3368, 0]]\n",
      "> [06_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[1, 0, 6631, 0, 0, 0, 3368, 0]]\n",
      "> [07_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 49, 0, 0, 0, 9950, 1]]\n",
      "> [07_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 49, 0, 0, 0, 9950, 1]]\n",
      "> [08_pool] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[680, 4289, 0, 0, 0, 0, 5031]]\n",
      "> [08_pool] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[680, 4289, 0, 0, 0, 0, 5031]]\n",
      "> [09_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 1398, 8602, 0, 0, 0, 0]]\n",
      "> [09_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 1398, 8602, 0, 0, 0, 0]]\n",
      "> [10_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 1, 0, 0, 0, 0, 9999, 0]]\n",
      "> [10_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 1, 0, 0, 0, 0, 9999, 0]]\n",
      "> [11_pool] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[7, 9993, 0, 0, 0, 0, 0]]\n",
      "391/391 [==============================] - 655s 2s/step - loss: 0.4682 - accuracy: 0.8362 - val_loss: 0.5802 - val_accuracy: 0.8023\n",
      "> [11_pool] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[7, 9993, 0, 0, 0, 0, 0]]\n",
      "391/391 [==============================] - 655s 2s/step - loss: 0.4682 - accuracy: 0.8362 - val_loss: 0.5802 - val_accuracy: 0.8023\n",
      "[TopNScheduler] epoch 10: 01_stem.top_n=3/7; 02_pool.top_n=3/7; 03_block.top_n=3/8; 04_block.top_n=3/8; 05_pool.top_n=3/7; 06_block.top_n=3/8; 07_block.top_n=3/8; 08_pool.top_n=3/7; 09_block.top_n=3/8; 10_block.top_n=3/8; 11_pool.top_n=3/7\n",
      "[TopNScheduler] epoch 10: 01_stem.top_n=3/7; 02_pool.top_n=3/7; 03_block.top_n=3/8; 04_block.top_n=3/8; 05_pool.top_n=3/7; 06_block.top_n=3/8; 07_block.top_n=3/8; 08_pool.top_n=3/7; 09_block.top_n=3/8; 10_block.top_n=3/8; 11_pool.top_n=3/7\n",
      "Epoch 11/20\n",
      "Epoch 11/20\n",
      "391/391 [==============================] - 486s 1s/step - loss: 0.4250 - accuracy: 0.8525 - val_loss: 0.5717 - val_accuracy: 0.8083\n",
      "391/391 [==============================] - 486s 1s/step - loss: 0.4250 - accuracy: 0.8525 - val_loss: 0.5717 - val_accuracy: 0.8083\n",
      "Epoch 12/20\n",
      "Epoch 12/20\n",
      "391/391 [==============================] - 546s 1s/step - loss: 0.3872 - accuracy: 0.8670 - val_loss: 0.5590 - val_accuracy: 0.8147\n",
      "391/391 [==============================] - 546s 1s/step - loss: 0.3872 - accuracy: 0.8670 - val_loss: 0.5590 - val_accuracy: 0.8147\n",
      "[TopNScheduler] epoch 12: 01_stem.top_n=2/7; 02_pool.top_n=2/7; 03_block.top_n=2/8; 04_block.top_n=2/8; 05_pool.top_n=2/7; 06_block.top_n=2/8; 07_block.top_n=2/8; 08_pool.top_n=2/7; 09_block.top_n=2/8; 10_block.top_n=2/8; 11_pool.top_n=2/7\n",
      "[TopNScheduler] epoch 12: 01_stem.top_n=2/7; 02_pool.top_n=2/7; 03_block.top_n=2/8; 04_block.top_n=2/8; 05_pool.top_n=2/7; 06_block.top_n=2/8; 07_block.top_n=2/8; 08_pool.top_n=2/7; 09_block.top_n=2/8; 10_block.top_n=2/8; 11_pool.top_n=2/7\n",
      "Epoch 13/20\n",
      "Epoch 13/20\n",
      "391/391 [==============================] - 557s 1s/step - loss: 0.3509 - accuracy: 0.8802 - val_loss: 0.5499 - val_accuracy: 0.8193\n",
      "391/391 [==============================] - 557s 1s/step - loss: 0.3509 - accuracy: 0.8802 - val_loss: 0.5499 - val_accuracy: 0.8193\n",
      "Epoch 14/20\n",
      "Epoch 14/20\n",
      "391/391 [==============================] - 520s 1s/step - loss: 0.3195 - accuracy: 0.8913 - val_loss: 0.5527 - val_accuracy: 0.8181\n",
      "391/391 [==============================] - 520s 1s/step - loss: 0.3195 - accuracy: 0.8913 - val_loss: 0.5527 - val_accuracy: 0.8181\n",
      "> [LR Scheduler] epoch 15: lr=0.000170\n",
      "> [LR Scheduler] epoch 15: lr=0.000170\n",
      "Epoch 15/20\n",
      "Epoch 15/20\n",
      "391/391 [==============================] - 537s 1s/step - loss: 0.2943 - accuracy: 0.9008 - val_loss: 0.5469 - val_accuracy: 0.8215\n",
      "391/391 [==============================] - 537s 1s/step - loss: 0.2943 - accuracy: 0.9008 - val_loss: 0.5469 - val_accuracy: 0.8215\n",
      "[TopNScheduler] epoch 15: 01_stem.top_n=1/7; 02_pool.top_n=1/7; 03_block.top_n=1/8; 04_block.top_n=1/8; 05_pool.top_n=1/7; 06_block.top_n=1/8; 07_block.top_n=1/8; 08_pool.top_n=1/7; 09_block.top_n=1/8; 10_block.top_n=1/8; 11_pool.top_n=1/7\n",
      "[TopNScheduler] epoch 15: 01_stem.top_n=1/7; 02_pool.top_n=1/7; 03_block.top_n=1/8; 04_block.top_n=1/8; 05_pool.top_n=1/7; 06_block.top_n=1/8; 07_block.top_n=1/8; 08_pool.top_n=1/7; 09_block.top_n=1/8; 10_block.top_n=1/8; 11_pool.top_n=1/7\n",
      "Epoch 16/20\n",
      "Epoch 16/20\n",
      "391/391 [==============================] - 587s 2s/step - loss: 0.2717 - accuracy: 0.9101 - val_loss: 0.5486 - val_accuracy: 0.8213\n",
      "391/391 [==============================] - 587s 2s/step - loss: 0.2717 - accuracy: 0.9101 - val_loss: 0.5486 - val_accuracy: 0.8213\n",
      "Epoch 17/20\n",
      "Epoch 17/20\n",
      "391/391 [==============================] - 520s 1s/step - loss: 0.2584 - accuracy: 0.9138 - val_loss: 0.5455 - val_accuracy: 0.8207\n",
      "391/391 [==============================] - 520s 1s/step - loss: 0.2584 - accuracy: 0.9138 - val_loss: 0.5455 - val_accuracy: 0.8207\n",
      "Epoch 18/20\n",
      "Epoch 18/20\n",
      "391/391 [==============================] - 479s 1s/step - loss: 0.2483 - accuracy: 0.9187 - val_loss: 0.5438 - val_accuracy: 0.8258\n",
      "391/391 [==============================] - 479s 1s/step - loss: 0.2483 - accuracy: 0.9187 - val_loss: 0.5438 - val_accuracy: 0.8258\n",
      "Epoch 19/20\n",
      "Epoch 19/20\n",
      "391/391 [==============================] - 513s 1s/step - loss: 0.2359 - accuracy: 0.9229 - val_loss: 0.5422 - val_accuracy: 0.8264\n",
      "391/391 [==============================] - 513s 1s/step - loss: 0.2359 - accuracy: 0.9229 - val_loss: 0.5422 - val_accuracy: 0.8264\n",
      "> [LR Scheduler] epoch 20: lr=0.000010\n",
      "> [LR Scheduler] epoch 20: lr=0.000010\n",
      "Epoch 20/20\n",
      "Epoch 20/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2329 - accuracy: 0.9231> [01_stem] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 0, 0, 10000, 0, 0]]\n",
      "> [01_stem] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 0, 0, 10000, 0, 0]]\n",
      "> [02_pool] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[1356, 8485, 0, 0, 3, 156, 0]]\n",
      "> [02_pool] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[1356, 8485, 0, 0, 3, 156, 0]]\n",
      "> [03_block] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 0, 0, 0, 0, 10000, 0]]\n",
      "> [03_block] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 0, 0, 0, 0, 10000, 0]]\n",
      "> [04_block] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 9839, 1, 0, 3, 0, 157]]\n",
      "> [04_block] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 9839, 1, 0, 3, 0, 157]]\n",
      "> [05_pool] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[760, 9212, 0, 0, 0, 8, 20]]\n",
      "> [05_pool] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[760, 9212, 0, 0, 0, 8, 20]]\n",
      "> [06_block] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[2007, 0, 3018, 0, 0, 0, 4975, 0]]\n",
      "> [06_block] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[2007, 0, 3018, 0, 0, 0, 4975, 0]]\n",
      "> [07_block] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[171, 738, 102, 6, 149, 3, 8578, 253]]\n",
      "> [07_block] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[171, 738, 102, 6, 149, 3, 8578, 253]]\n",
      "> [08_pool] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[250, 2193, 178, 16, 118, 0, 7245]]\n",
      "> [08_pool] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[250, 2193, 178, 16, 118, 0, 7245]]\n",
      "> [09_block] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[88, 25, 7118, 2753, 0, 0, 0, 16]]\n",
      "> [09_block] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[88, 25, 7118, 2753, 0, 0, 0, 16]]\n",
      "> [10_block] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[15, 1, 2, 12, 0, 4, 9965, 1]]\n",
      "> [10_block] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[15, 1, 2, 12, 0, 4, 9965, 1]]\n",
      "> [11_pool] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[82, 9918, 0, 0, 0, 0, 0]]\n",
      "391/391 [==============================] - 805s 2s/step - loss: 0.2329 - accuracy: 0.9231 - val_loss: 0.5407 - val_accuracy: 0.8280\n",
      "> [11_pool] epoch 20: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[82, 9918, 0, 0, 0, 0, 0]]\n",
      "391/391 [==============================] - 805s 2s/step - loss: 0.2329 - accuracy: 0.9231 - val_loss: 0.5407 - val_accuracy: 0.8280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3edcc4820>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_model.fit(ds_train, epochs=EPOCHS, validation_data=ds_val, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d45a651",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for layer in layers:\n",
    "    print_router_stats(router_model, x_test, y_test, layer_name=layer.name, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "948195ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150 \n",
    "ROUTER_TEMP = 1.5\n",
    "DIVERSITY_TAU = 1e-2\n",
    "TOP_N = 3\n",
    "\n",
    "layers = []\n",
    "layers.append(get_stem([DepthwiseConvStem(16, 5)], \"01_stem\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_pooling([Conv3x3PoolingLayer(32), Conv5x5PoolingLayer(32)], \"02_pool\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_block([SpatialSE()], \"03_block\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_block([ResidualBlockDepthwise3x3(32), SpatialAttention(kernel_size=7)], \"04_block\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_pooling([Conv3x3PoolingLayer(64), Conv5x5PoolingLayer(64)], \"05_pool\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_block([ResidualBlock3x3(64), ResidualBlockDepthwise3x3(64), SpatialSE()], \"06_block\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_block([ResidualBlock5x5(64), SpatialSE(), SpatialAttention(kernel_size=7)], \"07_block\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_pooling([Conv5x5PoolingLayer(128), AvgPoolingLayer(128)], \"08_pool\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_block([ResidualBlockDepthwise3x3(128), ResidualBlockDepthwise5x5(128)], \"09_block\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_block([SpatialSE()], \"10_block\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "layers.append(get_pooling([Conv5x5PoolingLayer(256)], \"11_pool\", TOP_N, ROUTER_TEMP, DIVERSITY_TAU))\n",
    "\n",
    "all_layers = []\n",
    "\n",
    "\n",
    "router_model = build_conv_router_net(layers, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a768b3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"adaptive_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " 01_stem (AdaptiveRouter)    (None, 32, 32, 16)        5161      \n",
      "                                                                 \n",
      " 02_pool (AdaptiveRouter)    (None, 16, 16, 32)        30145     \n",
      "                                                                 \n",
      " 03_block (AdaptiveRouter)   (None, 16, 16, 32)        11457     \n",
      "                                                                 \n",
      " 04_block (AdaptiveRouter)   (None, 16, 16, 32)        11363     \n",
      "                                                                 \n",
      " 05_pool (AdaptiveRouter)    (None, 8, 8, 64)          86657     \n",
      "                                                                 \n",
      " 06_block (AdaptiveRouter)   (None, 8, 8, 64)          111105    \n",
      "                                                                 \n",
      " 07_block (AdaptiveRouter)   (None, 8, 8, 64)          74499     \n",
      "                                                                 \n",
      " 08_pool (AdaptiveRouter)    (None, 4, 4, 128)         238593    \n",
      "                                                                 \n",
      " 01_stem (AdaptiveRouter)    (None, 32, 32, 16)        5161      \n",
      "                                                                 \n",
      " 02_pool (AdaptiveRouter)    (None, 16, 16, 32)        30145     \n",
      "                                                                 \n",
      " 03_block (AdaptiveRouter)   (None, 16, 16, 32)        11457     \n",
      "                                                                 \n",
      " 04_block (AdaptiveRouter)   (None, 16, 16, 32)        11363     \n",
      "                                                                 \n",
      " 05_pool (AdaptiveRouter)    (None, 8, 8, 64)          86657     \n",
      "                                                                 \n",
      " 06_block (AdaptiveRouter)   (None, 8, 8, 64)          111105    \n",
      "                                                                 \n",
      " 07_block (AdaptiveRouter)   (None, 8, 8, 64)          74499     \n",
      "                                                                 \n",
      " 08_pool (AdaptiveRouter)    (None, 4, 4, 128)         238593    \n",
      "                                                                 \n",
      " 09_block (AdaptiveRouter)   (None, 4, 4, 128)         96961     \n",
      "                                                                 \n",
      " 10_block (AdaptiveRouter)   (None, 4, 4, 128)         33153     \n",
      "                                                                 \n",
      " 11_pool (AdaptiveRouter)    (None, 2, 2, 256)         861121    \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 256)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      " 09_block (AdaptiveRouter)   (None, 4, 4, 128)         96961     \n",
      "                                                                 \n",
      " 10_block (AdaptiveRouter)   (None, 4, 4, 128)         33153     \n",
      "                                                                 \n",
      " 11_pool (AdaptiveRouter)    (None, 2, 2, 256)         861121    \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 256)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1562785 (5.96 MB)\n",
      "Trainable params: 1562785 (5.96 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Total params: 1562785 (5.96 MB)\n",
      "Trainable params: 1562785 (5.96 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    CosineAnnealingScheduler(base_lr=1e-3, min_lr=1e-5, epochs=EPOCHS),\n",
    "]\n",
    "for layer in layers:\n",
    "    callbacks.append(RouterStatsCallback(x_test, y_test, layer_name=layer.name, verbose_every=10))\n",
    "    callbacks.append(TempScheduler(layer_name=layer.name, epochs=EPOCHS, route=(ROUTER_TEMP, 0.5), eps=(0, 0.00), ent=(0, 0.0), lb=(0, 0.0), verbose=1, log=False))\n",
    "    callbacks.append(TopNScheduler(schedule=linear_topn_schedule(n_start=TOP_N, n_end=1, start_epoch=0, end_epoch=int(EPOCHS * 0.8)),verbose=1))\n",
    "\n",
    "router_model.build(input_shape=(None, 32, 32, 3))\n",
    "router_model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "#AdamW(lr=3e-3, weight_decay=0.05)\n",
    "\n",
    "router_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbc27d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [LR Scheduler] epoch 1: lr=0.001000\n",
      "[TopNScheduler] epoch 0: 01_stem.top_n=1/1; 02_pool.top_n=2/2; 03_block.top_n=1/1; 04_block.top_n=2/2; 05_pool.top_n=2/2; 08_pool.top_n=2/2; 09_block.top_n=2/2; 10_block.top_n=1/1; 11_pool.top_n=1/1\n",
      "Epoch 1/150\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 12:51:26.740866: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - ETA: 0s - loss: 1.8089 - accuracy: 0.3260"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 12:53:06.064393: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [01_stem] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[10000]]\n",
      "> [02_pool] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[5042, 4958]]\n",
      "> [02_pool] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[5042, 4958]]\n",
      "> [03_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[10000]]\n",
      "> [03_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[10000]]\n",
      "> [04_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[9517, 483]]\n",
      "> [04_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[9517, 483]]\n",
      "> [05_pool] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[10000, 0]]\n",
      "> [05_pool] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[10000, 0]]\n",
      "> [06_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[1, 0, 9999]]\n",
      "> [06_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[1, 0, 9999]]\n",
      "> [07_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 10000, 0]]\n",
      "> [07_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 10000, 0]]\n",
      "> [08_pool] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[768, 9232]]\n",
      "> [08_pool] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[768, 9232]]\n",
      "> [09_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[8111, 1889]]\n",
      "> [09_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[8111, 1889]]\n",
      "> [10_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[10000]]\n",
      "> [10_block] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[10000]]\n",
      "> [11_pool] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[10000]]\n",
      "391/391 [==============================] - 190s 459ms/step - loss: 1.8089 - accuracy: 0.3260 - val_loss: 1.3361 - val_accuracy: 0.5109\n",
      "> [11_pool] epoch 1: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[10000]]\n",
      "391/391 [==============================] - 190s 459ms/step - loss: 1.8089 - accuracy: 0.3260 - val_loss: 1.3361 - val_accuracy: 0.5109\n",
      "Epoch 2/150\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 88s 223ms/step - loss: 1.1696 - accuracy: 0.5831 - val_loss: 1.0349 - val_accuracy: 0.6318\n",
      "391/391 [==============================] - 88s 223ms/step - loss: 1.1696 - accuracy: 0.5831 - val_loss: 1.0349 - val_accuracy: 0.6318\n",
      "Epoch 3/150\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 88s 225ms/step - loss: 0.9514 - accuracy: 0.6658 - val_loss: 0.9099 - val_accuracy: 0.6804\n",
      "391/391 [==============================] - 88s 225ms/step - loss: 0.9514 - accuracy: 0.6658 - val_loss: 0.9099 - val_accuracy: 0.6804\n",
      "Epoch 4/150\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 88s 224ms/step - loss: 0.8256 - accuracy: 0.7106 - val_loss: 0.7549 - val_accuracy: 0.7342\n",
      "391/391 [==============================] - 88s 224ms/step - loss: 0.8256 - accuracy: 0.7106 - val_loss: 0.7549 - val_accuracy: 0.7342\n",
      "> [LR Scheduler] epoch 5: lr=0.000998\n",
      "> [LR Scheduler] epoch 5: lr=0.000998\n",
      "Epoch 5/150\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 94s 240ms/step - loss: 0.7293 - accuracy: 0.7440 - val_loss: 0.7082 - val_accuracy: 0.7539\n",
      "391/391 [==============================] - 94s 240ms/step - loss: 0.7293 - accuracy: 0.7440 - val_loss: 0.7082 - val_accuracy: 0.7539\n",
      "Epoch 6/150\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 91s 233ms/step - loss: 0.6671 - accuracy: 0.7684 - val_loss: 0.6780 - val_accuracy: 0.7664\n",
      "391/391 [==============================] - 91s 233ms/step - loss: 0.6671 - accuracy: 0.7684 - val_loss: 0.6780 - val_accuracy: 0.7664\n",
      "Epoch 7/150\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 92s 233ms/step - loss: 0.6029 - accuracy: 0.7903 - val_loss: 0.6574 - val_accuracy: 0.7755\n",
      "391/391 [==============================] - 92s 233ms/step - loss: 0.6029 - accuracy: 0.7903 - val_loss: 0.6574 - val_accuracy: 0.7755\n",
      "Epoch 8/150\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 91s 232ms/step - loss: 0.5564 - accuracy: 0.8061 - val_loss: 0.6547 - val_accuracy: 0.7750\n",
      "391/391 [==============================] - 91s 232ms/step - loss: 0.5564 - accuracy: 0.8061 - val_loss: 0.6547 - val_accuracy: 0.7750\n",
      "Epoch 9/150\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 91s 232ms/step - loss: 0.5274 - accuracy: 0.8163 - val_loss: 0.6014 - val_accuracy: 0.7923\n",
      "391/391 [==============================] - 91s 232ms/step - loss: 0.5274 - accuracy: 0.8163 - val_loss: 0.6014 - val_accuracy: 0.7923\n",
      "> [LR Scheduler] epoch 10: lr=0.000991\n",
      "> [LR Scheduler] epoch 10: lr=0.000991\n",
      "Epoch 10/150\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4882 - accuracy: 0.8313> [01_stem] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[10000]]\n",
      "> [01_stem] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[10000]]\n",
      "> [02_pool] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[9, 9991]]\n",
      "> [02_pool] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[9, 9991]]\n",
      "> [03_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[10000]]\n",
      "> [03_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[10000]]\n",
      "> [04_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[830, 9170]]\n",
      "> [04_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[830, 9170]]\n",
      "> [05_pool] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[9617, 383]]\n",
      "> [05_pool] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[9617, 383]]\n",
      "> [06_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 10000]]\n",
      "> [06_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 0, 10000]]\n",
      "> [07_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 10000, 0]]\n",
      "> [07_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[0, 10000, 0]]\n",
      "> [08_pool] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[6776, 3224]]\n",
      "> [08_pool] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[6776, 3224]]\n",
      "> [09_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[9950, 50]]\n",
      "> [09_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[9950, 50]]\n",
      "> [10_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[10000]]\n",
      "> [10_block] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[10000]]\n",
      "> [11_pool] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[10000]]\n",
      "391/391 [==============================] - 168s 430ms/step - loss: 0.4882 - accuracy: 0.8313 - val_loss: 0.6080 - val_accuracy: 0.7949\n",
      "> [11_pool] epoch 10: avg_steps=1.00  steps_hist=[0, 10000]  expert_hist=[[10000]]\n",
      "391/391 [==============================] - 168s 430ms/step - loss: 0.4882 - accuracy: 0.8313 - val_loss: 0.6080 - val_accuracy: 0.7949\n",
      "Epoch 11/150\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 93s 236ms/step - loss: 0.4672 - accuracy: 0.8396 - val_loss: 0.5630 - val_accuracy: 0.8069\n",
      "391/391 [==============================] - 93s 236ms/step - loss: 0.4672 - accuracy: 0.8396 - val_loss: 0.5630 - val_accuracy: 0.8069\n",
      "Epoch 12/150\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.4245 - accuracy: 0.8534 - val_loss: 0.5942 - val_accuracy: 0.8052\n",
      "391/391 [==============================] - 90s 229ms/step - loss: 0.4245 - accuracy: 0.8534 - val_loss: 0.5942 - val_accuracy: 0.8052\n",
      "Epoch 13/150\n",
      "Epoch 13/150\n",
      "387/391 [============================>.] - ETA: 0s - loss: 0.3893 - accuracy: 0.8653"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrouter_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "router_model.fit(ds_train, epochs=EPOCHS, validation_data=ds_val, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c458e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CGANs - Conditional Generative Adversarial Nets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
      "  METAL, no compute capability (probably not an Nvidia GPU)\n",
      "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, Dense, GlobalAveragePooling2D, LayerNormalization, Add, DepthwiseConv2D, MaxPool2D\n",
    ")\n",
    "from utils import PoolingLayer, ResidualBlock, ResidualBlock3x3, ResidualBlock5x5, ResidualBlock7x7, SpatialSE, ChannelSE, ResidualBlockDepthwise3x3, ResidualBlockDepthwise5x5, ResidualBlockDepthwise7x7\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Softmax Router (no Gumbel). Optional hard mode at inference.\n",
    "# ---------------------------------------------------------\n",
    "class SoftmaxRouter(layers.Layer):\n",
    "    def __init__(self, num_choices, hard_at_inference=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_choices = num_choices\n",
    "        self.hard_at_inference = hard_at_inference\n",
    "        self.logits_layer = Dense(num_choices)\n",
    "\n",
    "    def call(self, features, training=None):\n",
    "        logits = self.logits_layer(GlobalAveragePooling2D()(features))  # (B, K)\n",
    "        if training or not self.hard_at_inference:\n",
    "            probs = tf.nn.softmax(logits, axis=-1)                      # (B, K)\n",
    "        else:\n",
    "            idx = tf.argmax(logits, axis=-1)\n",
    "            probs = tf.one_hot(idx, depth=self.num_choices, dtype=tf.float32)\n",
    "        return probs  # (B, K)\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Hazard-rate Halting Head (ST hard gate; good gradients)\n",
    "# ------------------------------------------------------------\n",
    "class HazardHaltingHead(layers.Layer):\n",
    "    \"\"\"\n",
    "    Predicts a hazard p_t in (0,1) from features; uses a straight-through\n",
    "    hard decision in forward, with gradients from the sigmoid (Concrete).\n",
    "    call(...) returns:\n",
    "      p_soft: [B,1]  (prob of halting at this step, for gradients/metrics)\n",
    "      h_st  : [B,1]  (hard 0/1 gate with ST gradient)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden=64, halt_temp=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = int(hidden)\n",
    "        self.halt_temp = float(halt_temp)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input can be [B,H,W,C] or [B,C]; we GAP if rank-4\n",
    "        if len(input_shape) == 4:\n",
    "            in_dim = int(input_shape[-1])\n",
    "        else:\n",
    "            in_dim = int(input_shape[-1])\n",
    "        if self.hidden > 0:\n",
    "            self.mlp = keras.Sequential([\n",
    "                layers.Dense(self.hidden, activation=\"swish\"),\n",
    "                layers.Dense(1),\n",
    "            ])\n",
    "        else:\n",
    "            self.mlp = layers.Dense(1) #, bias_initializer=tf.keras.initializers.Constant(-0.2)\n",
    "\n",
    "    def call(self, feat, *, can_halt_mask, training=None):\n",
    "        # feat: [B,H,W,C] or [B,C]\n",
    "        if len(feat.shape) == 4:\n",
    "            pooled = tf.reduce_mean(feat, axis=[1,2])  # [B,C]\n",
    "        else:\n",
    "            pooled = feat                               # [B,C]\n",
    "\n",
    "        logits = self.mlp(pooled, training=training)    # [B,1]\n",
    "        p_soft = tf.nn.sigmoid(logits / self.halt_temp) # [B,1]\n",
    "        p_soft = p_soft * can_halt_mask                 # respect min_steps\n",
    "\n",
    "        # straight-through hard gate\n",
    "        h_hard = tf.cast(p_soft > 0.5, p_soft.dtype)    # [B,1]\n",
    "        h_st   = h_hard + tf.stop_gradient(p_soft - h_hard)\n",
    "        return p_soft, h_st\n",
    "\n",
    "# ---------------------------\n",
    "# Tiny conv stem\n",
    "# ---------------------------\n",
    "class ConvStem(layers.Layer):\n",
    "    def __init__(self, out_ch, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.conv = layers.Conv2D(out_ch, 3, padding=\"same\", use_bias=False)\n",
    "        self.norm = layers.LayerNormalization()\n",
    "        self.act  = layers.Activation(\"swish\")\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x, training=training)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Multi-head attention pooling router\n",
    "# Produces logits over K experts\n",
    "# ---------------------------\n",
    "class AttnPoolRouter(layers.Layer):\n",
    "    def __init__(self, K, heads=2, dim_head=64, mlp_hidden=64, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.K = int(K)\n",
    "        self.heads = int(heads)\n",
    "        self.dim_head = int(dim_head)\n",
    "        self.mlp_hidden = int(mlp_hidden)\n",
    "\n",
    "        self.q = self.add_weight(\n",
    "            name=\"queries\", shape=(self.heads, self.dim_head),\n",
    "            initializer=\"glorot_uniform\", trainable=True)\n",
    "\n",
    "        self.key_proj = layers.Conv2D(self.heads*self.dim_head, 1, use_bias=False)\n",
    "        self.val_proj = layers.Conv2D(self.heads*self.dim_head, 1, use_bias=False)\n",
    "\n",
    "        # Head aggregator -> K logits\n",
    "        if self.mlp_hidden > 0:\n",
    "            self.head_mlp = keras.Sequential([\n",
    "                layers.Dense(self.mlp_hidden, activation=\"swish\"),\n",
    "                layers.Dense(self.K)\n",
    "            ])\n",
    "        else:\n",
    "            self.head_mlp = layers.Dense(self.K)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        # x: [B,H,W,C]\n",
    "        B, H, W, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "        k = self.key_proj(x)  # [B,H,W,heads*dim]\n",
    "        v = self.val_proj(x)\n",
    "        k = tf.reshape(k, [B, H*W, self.heads, self.dim_head])  # [B,HW,Hd,D]\n",
    "        v = tf.reshape(v, [B, H*W, self.heads, self.dim_head])  # [B,HW,Hd,D]\n",
    "        k = tf.transpose(k, [0,2,1,3])  # [B,heads,HW,dim]\n",
    "        v = tf.transpose(v, [0,2,1,3])  # [B,heads,HW,dim]\n",
    "\n",
    "        # queries: [heads, dim] -> [B,heads,1,dim]\n",
    "        q = tf.expand_dims(self.q, axis=0)\n",
    "        q = tf.expand_dims(q, axis=2)\n",
    "\n",
    "        # attn: [B,heads,1,HW]\n",
    "        attn = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(self.dim_head, x.dtype))\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "\n",
    "        # pooled heads: [B,heads,1,dim]\n",
    "        pooled = tf.matmul(attn, v)  # [B,heads,1,dim]\n",
    "        pooled = tf.squeeze(pooled, axis=2)  # [B,heads,dim]\n",
    "\n",
    "        # flatten heads\n",
    "        pooled = tf.reshape(pooled, [B, self.heads*self.dim_head])  # [B, heads*dim]\n",
    "\n",
    "        logits = self.head_mlp(pooled, training=training)  # [B,K]\n",
    "        return logits, pooled  # pooled can be used as a feature if needed\n",
    "\n",
    "\n",
    "class HaltingClassifierHead(layers.Layer):\n",
    "    \"\"\"\n",
    "    Predicts class probabilities; halts when max prob > tau.\n",
    "    ST gating: forward uses hard threshold; backward uses a smooth sigmoid around tau.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, hidden=64, halt_temp=3.0, tau=0.8, bias_init=-2.5, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.num_classes = int(num_classes)\n",
    "        self.hidden = int(hidden)\n",
    "        self.halt_temp = float(halt_temp)\n",
    "        self.tau = float(tau)\n",
    "\n",
    "        if hidden > 0:\n",
    "            self.classifier = keras.Sequential([\n",
    "                layers.GlobalAveragePooling2D(),\n",
    "                layers.Dense(hidden, activation=\"swish\"),\n",
    "                layers.Dense(num_classes)\n",
    "            ])\n",
    "        else:\n",
    "            self.classifier = keras.Sequential([\n",
    "                layers.GlobalAveragePooling2D(),\n",
    "                layers.Dense(num_classes)\n",
    "            ])\n",
    "\n",
    "        # a tiny scalar bias we add to (max_prob - tau) before the sigmoid\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"halt_bias\", shape=(), initializer=tf.keras.initializers.Constant(bias_init),\n",
    "            trainable=True)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        logits = self.classifier(x, training=training)          # [B,C]\n",
    "        probs  = tf.nn.softmax(logits, axis=-1)                 # [B,C]\n",
    "        maxp   = tf.reduce_max(probs, axis=-1, keepdims=True)   # [B,1]\n",
    "        tau = tf.cast(self.tau, maxp.dtype)\n",
    "        halt_temp = tf.cast(self.halt_temp, maxp.dtype)\n",
    "        z = (maxp - tau) / tf.maximum(tf.constant(1e-6, dtype=maxp.dtype), halt_temp)\n",
    "        p_soft = tf.nn.sigmoid(z + tf.cast(self.bias, maxp.dtype))                   # [B,1]\n",
    "        p_hard = tf.cast(maxp > tau, x.dtype)              # [B,1]\n",
    "        p_st = p_hard + tf.stop_gradient(p_soft - p_hard)\n",
    "        return probs, p_soft, p_hard, p_st   # class probs, soft gate, hard gate, ST gate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 19:02:54.358155: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-10-14 19:02:54.358187: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-10-14 19:02:54.358194: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-10-14 19:02:54.358225: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-10-14 19:02:54.358239: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# simple CIFAR-10 aug\n",
    "def cifar_preprocess(x, y):\n",
    "    x = tf.image.resize_with_crop_or_pad(x, 36, 36)\n",
    "    x = tf.image.random_crop(x, [tf.shape(x)[0], 32, 32, 3])\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    return x, y\n",
    "\n",
    "def make_dataset(x, y, batch=128, train=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    if train:\n",
    "        ds = ds.shuffle(5000).batch(batch).map(cifar_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        ds = ds.batch(batch)\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Usage\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype(\"float32\")/255.0; x_test = x_test.astype(\"float32\")/255.0\n",
    "y_train = y_train.flatten(); y_test = y_test.flatten()\n",
    "\n",
    "\n",
    "ds_train = make_dataset(x_train, y_train, batch=128, train=True)\n",
    "ds_val   = make_dataset(x_test, y_test, batch=256, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRouterBlockTop1Vectorized(layers.Layer):\n",
    "    \"\"\"\n",
    "    Deterministic Top-1 routing (same in train & infer) with ST grads.\n",
    "    Pooling is disabled: no pooling layers or events are used.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 branches,\n",
    "                 min_steps=1,\n",
    "                 max_steps=5,\n",
    "                 ponder_lambda=0.0,\n",
    "                 route_temp=1.0,\n",
    "                 halt_temp=1.0,\n",
    "                 name=None):\n",
    "        super().__init__(name=name)\n",
    "        assert 1 <= min_steps <= max_steps\n",
    "        self.branches = branches\n",
    "        self.K = len(branches)\n",
    "        self.router = AttnPoolRouter(K=self.K, dim_head=64, mlp_hidden=0)\n",
    "        self.halt   = HazardHaltingHead(hidden=64, halt_temp=float(halt_temp))\n",
    "        self._route_temp = float(route_temp)\n",
    "        self.ponder_lambda = float(ponder_lambda)\n",
    "        self.min_steps = int(min_steps)\n",
    "        self.max_steps = int(max_steps)\n",
    "    @property\n",
    "    def route_temp(self):\n",
    "        return self._route_temp\n",
    "    @route_temp.setter\n",
    "    def route_temp(self, v: float):\n",
    "        self._route_temp = float(v)\n",
    "    @property\n",
    "    def halt_temp(self):\n",
    "        return self.halt.halt_temp\n",
    "    @halt_temp.setter\n",
    "    def halt_temp(self, v: float):\n",
    "        self.halt.halt_temp = float(v)\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update(dict(\n",
    "            min_steps=self.min_steps,\n",
    "            max_steps=self.max_steps,\n",
    "            ponder_lambda=self.ponder_lambda,\n",
    "            route_temp=self.route_temp,\n",
    "            halt_temp=self.halt_temp,\n",
    "            K=self.K,\n",
    "        ))\n",
    "        return cfg\n",
    "    def call(self, features, training=None):\n",
    "        x = features\n",
    "        B = tf.shape(x)[0]\n",
    "        dtype = x.dtype\n",
    "        halted = tf.zeros([B,1,1,1], dtype=dtype)\n",
    "        ponder_cost = tf.constant(0.0, dtype=dtype)\n",
    "        for t in range(self.max_steps):\n",
    "            router_logits, _ = self.router(x, training=training)              # [B,K]\n",
    "            probs  = tf.nn.softmax(router_logits / self.route_temp, axis=-1)  # [B,K]\n",
    "            top_idx = tf.argmax(probs, axis=-1, output_type=tf.int32)         # [B]\n",
    "            onehot  = tf.one_hot(top_idx, depth=self.K, dtype=dtype)          # [B,K]\n",
    "            onehot_st = onehot + tf.stop_gradient(probs - onehot)             # ST\n",
    "            y_list = [br(x, training=training) for br in self.branches]       # K x [B,H,W,C]\n",
    "            y_stack = tf.stack(y_list, axis=1)                                 # [B,K,H,W,C]\n",
    "            mask = tf.reshape(onehot_st, [-1, self.K, 1, 1, 1])\n",
    "            y_sel = tf.reduce_sum(mask * y_stack, axis=1)                      # [B,H,W,C]\n",
    "            can_halt = tf.cast(t >= self.min_steps - 1, dtype) * tf.ones([B,1], dtype)\n",
    "            p_soft, h_st = self.halt(y_sel, can_halt_mask=can_halt, training=training)  # [B,1]\n",
    "            h_st4 = tf.reshape(h_st, [-1,1,1,1])\n",
    "            halted = tf.clip_by_value(halted + (1.0 - halted) * h_st4, 0.0, 1.0)\n",
    "            if training and self.ponder_lambda > 0.0:\n",
    "                running_frac = tf.reduce_mean(1.0 - tf.squeeze(halted, [1,2,3]))\n",
    "                ponder_cost += tf.cast(running_frac, dtype)\n",
    "            if not training and tf.executing_eagerly():\n",
    "                if bool(tf.reduce_all(tf.squeeze(halted, [1,2,3]) > 0.5).numpy()):\n",
    "                    break\n",
    "        if training and self.ponder_lambda > 0.0:\n",
    "            self.add_loss(self.ponder_lambda * ponder_cost)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Linearly (or cosine) anneal route_temp and halt_temp over epochs.\n",
    "    route: 1.5 -> 0.7\n",
    "    halt:  3.0 -> 1.5\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_name=\"adaptive_router\",\n",
    "                 route_start=1.5, route_end=0.7,\n",
    "                 halt_start=3.0,  halt_end=0.5,\n",
    "                 epochs=150, mode=\"cosine\"):\n",
    "        super().__init__()\n",
    "        self.layer_name = layer_name\n",
    "        self.rs, self.re = float(route_start), float(route_end)\n",
    "        self.hs, self.he = float(halt_start),  float(halt_end)\n",
    "        self.E = int(epochs)\n",
    "        self.mode = mode\n",
    "\n",
    "    def _interp(self, e):\n",
    "        p = min(1.0, e / max(1, self.E-1))\n",
    "        if self.mode == \"cosine\":\n",
    "            p = 0.5*(1 - np.cos(np.pi*p))\n",
    "        return p\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        p = self._interp(epoch)\n",
    "        rtemp = self.rs + (self.re - self.rs)*p\n",
    "        htemp = self.hs + (self.he - self.hs)*p\n",
    "        layer = self.model.get_layer(self.layer_name)\n",
    "        layer.route_temp = rtemp\n",
    "        layer.halt_temp  = htemp\n",
    "        print(f\" > [TempScheduler] epoch {epoch+1}: route_temp={rtemp:.3f}, halt_temp={htemp:.3f}\")\n",
    "\n",
    "\n",
    "class RouterStatsCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, x_val, y_val, layer_name=\"adaptive_router\", batch_size=256):\n",
    "        super().__init__()\n",
    "        self.xv = x_val\n",
    "        self.yv = y_val\n",
    "        self.layer_name = layer_name\n",
    "        self.bs = batch_size\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        layer = self.model.get_layer(self.layer_name)\n",
    "        T, K = layer.max_steps, layer.K\n",
    "        steps_hist = np.zeros(T+1, np.int64)\n",
    "        expert_hist = np.zeros((T, K), np.int64)\n",
    "        n = len(self.xv)\n",
    "        for i in range(0, n, self.bs):\n",
    "            xb = self.xv[i:i+self.bs]\n",
    "            tb = trace_batch(self.model, xb, layer_name=self.layer_name, force_full=False)\n",
    "            t_used = tb[\"top_indices\"].shape[0]\n",
    "            running = tb[\"running_after\"]  # [t_used, B]\n",
    "            stopped = ~running\n",
    "            ever = stopped.any(axis=0)\n",
    "            first = np.argmax(stopped, axis=0)\n",
    "            used = np.where(ever, first+1, t_used)\n",
    "            for s in used:\n",
    "                steps_hist[min(int(s), T)] += 1\n",
    "            for t in range(t_used):\n",
    "                ch = tb[\"top_indices\"][t]\n",
    "                cnt = np.bincount(ch, minlength=K)\n",
    "                expert_hist[t] += cnt\n",
    "        avg_steps = np.sum(np.arange(T+1)*steps_hist)/max(1, steps_hist.sum())\n",
    "        print(f\"> [RouterStats] epoch {epoch+1}: avg_steps={avg_steps:.2f}  steps_hist={steps_hist.tolist()} expert_hist={expert_hist.tolist()}\")\n",
    "\n",
    "\n",
    "def trace_and_predict(\n",
    "    model,\n",
    "    x_input,\n",
    "    y_true=None,\n",
    "    layer_name=\"adaptive_router\",\n",
    "    force_full=False,   # set True to always loop max_steps, no early exit\n",
    "):\n",
    "    layer = model.get_layer(layer_name)\n",
    "    info = {\n",
    "        \"min_steps\": layer.min_steps,\n",
    "        \"max_steps\": layer.max_steps,\n",
    "        \"K\": layer.K,\n",
    "        \"route_temp\": getattr(layer, \"route_temp\", None),\n",
    "        \"halt_temp\": getattr(getattr(layer, \"halt\", None), \"halt_temp\", None),\n",
    "        \"unique_pools\": bool(getattr(layer, \"_pools\", []) not in (None, [])),\n",
    "        \"num_pools\": len(getattr(layer, \"_pools\", []) or []),\n",
    "    }\n",
    "    pre = keras.Model(model.input, layer.input)\n",
    "    x_in = tf.convert_to_tensor(x_input)\n",
    "    x = pre(x_in, training=False)\n",
    "    B = int(x.shape[0])\n",
    "    K, T = layer.K, layer.max_steps\n",
    "    dtype = x.dtype\n",
    "    halted = np.zeros((B,1,1,1), dtype=np.float32)\n",
    "    top_indices, probs_list = [], []\n",
    "    halt_soft_list, halt_hard_list = [], []\n",
    "    running_after_list = []\n",
    "    for t in range(T):\n",
    "        router_logits, _ = layer.router(x, training=False)\n",
    "        probs = tf.nn.softmax(router_logits / layer.route_temp, axis=-1)\n",
    "        top_idx = tf.argmax(probs, axis=-1, output_type=tf.int32)\n",
    "        onehot = tf.one_hot(top_idx, depth=K, dtype=dtype)\n",
    "        onehot_st = onehot + tf.stop_gradient(probs - onehot)\n",
    "        y_list = [br(x, training=False) for br in layer.branches]\n",
    "        y_stack = tf.stack(y_list, axis=1)\n",
    "        mask = tf.reshape(onehot_st, [-1, K, 1, 1, 1])\n",
    "        y_sel = tf.reduce_sum(mask * y_stack, axis=1)\n",
    "        can_halt = float(t >= layer.min_steps - 1)\n",
    "        can_mask = tf.ones([B,1], dtype) * can_halt\n",
    "        p_soft, h_st = layer.halt(y_sel, can_halt_mask=can_mask, training=False)\n",
    "        h_st_np = h_st.numpy()\n",
    "        halted = np.clip(halted + (1.0 - halted) * h_st_np, 0.0, 1.0)\n",
    "        running = np.squeeze(1.0 - halted, axis=(1,2,3)) > 0.5\n",
    "        x = y_sel  # no pooling\n",
    "        top_indices.append(top_idx.numpy())\n",
    "        probs_list.append(probs.numpy())\n",
    "        halt_soft_list.append(np.squeeze(p_soft.numpy(), axis=1).copy())\n",
    "        halt_hard_list.append(np.squeeze(h_st_np, axis=1).copy())\n",
    "        running_after_list.append(running.copy())\n",
    "        if (not force_full) and (not running.any()):\n",
    "            break\n",
    "    pred_probs = model(x_in, training=False).numpy()\n",
    "    pred_label = pred_probs.argmax(axis=-1).astype(np.int32)\n",
    "    if y_true is not None:\n",
    "        y_true_arr = np.asarray(y_true).reshape(-1)\n",
    "        correct = (pred_label == y_true_arr)\n",
    "    else:\n",
    "        correct = None\n",
    "    trace = {\n",
    "        \"top_indices\":       np.array(top_indices),\n",
    "        \"probs\":             np.array(probs_list),\n",
    "        \"halt_soft\":         np.array(halt_soft_list),\n",
    "        \"halt_hard\":         np.array(halt_hard_list),\n",
    "        \"running_after\":     np.array(running_after_list),\n",
    "    }\n",
    "    return {\n",
    "        \"trace\": trace,\n",
    "        \"pred_probs\": pred_probs,\n",
    "        \"pred_label\": pred_label,\n",
    "        \"true_label\": None if y_true is None else np.asarray(y_true),\n",
    "        \"layer_info\": info,\n",
    "        \"correct\": correct,\n",
    "    }\n",
    "\n",
    "\n",
    "def trace_batch(model, x_batch, layer_name=\"adaptive_router_top1\", force_full=False):\n",
    "    \"\"\"\n",
    "    Trace routing/halting for a batch. Pooling is disabled.\n",
    "    Returns a dict with arrays shaped [t_used, B, ...]:\n",
    "      - top_indices:  chosen expert index per step\n",
    "      - probs:        soft routing probabilities per step\n",
    "      - halt_soft:    hazard p_t per step\n",
    "      - halt_hard:    0/1 applied halts per step (float)\n",
    "      - running_after: boolean mask after each step (True = still running)\n",
    "    \"\"\"\n",
    "    layer = model.get_layer(layer_name)\n",
    "    pre = keras.Model(model.input, layer.input)\n",
    "\n",
    "    x_in = tf.convert_to_tensor(x_batch)\n",
    "    x = pre(x_in, training=False)\n",
    "\n",
    "    B = int(x.shape[0])\n",
    "    K, T = layer.K, layer.max_steps\n",
    "    running = np.ones((B,), dtype=bool)\n",
    "\n",
    "    top_indices, probs_list = [], []\n",
    "    halt_soft_list, halt_hard_list, running_after_list = [], [], []\n",
    "\n",
    "    for t in range(T):\n",
    "        # ---- routing (deterministic Top-1) ----\n",
    "        router_logits, _ = layer.router(x, training=False)                         # [B,K]\n",
    "        probs = tf.nn.softmax(router_logits / layer.route_temp, axis=-1)           # [B,K]\n",
    "        top_idx = tf.argmax(probs, axis=-1, output_type=tf.int32)                  # [B]\n",
    "        onehot  = tf.one_hot(top_idx, depth=K, dtype=x.dtype)\n",
    "        onehot_st = onehot + tf.stop_gradient(probs - onehot)                      # ST\n",
    "\n",
    "        # ---- experts (vectorized) + select ----\n",
    "        y_stack = tf.stack([br(x, training=False) for br in layer.branches], axis=1)  # [B,K,H,W,C]\n",
    "        y_sel = tf.reduce_sum(tf.reshape(onehot_st, [-1, K, 1, 1, 1]) * y_stack, axis=1)\n",
    "\n",
    "        # ---- halting (hazard) ----\n",
    "        can_halt = (t >= layer.min_steps - 1)\n",
    "        can_mask = tf.ones([B,1], x.dtype) if can_halt else tf.zeros([B,1], x.dtype)\n",
    "        p_soft, _ = layer.halt(y_sel, can_halt_mask=can_mask, training=False)      # [B,1]\n",
    "        p_np = tf.squeeze(p_soft, axis=1).numpy()                                   # [B]\n",
    "        halt_this = (p_np > 0.5) & running & can_halt                               # [B] bool\n",
    "\n",
    "        running = running & (~halt_this)\n",
    "\n",
    "        x = y_sel  # no pooling\n",
    "\n",
    "        # ---- collect step data ----\n",
    "        top_indices.append(top_idx.numpy())\n",
    "        probs_list.append(probs.numpy())\n",
    "        halt_soft_list.append(p_np.copy())\n",
    "        halt_hard_list.append(halt_this.astype(np.float32))\n",
    "        running_after_list.append(running.copy())\n",
    "\n",
    "        # early exit only if no sample is still running\n",
    "        if (not force_full) and (not running.any()):\n",
    "            break\n",
    "\n",
    "    trace = {\n",
    "        \"top_indices\":   np.array(top_indices),                 # [t_used, B]\n",
    "        \"probs\":         np.array(probs_list),                  # [t_used, B, K]\n",
    "        \"halt_soft\":     np.array(halt_soft_list),              # [t_used, B]\n",
    "        \"halt_hard\":     np.array(halt_hard_list),              # [t_used, B]\n",
    "        \"running_after\": np.array(running_after_list, dtype=bool),  # [t_used, B]\n",
    "    }\n",
    "    return trace\n",
    "\n",
    "def steps_used_from_running(running_after):\n",
    "    \"\"\"\n",
    "    running_after: [t_used, B] bool (True = still running after that step)\n",
    "    Returns: [B] int steps used (first time running becomes False; else t_used)\n",
    "    \"\"\"\n",
    "    t_used, B = running_after.shape\n",
    "    # A sample stops running the step it halts; so steps_used is the first index\n",
    "    # where running becomes False, +1. If never False, it's t_used.\n",
    "    stopped = ~running_after\n",
    "    ever_stopped = stopped.any(axis=0)\n",
    "    first_stop = np.argmax(stopped, axis=0)  # undefined when never stopped, fine below\n",
    "    return np.where(ever_stopped, first_stop + 1, t_used)\n",
    "\n",
    "\n",
    "def evaluate_with_router_stats(model, x, y, layer_name=\"adaptive_router_top1\",\n",
    "                               batch_size=256, force_full=False):\n",
    "    # accuracy\n",
    "    loss, acc = model.evaluate(x, y, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # router stats\n",
    "    layer = model.get_layer(layer_name)\n",
    "    K, T = layer.K, layer.max_steps\n",
    "\n",
    "    steps_hist = np.zeros(T+1, dtype=np.int64)   # index t for steps=t, last bin for \"T or more\"\n",
    "    expert_hist = np.zeros((T, K), dtype=np.int64)\n",
    "    halt_rate   = np.zeros(T, dtype=np.float64)\n",
    "    n_seen = 0\n",
    "\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        xb = x[i:i+batch_size]\n",
    "        tb = trace_batch(model, xb, layer_name=layer_name, force_full=force_full)\n",
    "\n",
    "        t_used, B = tb[\"top_indices\"].shape[0], tb[\"top_indices\"].shape[1]\n",
    "        n_seen += B\n",
    "\n",
    "        # expert usage per step (only for steps that exist in this batch)\n",
    "        for t in range(t_used):\n",
    "            choices = tb[\"top_indices\"][t]  # [B]\n",
    "            counts = np.bincount(choices, minlength=K)\n",
    "            expert_hist[t, :] += counts\n",
    "            halt_rate[t] += tb[\"halt_hard\"][t].mean()\n",
    "\n",
    "        # steps used per sample\n",
    "        steps_used = steps_used_from_running(tb[\"running_after\"])  # [B]\n",
    "        # cap into histogram (if force_full=False, some batches may stop early)\n",
    "        for s in steps_used:\n",
    "            s_idx = min(int(s), T)  # put \"==T\" also into T bin\n",
    "            steps_hist[s_idx] += 1\n",
    "\n",
    "    # normalize\n",
    "    halt_rate[:t_used] = halt_rate[:t_used] / max(1, (len(x) + batch_size - 1) // batch_size)\n",
    "\n",
    "    return {\n",
    "        \"loss\": float(loss),\n",
    "        \"acc\": float(acc),\n",
    "        \"steps_hist\": steps_hist,    # length T+1\n",
    "        \"expert_hist\": expert_hist,  # [T, K]\n",
    "        \"halt_rate\": halt_rate,      # [T] avg hard halts at step t\n",
    "        \"seen\": n_seen,\n",
    "        \"K\": K,\n",
    "        \"T\": T,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def build_adaptive_model_sparse(input_shape=(32,32,3), num_classes=10, filters=32,\n",
    "                                min_steps=1, max_steps=5, ponder_lambda=0.0,\n",
    "                                route_temp=1.0, halt_temp=1.0, pool_every_n=1):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # For CNNs on GPU, BatchNorm is faster than LayerNorm:\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = PoolingLayer(filters=filters, frac_ratio=2.0)(x)\n",
    "    x = ResidualBlock(filters=filters)(x)\n",
    "    x = PoolingLayer(filters=filters, frac_ratio=2.0)(x)\n",
    "\n",
    "    branches = [\n",
    "        ResidualBlock3x3(filters),\n",
    "        ResidualBlock5x5(filters),\n",
    "        ResidualBlockDepthwise5x5(filters),\n",
    "        ResidualBlockDepthwise7x7(filters),\n",
    "        ChannelSE(filters)\n",
    "    ]\n",
    "    pools = [PoolingLayer(filters=filters, frac_ratio=2.0) for _ in range(max_steps//pool_every_n)]\n",
    "    # Prefer simple/fast pooling (or a stride-2 Conv2D inside PoolingLayer)\n",
    "    #pooling  = PoolingLayer(filters=filters, frac_ratio=2.0)\n",
    "\n",
    "    x = AdaptiveRouterBlockTop1Vectorized(\n",
    "        branches=branches,\n",
    "        min_steps=min_steps,\n",
    "        max_steps=max_steps,\n",
    "        ponder_lambda=ponder_lambda,\n",
    "        route_temp=route_temp,\n",
    "        halt_temp=halt_temp,\n",
    "        name=\"adaptive_router\",\n",
    "    )(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # keep fp32 logits\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRouterBlockTop1Vectorized(layers.Layer):\n",
    "    \"\"\"\n",
    "    Deterministic Top-1 routing (same in train & infer) with ST grads.\n",
    "    Pooling is disabled: no pooling layers or events are used.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 branches,\n",
    "                 min_steps=1,\n",
    "                 max_steps=5,\n",
    "                 ponder_lambda=0.0,\n",
    "                 route_temp=1.0,\n",
    "                 halt_temp=1.0,\n",
    "                 name=None):\n",
    "        super().__init__(name=name)\n",
    "        assert 1 <= min_steps <= max_steps\n",
    "        self.branches = branches\n",
    "        self.K = len(branches)\n",
    "        self.router = AttnPoolRouter(K=self.K, dim_head=64, mlp_hidden=0)\n",
    "        self.halt   = HazardHaltingHead(hidden=64, halt_temp=float(halt_temp))\n",
    "        self._route_temp = float(route_temp)\n",
    "        self.ponder_lambda = float(ponder_lambda)\n",
    "        self.min_steps = int(min_steps)\n",
    "        self.max_steps = int(max_steps)\n",
    "    @property\n",
    "    def route_temp(self):\n",
    "        return self._route_temp\n",
    "    @route_temp.setter\n",
    "    def route_temp(self, v: float):\n",
    "        self._route_temp = float(v)\n",
    "    @property\n",
    "    def halt_temp(self):\n",
    "        return self.halt.halt_temp\n",
    "    @halt_temp.setter\n",
    "    def halt_temp(self, v: float):\n",
    "        self.halt.halt_temp = float(v)\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update(dict(\n",
    "            min_steps=self.min_steps,\n",
    "            max_steps=self.max_steps,\n",
    "            ponder_lambda=self.ponder_lambda,\n",
    "            route_temp=self.route_temp,\n",
    "            halt_temp=self.halt_temp,\n",
    "            K=self.K,\n",
    "        ))\n",
    "        return cfg\n",
    "    def call(self, features, training=None):\n",
    "        x = features\n",
    "        B = tf.shape(x)[0]\n",
    "        dtype = x.dtype\n",
    "        halted = tf.zeros([B,1,1,1], dtype=dtype)\n",
    "        ponder_cost = tf.constant(0.0, dtype=dtype)\n",
    "        for t in range(self.max_steps):\n",
    "            router_logits, _ = self.router(x, training=training)              # [B,K]\n",
    "            probs  = tf.nn.softmax(router_logits / self.route_temp, axis=-1)  # [B,K]\n",
    "            top_idx = tf.argmax(probs, axis=-1, output_type=tf.int32)         # [B]\n",
    "            onehot  = tf.one_hot(top_idx, depth=self.K, dtype=dtype)          # [B,K]\n",
    "            onehot_st = onehot + tf.stop_gradient(probs - onehot)             # ST\n",
    "            y_list = [br(x, training=training) for br in self.branches]       # K x [B,H,W,C]\n",
    "            y_stack = tf.stack(y_list, axis=1)                                 # [B,K,H,W,C]\n",
    "            mask = tf.reshape(onehot_st, [-1, self.K, 1, 1, 1])\n",
    "            y_sel = tf.reduce_sum(mask * y_stack, axis=1)                      # [B,H,W,C]\n",
    "            can_halt = tf.cast(t >= self.min_steps - 1, dtype) * tf.ones([B,1], dtype)\n",
    "            p_soft, h_st = self.halt(y_sel, can_halt_mask=can_halt, training=training)  # [B,1]\n",
    "            h_st4 = tf.reshape(h_st, [-1,1,1,1])\n",
    "            halted = tf.clip_by_value(halted + (1.0 - halted) * h_st4, 0.0, 1.0)\n",
    "            if training and self.ponder_lambda > 0.0:\n",
    "                running_frac = tf.reduce_mean(1.0 - tf.squeeze(halted, [1,2,3]))\n",
    "                ponder_cost += tf.cast(running_frac, dtype)\n",
    "            if not training and tf.executing_eagerly():\n",
    "                if bool(tf.reduce_all(tf.squeeze(halted, [1,2,3]) > 0.5).numpy()):\n",
    "                    break\n",
    "        if training and self.ponder_lambda > 0.0:\n",
    "            self.add_loss(self.ponder_lambda * ponder_cost)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_682\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 32, 32, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " pooling_layer_14 (PoolingL  (None, 16, 16, 64)        2240      \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " residual_block_2 (Residual  (None, 16, 16, 64)        8320      \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " pooling_layer_15 (PoolingL  (None, 8, 8, 64)          4288      \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " adaptive_router (AdaptiveR  (None, 8, 8, 64)          186406    \n",
      " outerBlockTop1Vectorized)                                       \n",
      "                                                                 \n",
      " global_average_pooling2d_2  (None, 64)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 32, 32, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " pooling_layer_14 (PoolingL  (None, 16, 16, 64)        2240      \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " residual_block_2 (Residual  (None, 16, 16, 64)        8320      \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " pooling_layer_15 (PoolingL  (None, 8, 8, 64)          4288      \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " adaptive_router (AdaptiveR  (None, 8, 8, 64)          186406    \n",
      " outerBlockTop1Vectorized)                                       \n",
      "                                                                 \n",
      " global_average_pooling2d_2  (None, 64)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 202928 (792.69 KB)\n",
      "Trainable params: 202864 (792.44 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "=================================================================\n",
      "Total params: 202928 (792.69 KB)\n",
      "Trainable params: 202864 (792.44 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "FILTERS = 64\n",
    "MIN_STEPS = 2\n",
    "MAX_STEPS = 6\n",
    "PONDER_LAMBDA = 5e-5  # >0 encourages fewer steps\n",
    "HALT_TEMP = 2.0\n",
    "ROUTE_TEMP = 2.0\n",
    "EPOCHS = 50\n",
    "\n",
    "model = build_adaptive_model_sparse(\n",
    "    input_shape=(32,32,3),\n",
    "    num_classes=10,\n",
    "    filters=FILTERS,\n",
    "    min_steps=MIN_STEPS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    ponder_lambda=PONDER_LAMBDA,         # >0 encourages fewer steps; tune as needed\n",
    "    halt_temp=HALT_TEMP,\n",
    "    route_temp=ROUTE_TEMP\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    TempScheduler(layer_name=\"adaptive_router\", epochs=EPOCHS, mode=\"cosine\", route_start=ROUTE_TEMP, route_end=0.7, halt_start=HALT_TEMP, halt_end=0.5),\n",
    "    RouterStatsCallback(x_test, y_test, layer_name=\"adaptive_router\")\n",
    "]\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > [TempScheduler] epoch 1: route_temp=1.500, halt_temp=1.500\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adaptive_router/residual_block3x3_2/conv2d/kernel:0', 'adaptive_router/residual_block3x3_2/conv2d/bias:0', 'adaptive_router/residual_block3x3_2/group_conv2d/conv2d_2/kernel:0', 'adaptive_router/residual_block3x3_2/group_conv2d/conv2d_2/bias:0', 'adaptive_router/residual_block3x3_2/layer_normalization/gamma:0', 'adaptive_router/residual_block3x3_2/layer_normalization/beta:0', 'adaptive_router/residual_block3x3_2/group_conv2d_1/conv2d_3/kernel:0', 'adaptive_router/residual_block3x3_2/group_conv2d_1/conv2d_3/bias:0', 'adaptive_router/residual_block3x3_2/layer_normalization_1/gamma:0', 'adaptive_router/residual_block3x3_2/layer_normalization_1/beta:0', 'adaptive_router/residual_block3x3_2/conv2d_1/kernel:0', 'adaptive_router/residual_block3x3_2/conv2d_1/bias:0', 'adaptive_router/residual_block5x5_2/conv2d_4/kernel:0', 'adaptive_router/residual_block5x5_2/conv2d_4/bias:0', 'adaptive_router/residual_block5x5_2/group_conv2d_2/conv2d_6/kernel:0', 'adaptive_router/residual_block5x5_2/group_conv2d_2/conv2d_6/bias:0', 'adaptive_router/residual_block5x5_2/layer_normalization_2/gamma:0', 'adaptive_router/residual_block5x5_2/layer_normalization_2/beta:0', 'adaptive_router/residual_block5x5_2/group_conv2d_3/conv2d_7/kernel:0', 'adaptive_router/residual_block5x5_2/group_conv2d_3/conv2d_7/bias:0', 'adaptive_router/residual_block5x5_2/layer_normalization_3/gamma:0', 'adaptive_router/residual_block5x5_2/layer_normalization_3/beta:0', 'adaptive_router/residual_block5x5_2/conv2d_5/kernel:0', 'adaptive_router/residual_block5x5_2/conv2d_5/bias:0', 'adaptive_router/residual_block_depthwise5x5_2/conv2d_8/kernel:0', 'adaptive_router/residual_block_depthwise5x5_2/conv2d_8/bias:0', 'adaptive_router/residual_block_depthwise5x5_2/depthwise_conv2d/depthwise_kernel:0', 'adaptive_router/residual_block_depthwise5x5_2/depthwise_conv2d/bias:0', 'adaptive_router/residual_block_depthwise5x5_2/layer_normalization_4/gamma:0', 'adaptive_router/residual_block_depthwise5x5_2/layer_normalization_4/beta:0', 'adaptive_router/residual_block_depthwise5x5_2/depthwise_conv2d_1/depthwise_kernel:0', 'adaptive_router/residual_block_depthwise5x5_2/depthwise_conv2d_1/bias:0', 'adaptive_router/residual_block_depthwise5x5_2/layer_normalization_5/gamma:0', 'adaptive_router/residual_block_depthwise5x5_2/layer_normalization_5/beta:0', 'adaptive_router/residual_block_depthwise5x5_2/conv2d_9/kernel:0', 'adaptive_router/residual_block_depthwise5x5_2/conv2d_9/bias:0', 'adaptive_router/residual_block_depthwise7x7_2/conv2d_10/kernel:0', 'adaptive_router/residual_block_depthwise7x7_2/conv2d_10/bias:0', 'adaptive_router/residual_block_depthwise7x7_2/depthwise_conv2d_2/depthwise_kernel:0', 'adaptive_router/residual_block_depthwise7x7_2/depthwise_conv2d_2/bias:0', 'adaptive_router/residual_block_depthwise7x7_2/layer_normalization_6/gamma:0', 'adaptive_router/residual_block_depthwise7x7_2/layer_normalization_6/beta:0', 'adaptive_router/residual_block_depthwise7x7_2/depthwise_conv2d_3/depthwise_kernel:0', 'adaptive_router/residual_block_depthwise7x7_2/depthwise_conv2d_3/bias:0', 'adaptive_router/residual_block_depthwise7x7_2/layer_normalization_7/gamma:0', 'adaptive_router/residual_block_depthwise7x7_2/layer_normalization_7/beta:0', 'adaptive_router/residual_block_depthwise7x7_2/conv2d_11/kernel:0', 'adaptive_router/residual_block_depthwise7x7_2/conv2d_11/bias:0', 'adaptive_router/channel_se_2/dense/kernel:0', 'adaptive_router/channel_se_2/dense_1/kernel:0', 'queries:0', 'adaptive_router/attn_pool_router_2/conv2d_7/kernel:0', 'adaptive_router/attn_pool_router_2/conv2d_8/kernel:0', 'adaptive_router/attn_pool_router_2/dense_4/kernel:0', 'adaptive_router/attn_pool_router_2/dense_4/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adaptive_router/residual_block3x3_2/conv2d/kernel:0', 'adaptive_router/residual_block3x3_2/conv2d/bias:0', 'adaptive_router/residual_block3x3_2/group_conv2d/conv2d_2/kernel:0', 'adaptive_router/residual_block3x3_2/group_conv2d/conv2d_2/bias:0', 'adaptive_router/residual_block3x3_2/layer_normalization/gamma:0', 'adaptive_router/residual_block3x3_2/layer_normalization/beta:0', 'adaptive_router/residual_block3x3_2/group_conv2d_1/conv2d_3/kernel:0', 'adaptive_router/residual_block3x3_2/group_conv2d_1/conv2d_3/bias:0', 'adaptive_router/residual_block3x3_2/layer_normalization_1/gamma:0', 'adaptive_router/residual_block3x3_2/layer_normalization_1/beta:0', 'adaptive_router/residual_block3x3_2/conv2d_1/kernel:0', 'adaptive_router/residual_block3x3_2/conv2d_1/bias:0', 'adaptive_router/residual_block5x5_2/conv2d_4/kernel:0', 'adaptive_router/residual_block5x5_2/conv2d_4/bias:0', 'adaptive_router/residual_block5x5_2/group_conv2d_2/conv2d_6/kernel:0', 'adaptive_router/residual_block5x5_2/group_conv2d_2/conv2d_6/bias:0', 'adaptive_router/residual_block5x5_2/layer_normalization_2/gamma:0', 'adaptive_router/residual_block5x5_2/layer_normalization_2/beta:0', 'adaptive_router/residual_block5x5_2/group_conv2d_3/conv2d_7/kernel:0', 'adaptive_router/residual_block5x5_2/group_conv2d_3/conv2d_7/bias:0', 'adaptive_router/residual_block5x5_2/layer_normalization_3/gamma:0', 'adaptive_router/residual_block5x5_2/layer_normalization_3/beta:0', 'adaptive_router/residual_block5x5_2/conv2d_5/kernel:0', 'adaptive_router/residual_block5x5_2/conv2d_5/bias:0', 'adaptive_router/residual_block_depthwise5x5_2/conv2d_8/kernel:0', 'adaptive_router/residual_block_depthwise5x5_2/conv2d_8/bias:0', 'adaptive_router/residual_block_depthwise5x5_2/depthwise_conv2d/depthwise_kernel:0', 'adaptive_router/residual_block_depthwise5x5_2/depthwise_conv2d/bias:0', 'adaptive_router/residual_block_depthwise5x5_2/layer_normalization_4/gamma:0', 'adaptive_router/residual_block_depthwise5x5_2/layer_normalization_4/beta:0', 'adaptive_router/residual_block_depthwise5x5_2/depthwise_conv2d_1/depthwise_kernel:0', 'adaptive_router/residual_block_depthwise5x5_2/depthwise_conv2d_1/bias:0', 'adaptive_router/residual_block_depthwise5x5_2/layer_normalization_5/gamma:0', 'adaptive_router/residual_block_depthwise5x5_2/layer_normalization_5/beta:0', 'adaptive_router/residual_block_depthwise5x5_2/conv2d_9/kernel:0', 'adaptive_router/residual_block_depthwise5x5_2/conv2d_9/bias:0', 'adaptive_router/residual_block_depthwise7x7_2/conv2d_10/kernel:0', 'adaptive_router/residual_block_depthwise7x7_2/conv2d_10/bias:0', 'adaptive_router/residual_block_depthwise7x7_2/depthwise_conv2d_2/depthwise_kernel:0', 'adaptive_router/residual_block_depthwise7x7_2/depthwise_conv2d_2/bias:0', 'adaptive_router/residual_block_depthwise7x7_2/layer_normalization_6/gamma:0', 'adaptive_router/residual_block_depthwise7x7_2/layer_normalization_6/beta:0', 'adaptive_router/residual_block_depthwise7x7_2/depthwise_conv2d_3/depthwise_kernel:0', 'adaptive_router/residual_block_depthwise7x7_2/depthwise_conv2d_3/bias:0', 'adaptive_router/residual_block_depthwise7x7_2/layer_normalization_7/gamma:0', 'adaptive_router/residual_block_depthwise7x7_2/layer_normalization_7/beta:0', 'adaptive_router/residual_block_depthwise7x7_2/conv2d_11/kernel:0', 'adaptive_router/residual_block_depthwise7x7_2/conv2d_11/bias:0', 'adaptive_router/channel_se_2/dense/kernel:0', 'adaptive_router/channel_se_2/dense_1/kernel:0', 'queries:0', 'adaptive_router/attn_pool_router_2/conv2d_7/kernel:0', 'adaptive_router/attn_pool_router_2/conv2d_8/kernel:0', 'adaptive_router/attn_pool_router_2/dense_4/kernel:0', 'adaptive_router/attn_pool_router_2/dense_4/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adaptive_router/residual_block3x3_2/conv2d/kernel:0', 'adaptive_router/residual_block3x3_2/conv2d/bias:0', 'adaptive_router/residual_block3x3_2/group_conv2d/conv2d_2/kernel:0', 'adaptive_router/residual_block3x3_2/group_conv2d/conv2d_2/bias:0', 'adaptive_router/residual_block3x3_2/layer_normalization/gamma:0', 'adaptive_router/residual_block3x3_2/layer_normalization/beta:0', 'adaptive_router/residual_block3x3_2/group_conv2d_1/conv2d_3/kernel:0', 'adaptive_router/residual_block3x3_2/group_conv2d_1/conv2d_3/bias:0', 'adaptive_router/residual_block3x3_2/layer_normalization_1/gamma:0', 'adaptive_router/residual_block3x3_2/layer_normalization_1/beta:0', 'adaptive_router/residual_block3x3_2/conv2d_1/kernel:0', 'adaptive_router/residual_block3x3_2/conv2d_1/bias:0', 'adaptive_router/residual_block5x5_2/conv2d_4/kernel:0', 'adaptive_router/residual_block5x5_2/conv2d_4/bias:0', 'adaptive_router/residual_block5x5_2/group_conv2d_2/conv2d_6/kernel:0', 'adaptive_router/residual_block5x5_2/group_conv2d_2/conv2d_6/bias:0', 'adaptive_router/residual_block5x5_2/layer_normalization_2/gamma:0', 'adaptive_router/residual_block5x5_2/layer_normalization_2/beta:0', 'adaptive_router/residual_block5x5_2/group_conv2d_3/conv2d_7/kernel:0', 'adaptive_router/residual_block5x5_2/group_conv2d_3/conv2d_7/bias:0', 'adaptive_router/residual_block5x5_2/layer_normalization_3/gamma:0', 'adaptive_router/residual_block5x5_2/layer_normalization_3/beta:0', 'adaptive_router/residual_block5x5_2/conv2d_5/kernel:0', 'adaptive_router/residual_block5x5_2/conv2d_5/bias:0', 'adaptive_router/residual_block_depthwise5x5_2/conv2d_8/kernel:0', 'adaptive_router/residual_block_depthwise5x5_2/conv2d_8/bias:0', 'adaptive_router/residual_block_depthwise5x5_2/depthwise_conv2d/depthwise_kernel:0', 'adaptive_router/residual_block_depthwise5x5_2/depthwise_conv2d/bias:0', 'adaptive_router/residual_block_depthwise5x5_2/layer_normalization_4/gamma:0', 'adaptive_router/residual_block_depthwise5x5_2/layer_normalization_4/beta:0', 'adaptive_router/residual_block_depthwise5x5_2/depthwise_conv2d_1/depthwise_kernel:0', 'adaptive_router/residual_block_depthwise5x5_2/depthwise_conv2d_1/bias:0', 'adaptive_router/residual_block_depthwise5x5_2/layer_normalization_5/gamma:0', 'adaptive_router/residual_block_depthwise5x5_2/layer_normalization_5/beta:0', 'adaptive_router/residual_block_depthwise5x5_2/conv2d_9/kernel:0', 'adaptive_router/residual_block_depthwise5x5_2/conv2d_9/bias:0', 'adaptive_router/residual_block_depthwise7x7_2/conv2d_10/kernel:0', 'adaptive_router/residual_block_depthwise7x7_2/conv2d_10/bias:0', 'adaptive_router/residual_block_depthwise7x7_2/depthwise_conv2d_2/depthwise_kernel:0', 'adaptive_router/residual_block_depthwise7x7_2/depthwise_conv2d_2/bias:0', 'adaptive_router/residual_block_depthwise7x7_2/layer_normalization_6/gamma:0', 'adaptive_router/residual_block_depthwise7x7_2/layer_normalization_6/beta:0', 'adaptive_router/residual_block_depthwise7x7_2/depthwise_conv2d_3/depthwise_kernel:0', 'adaptive_router/residual_block_depthwise7x7_2/depthwise_conv2d_3/bias:0', 'adaptive_router/residual_block_depthwise7x7_2/layer_normalization_7/gamma:0', 'adaptive_router/residual_block_depthwise7x7_2/layer_normalization_7/beta:0', 'adaptive_router/residual_block_depthwise7x7_2/conv2d_11/kernel:0', 'adaptive_router/residual_block_depthwise7x7_2/conv2d_11/bias:0', 'adaptive_router/channel_se_2/dense/kernel:0', 'adaptive_router/channel_se_2/dense_1/kernel:0', 'queries:0', 'adaptive_router/attn_pool_router_2/conv2d_7/kernel:0', 'adaptive_router/attn_pool_router_2/conv2d_8/kernel:0', 'adaptive_router/attn_pool_router_2/dense_4/kernel:0', 'adaptive_router/attn_pool_router_2/dense_4/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['adaptive_router/residual_block3x3_2/conv2d/kernel:0', 'adaptive_router/residual_block3x3_2/conv2d/bias:0', 'adaptive_router/residual_block3x3_2/group_conv2d/conv2d_2/kernel:0', 'adaptive_router/residual_block3x3_2/group_conv2d/conv2d_2/bias:0', 'adaptive_router/residual_block3x3_2/layer_normalization/gamma:0', 'adaptive_router/residual_block3x3_2/layer_normalization/beta:0', 'adaptive_router/residual_block3x3_2/group_conv2d_1/conv2d_3/kernel:0', 'adaptive_router/residual_block3x3_2/group_conv2d_1/conv2d_3/bias:0', 'adaptive_router/residual_block3x3_2/layer_normalization_1/gamma:0', 'adaptive_router/residual_block3x3_2/layer_normalization_1/beta:0', 'adaptive_router/residual_block3x3_2/conv2d_1/kernel:0', 'adaptive_router/residual_block3x3_2/conv2d_1/bias:0', 'adaptive_router/residual_block5x5_2/conv2d_4/kernel:0', 'adaptive_router/residual_block5x5_2/conv2d_4/bias:0', 'adaptive_router/residual_block5x5_2/group_conv2d_2/conv2d_6/kernel:0', 'adaptive_router/residual_block5x5_2/group_conv2d_2/conv2d_6/bias:0', 'adaptive_router/residual_block5x5_2/layer_normalization_2/gamma:0', 'adaptive_router/residual_block5x5_2/layer_normalization_2/beta:0', 'adaptive_router/residual_block5x5_2/group_conv2d_3/conv2d_7/kernel:0', 'adaptive_router/residual_block5x5_2/group_conv2d_3/conv2d_7/bias:0', 'adaptive_router/residual_block5x5_2/layer_normalization_3/gamma:0', 'adaptive_router/residual_block5x5_2/layer_normalization_3/beta:0', 'adaptive_router/residual_block5x5_2/conv2d_5/kernel:0', 'adaptive_router/residual_block5x5_2/conv2d_5/bias:0', 'adaptive_router/residual_block_depthwise5x5_2/conv2d_8/kernel:0', 'adaptive_router/residual_block_depthwise5x5_2/conv2d_8/bias:0', 'adaptive_router/residual_block_depthwise5x5_2/depthwise_conv2d/depthwise_kernel:0', 'adaptive_router/residual_block_depthwise5x5_2/depthwise_conv2d/bias:0', 'adaptive_router/residual_block_depthwise5x5_2/layer_normalization_4/gamma:0', 'adaptive_router/residual_block_depthwise5x5_2/layer_normalization_4/beta:0', 'adaptive_router/residual_block_depthwise5x5_2/depthwise_conv2d_1/depthwise_kernel:0', 'adaptive_router/residual_block_depthwise5x5_2/depthwise_conv2d_1/bias:0', 'adaptive_router/residual_block_depthwise5x5_2/layer_normalization_5/gamma:0', 'adaptive_router/residual_block_depthwise5x5_2/layer_normalization_5/beta:0', 'adaptive_router/residual_block_depthwise5x5_2/conv2d_9/kernel:0', 'adaptive_router/residual_block_depthwise5x5_2/conv2d_9/bias:0', 'adaptive_router/residual_block_depthwise7x7_2/conv2d_10/kernel:0', 'adaptive_router/residual_block_depthwise7x7_2/conv2d_10/bias:0', 'adaptive_router/residual_block_depthwise7x7_2/depthwise_conv2d_2/depthwise_kernel:0', 'adaptive_router/residual_block_depthwise7x7_2/depthwise_conv2d_2/bias:0', 'adaptive_router/residual_block_depthwise7x7_2/layer_normalization_6/gamma:0', 'adaptive_router/residual_block_depthwise7x7_2/layer_normalization_6/beta:0', 'adaptive_router/residual_block_depthwise7x7_2/depthwise_conv2d_3/depthwise_kernel:0', 'adaptive_router/residual_block_depthwise7x7_2/depthwise_conv2d_3/bias:0', 'adaptive_router/residual_block_depthwise7x7_2/layer_normalization_7/gamma:0', 'adaptive_router/residual_block_depthwise7x7_2/layer_normalization_7/beta:0', 'adaptive_router/residual_block_depthwise7x7_2/conv2d_11/kernel:0', 'adaptive_router/residual_block_depthwise7x7_2/conv2d_11/bias:0', 'adaptive_router/channel_se_2/dense/kernel:0', 'adaptive_router/channel_se_2/dense_1/kernel:0', 'queries:0', 'adaptive_router/attn_pool_router_2/conv2d_7/kernel:0', 'adaptive_router/attn_pool_router_2/conv2d_8/kernel:0', 'adaptive_router/attn_pool_router_2/dense_4/kernel:0', 'adaptive_router/attn_pool_router_2/dense_4/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 19:23:57.643041: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - ETA: 0s - loss: 1.7784 - accuracy: 0.3478"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 19:24:40.105106: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [RouterStats] epoch 1: avg_steps=2.57  steps_hist=[0, 0, 8002, 105, 48, 1845] expert_hist=[[134, 4379, 35, 5451, 1], [60, 5302, 0, 4638, 0], [81, 6012, 0, 3907, 0], [387, 6648, 0, 2965, 0], [1457, 7116, 0, 1427, 0]]\n",
      "391/391 [==============================] - 72s 171ms/step - loss: 1.7784 - accuracy: 0.3478 - val_loss: 1.7858 - val_accuracy: 0.3170\n",
      " > [TempScheduler] epoch 2: route_temp=1.499, halt_temp=1.499\n",
      "Epoch 2/50\n",
      " > [TempScheduler] epoch 2: route_temp=1.499, halt_temp=1.499\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5026 - accuracy: 0.4595> [RouterStats] epoch 2: avg_steps=3.42  steps_hist=[0, 0, 5027, 247, 209, 4517] expert_hist=[[641, 6806, 0, 2449, 104], [316, 7723, 0, 1961, 0], [273, 8150, 0, 1577, 0], [406, 8330, 0, 1264, 0], [719, 8339, 0, 942, 0]]\n",
      "391/391 [==============================] - 63s 160ms/step - loss: 1.5026 - accuracy: 0.4595 - val_loss: 1.4338 - val_accuracy: 0.4788\n",
      "> [RouterStats] epoch 2: avg_steps=3.42  steps_hist=[0, 0, 5027, 247, 209, 4517] expert_hist=[[641, 6806, 0, 2449, 104], [316, 7723, 0, 1961, 0], [273, 8150, 0, 1577, 0], [406, 8330, 0, 1264, 0], [719, 8339, 0, 942, 0]]\n",
      "391/391 [==============================] - 63s 160ms/step - loss: 1.5026 - accuracy: 0.4595 - val_loss: 1.4338 - val_accuracy: 0.4788\n",
      " > [TempScheduler] epoch 3: route_temp=1.497, halt_temp=1.496\n",
      "Epoch 3/50\n",
      " > [TempScheduler] epoch 3: route_temp=1.497, halt_temp=1.496\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.3961 - accuracy: 0.4969> [RouterStats] epoch 3: avg_steps=2.32  steps_hist=[0, 0, 8869, 66, 35, 1030] expert_hist=[[937, 5700, 1, 3296, 66], [571, 6788, 0, 2638, 3], [421, 7602, 0, 1977, 0], [430, 8192, 0, 1378, 0], [484, 8634, 0, 882, 0]]\n",
      "391/391 [==============================] - 62s 158ms/step - loss: 1.3961 - accuracy: 0.4969 - val_loss: 1.4849 - val_accuracy: 0.4600\n",
      "> [RouterStats] epoch 3: avg_steps=2.32  steps_hist=[0, 0, 8869, 66, 35, 1030] expert_hist=[[937, 5700, 1, 3296, 66], [571, 6788, 0, 2638, 3], [421, 7602, 0, 1977, 0], [430, 8192, 0, 1378, 0], [484, 8634, 0, 882, 0]]\n",
      "391/391 [==============================] - 62s 158ms/step - loss: 1.3961 - accuracy: 0.4969 - val_loss: 1.4849 - val_accuracy: 0.4600\n",
      " > [TempScheduler] epoch 4: route_temp=1.493, halt_temp=1.491\n",
      "Epoch 4/50\n",
      " > [TempScheduler] epoch 4: route_temp=1.493, halt_temp=1.491\n",
      "Epoch 4/50\n",
      "256/391 [==================>...........] - ETA: 13s - loss: 1.3333 - accuracy: 0.5200"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(ds_train, epochs=50, validation_data=ds_val, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 49.42%  |  samples: 10000\n",
      "Steps histogram (0..T; last bin = T): [   0    0 3616  241  321 5822]\n",
      "Halt rate per step: [0.    0.363 0.024 0.032 0.043]\n",
      "Expert usage at step 1: [ 543 7741    2 1666   48]\n",
      "Expert usage at step 2: [ 386 8257    0 1354    3]\n",
      "Expert usage at step 3: [ 574 8476    0  949    1]\n",
      "Expert usage at step 4: [1112 8303    0  584    1]\n"
     ]
    }
   ],
   "source": [
    "stats = evaluate_with_router_stats(model, x_test, y_test,\n",
    "                                   layer_name=\"adaptive_router\",\n",
    "                                   batch_size=512,\n",
    "                                   force_full=False)\n",
    "\n",
    "print(f\"Test acc: {stats['acc']*100:.2f}%  |  samples: {stats['seen']}\")\n",
    "print(\"Steps histogram (0..T; last bin = T):\", stats[\"steps_hist\"])\n",
    "print(\"Halt rate per step:\", np.round(stats[\"halt_rate\"][:np.nonzero(stats['expert_hist'].sum(axis=1) > 0)[0][-1]+1], 3))\n",
    "print(\"Expert usage at step 1:\", stats[\"expert_hist\"][0])\n",
    "print(\"Expert usage at step 2:\", stats[\"expert_hist\"][1])\n",
    "print(\"Expert usage at step 3:\", stats[\"expert_hist\"][2])\n",
    "print(\"Expert usage at step 4:\", stats[\"expert_hist\"][3])\n",
    "#print(\"Expert usage at step 5:\", stats[\"expert_hist\"][4])\n",
    "#print(\"Expert usage at step 6:\", stats[\"expert_hist\"][5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Test sample 100 =========\n",
      "experts per step: [1 1]\n",
      "halt probs: [0.     0.5254]\n",
      "pred label: 4 true label: 4\n",
      "========= Test sample 101 =========\n",
      "experts per step: [1 1]\n",
      "halt probs: [0.    0.521]\n",
      "pred label: 4 true label: 5\n",
      "========= Test sample 101 =========\n",
      "experts per step: [1 1]\n",
      "halt probs: [0.    0.521]\n",
      "pred label: 4 true label: 5\n",
      "========= Test sample 102 =========\n",
      "experts per step: [3 3]\n",
      "halt probs: [0.     0.5366]\n",
      "pred label: 3 true label: 6\n",
      "========= Test sample 102 =========\n",
      "experts per step: [3 3]\n",
      "halt probs: [0.     0.5366]\n",
      "pred label: 3 true label: 6\n",
      "========= Test sample 103 =========\n",
      "experts per step: [1 1]\n",
      "halt probs: [0.    0.505]\n",
      "pred label: 3 true label: 3\n",
      "========= Test sample 103 =========\n",
      "experts per step: [1 1]\n",
      "halt probs: [0.    0.505]\n",
      "pred label: 3 true label: 3\n",
      "========= Test sample 104 =========\n",
      "experts per step: [1 1 1]\n",
      "halt probs: [0.     0.4595 0.459 ]\n",
      "pred label: 9 true label: 1\n",
      "========= Test sample 104 =========\n",
      "experts per step: [1 1 1]\n",
      "halt probs: [0.     0.4595 0.459 ]\n",
      "pred label: 9 true label: 1\n",
      "========= Test sample 105 =========\n",
      "experts per step: [0 0 0]\n",
      "halt probs: [0.     0.4802 0.4783]\n",
      "pred label: 1 true label: 1\n",
      "========= Test sample 105 =========\n",
      "experts per step: [0 0 0]\n",
      "halt probs: [0.     0.4802 0.4783]\n",
      "pred label: 1 true label: 1\n",
      "========= Test sample 106 =========\n",
      "experts per step: [1 1]\n",
      "halt probs: [0.    0.535]\n",
      "pred label: 6 true label: 3\n",
      "========= Test sample 106 =========\n",
      "experts per step: [1 1]\n",
      "halt probs: [0.    0.535]\n",
      "pred label: 6 true label: 3\n",
      "========= Test sample 107 =========\n",
      "experts per step: [1 1 1]\n",
      "halt probs: [0.     0.4768 0.4854]\n",
      "pred label: 6 true label: 6\n",
      "========= Test sample 107 =========\n",
      "experts per step: [1 1 1]\n",
      "halt probs: [0.     0.4768 0.4854]\n",
      "pred label: 6 true label: 6\n",
      "========= Test sample 108 =========\n",
      "experts per step: [0 1]\n",
      "halt probs: [0.     0.5454]\n",
      "pred label: 8 true label: 8\n",
      "========= Test sample 108 =========\n",
      "experts per step: [0 1]\n",
      "halt probs: [0.     0.5454]\n",
      "pred label: 8 true label: 8\n",
      "========= Test sample 109 =========\n",
      "experts per step: [3 3]\n",
      "halt probs: [0.    0.507]\n",
      "pred label: 3 true label: 7\n",
      "========= Test sample 109 =========\n",
      "experts per step: [3 3]\n",
      "halt probs: [0.    0.507]\n",
      "pred label: 3 true label: 7\n",
      "========= Test sample 110 =========\n",
      "experts per step: [1 1 1]\n",
      "halt probs: [0.     0.4915 0.5005]\n",
      "pred label: 4 true label: 4\n",
      "========= Test sample 110 =========\n",
      "experts per step: [1 1 1]\n",
      "halt probs: [0.     0.4915 0.5005]\n",
      "pred label: 4 true label: 4\n",
      "========= Test sample 111 =========\n",
      "experts per step: [1 1]\n",
      "halt probs: [0.     0.5093]\n",
      "pred label: 0 true label: 0\n",
      "========= Test sample 111 =========\n",
      "experts per step: [1 1]\n",
      "halt probs: [0.     0.5093]\n",
      "pred label: 0 true label: 0\n",
      "========= Test sample 112 =========\n",
      "experts per step: [1 1 1]\n",
      "halt probs: [0.     0.4746 0.4863]\n",
      "pred label: 2 true label: 6\n",
      "========= Test sample 112 =========\n",
      "experts per step: [1 1 1]\n",
      "halt probs: [0.     0.4746 0.4863]\n",
      "pred label: 2 true label: 6\n",
      "========= Test sample 113 =========\n",
      "experts per step: [1 1]\n",
      "halt probs: [0.     0.5215]\n",
      "pred label: 2 true label: 2\n",
      "========= Test sample 113 =========\n",
      "experts per step: [1 1]\n",
      "halt probs: [0.     0.5215]\n",
      "pred label: 2 true label: 2\n",
      "========= Test sample 114 =========\n",
      "experts per step: [1 1 1]\n",
      "halt probs: [0.     0.4885 0.4849]\n",
      "pred label: 1 true label: 1\n",
      "========= Test sample 114 =========\n",
      "experts per step: [1 1 1]\n",
      "halt probs: [0.     0.4885 0.4849]\n",
      "pred label: 1 true label: 1\n",
      "========= Test sample 115 =========\n",
      "experts per step: [1 1 1]\n",
      "halt probs: [0.     0.499  0.5005]\n",
      "pred label: 2 true label: 3\n",
      "========= Test sample 115 =========\n",
      "experts per step: [1 1 1]\n",
      "halt probs: [0.     0.499  0.5005]\n",
      "pred label: 2 true label: 3\n",
      "========= Test sample 116 =========\n",
      "experts per step: [3 3]\n",
      "halt probs: [0.    0.512]\n",
      "pred label: 0 true label: 0\n",
      "========= Test sample 116 =========\n",
      "experts per step: [3 3]\n",
      "halt probs: [0.    0.512]\n",
      "pred label: 0 true label: 0\n",
      "========= Test sample 117 =========\n",
      "experts per step: [1 1 1]\n",
      "halt probs: [0.     0.4917 0.4927]\n",
      "pred label: 9 true label: 4\n",
      "========= Test sample 117 =========\n",
      "experts per step: [1 1 1]\n",
      "halt probs: [0.     0.4917 0.4927]\n",
      "pred label: 9 true label: 4\n",
      "========= Test sample 118 =========\n",
      "experts per step: [3 3]\n",
      "halt probs: [0.    0.526]\n",
      "pred label: 6 true label: 2\n",
      "========= Test sample 118 =========\n",
      "experts per step: [3 3]\n",
      "halt probs: [0.    0.526]\n",
      "pred label: 6 true label: 2\n",
      "========= Test sample 119 =========\n",
      "experts per step: [1 1 1]\n",
      "halt probs: [0.     0.4858 0.4902]\n",
      "pred label: 7 true label: 7\n",
      "========= Test sample 119 =========\n",
      "experts per step: [1 1 1]\n",
      "halt probs: [0.     0.4858 0.4902]\n",
      "pred label: 7 true label: 7\n"
     ]
    }
   ],
   "source": [
    "for i in range(100, 120, 1):\n",
    "    res = trace_and_predict(model, x_test[i:i+1], y_true=y_test[i:i+1], layer_name=\"adaptive_router\")\n",
    "\n",
    "    trace = res[\"trace\"]\n",
    "    print(f\"========= Test sample {i} =========\")\n",
    "    print(\"experts per step:\", trace[\"top_indices\"][:, 0])\n",
    "    print(\"halt probs:\", trace[\"halt_soft\"][:, 0])\n",
    "    print(\"pred label:\", res[\"pred_label\"][0], \"true label:\", int(res[\"true_label\"][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'pool_every_n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_adaptive_model_sparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mponder_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# >0 encourages fewer steps; tune as needed\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhalt_temp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroute_temp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool_every_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m                 loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m                 metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[0;32mIn [4], line 264\u001b[0m, in \u001b[0;36mbuild_adaptive_model_sparse\u001b[0;34m(input_shape, num_classes, filters, min_steps, max_steps, ponder_lambda, route_temp, halt_temp, pool_every_n)\u001b[0m\n\u001b[1;32m    260\u001b[0m pools \u001b[38;5;241m=\u001b[39m [PoolingLayer(filters\u001b[38;5;241m=\u001b[39mfilters, frac_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mpool_every_n)]\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Prefer simple/fast pooling (or a stride-2 Conv2D inside PoolingLayer)\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m#pooling  = PoolingLayer(filters=filters, frac_ratio=2.0)\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mAdaptiveRouterBlockTop1Vectorized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbranches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbranches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mponder_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mponder_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroute_temp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroute_temp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhalt_temp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhalt_temp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool_every_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_every_n\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madaptive_router\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m(x)\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mGlobalAveragePooling2D()(x)\n\u001b[1;32m    276\u001b[0m outputs \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)  \u001b[38;5;66;03m# keep fp32 logits\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'pool_every_n'"
     ]
    }
   ],
   "source": [
    "def build_base_model_4_blocks(input_shape=(32,32,3), num_classes=10, filters=32):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # For CNNs on GPU, BatchNorm is faster than LayerNorm:\n",
    "    x = layers.Conv2D(filters, 3, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = ResidualBlock(filters)(x)\n",
    "    x = PoolingLayer(filters=filters, frac_ratio=2.0)(x)\n",
    "    x = ResidualBlock(filters)(x)\n",
    "    x = ResidualBlock(filters)(x)\n",
    "    x = PoolingLayer(filters=filters, frac_ratio=2.0)(x)\n",
    "    x = ResidualBlock(filters)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # keep fp32 logits\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "def build_base_model_6_blocks(input_shape=(32,32,3), num_classes=10, filters=32):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # For CNNs on GPU, BatchNorm is faster than LayerNorm:\n",
    "    x = layers.Conv2D(filters, 3, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = ResidualBlock(filters)(x)\n",
    "    x = PoolingLayer(filters=filters, frac_ratio=2.0)(x)\n",
    "    x = ResidualBlock(filters)(x)\n",
    "    x = ResidualBlock(filters)(x)\n",
    "    x = PoolingLayer(filters=filters, frac_ratio=2.0)(x)\n",
    "    x = ResidualBlock(filters)(x)\n",
    "    x = ResidualBlock(filters)(x)\n",
    "    x = PoolingLayer(filters=filters, frac_ratio=2.0)(x)\n",
    "    x = ResidualBlock(filters)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # keep fp32 logits\n",
    "    return keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_61\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 32, 32, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " residual_block_4 (Residual  (None, 32, 32, 32)        20736     \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " pooling_layer_1 (PoolingLa  (None, 16, 16, 32)        1120      \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " residual_block_5 (Residual  (None, 16, 16, 32)        20736     \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " residual_block_6 (Residual  (None, 16, 16, 32)        20736     \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " pooling_layer_2 (PoolingLa  (None, 8, 8, 32)          1120      \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " residual_block_7 (Residual  (None, 8, 8, 32)          20736     \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 32)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86538 (338.04 KB)\n",
      "Trainable params: 86474 (337.79 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 16:17:22.540030: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 1.8282 - accuracy: 0.3055"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 16:17:42.444025: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 23s 189ms/step - loss: 1.8282 - accuracy: 0.3055 - val_loss: 2.3907 - val_accuracy: 0.1831\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - 15s 151ms/step - loss: 1.4866 - accuracy: 0.4441 - val_loss: 1.9441 - val_accuracy: 0.3035\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - 15s 148ms/step - loss: 1.2755 - accuracy: 0.5356 - val_loss: 1.9825 - val_accuracy: 0.3222\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - 14s 143ms/step - loss: 1.1392 - accuracy: 0.5867 - val_loss: 1.7945 - val_accuracy: 0.3760\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 1.0311 - accuracy: 0.6264 - val_loss: 1.2198 - val_accuracy: 0.5541\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.9601 - accuracy: 0.6540 - val_loss: 1.0745 - val_accuracy: 0.6036\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - 15s 148ms/step - loss: 0.9133 - accuracy: 0.6695 - val_loss: 1.0224 - val_accuracy: 0.6342\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.8544 - accuracy: 0.6936 - val_loss: 1.0272 - val_accuracy: 0.6400\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - 14s 146ms/step - loss: 0.8335 - accuracy: 0.6998 - val_loss: 0.9523 - val_accuracy: 0.6620\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - 14s 143ms/step - loss: 0.7940 - accuracy: 0.7143 - val_loss: 0.9144 - val_accuracy: 0.6762\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - 14s 145ms/step - loss: 0.7706 - accuracy: 0.7247 - val_loss: 0.8708 - val_accuracy: 0.6916\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.7407 - accuracy: 0.7330 - val_loss: 0.9664 - val_accuracy: 0.6532\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.7240 - accuracy: 0.7395 - val_loss: 0.9708 - val_accuracy: 0.6591\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - 14s 148ms/step - loss: 0.6902 - accuracy: 0.7525 - val_loss: 0.8552 - val_accuracy: 0.6935\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - 15s 149ms/step - loss: 0.6881 - accuracy: 0.7530 - val_loss: 0.8673 - val_accuracy: 0.6926\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.6585 - accuracy: 0.7642 - val_loss: 0.8753 - val_accuracy: 0.6875\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.6431 - accuracy: 0.7707 - val_loss: 0.7975 - val_accuracy: 0.7194\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.6122 - accuracy: 0.7807 - val_loss: 0.8450 - val_accuracy: 0.7071\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.5958 - accuracy: 0.7866 - val_loss: 0.8561 - val_accuracy: 0.7045\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.5752 - accuracy: 0.7955 - val_loss: 0.8019 - val_accuracy: 0.7204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x158b70f40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_base_model_4_blocks(\n",
    "    input_shape=(32,32,3),\n",
    "    num_classes=10,\n",
    "    filters=FILTERS\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=20, batch_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_62\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 32, 32, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " residual_block_8 (Residual  (None, 32, 32, 32)        20736     \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " pooling_layer_3 (PoolingLa  (None, 16, 16, 32)        1120      \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " residual_block_9 (Residual  (None, 16, 16, 32)        20736     \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " residual_block_10 (Residua  (None, 16, 16, 32)        20736     \n",
      " lBlock)                                                         \n",
      "                                                                 \n",
      " pooling_layer_4 (PoolingLa  (None, 8, 8, 32)          1120      \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " residual_block_11 (Residua  (None, 8, 8, 32)          20736     \n",
      " lBlock)                                                         \n",
      "                                                                 \n",
      " residual_block_12 (Residua  (None, 8, 8, 32)          20736     \n",
      " lBlock)                                                         \n",
      "                                                                 \n",
      " pooling_layer_5 (PoolingLa  (None, 4, 4, 32)          1120      \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " residual_block_13 (Residua  (None, 4, 4, 32)          20736     \n",
      " lBlock)                                                         \n",
      "                                                                 \n",
      " global_average_pooling2d_2  (None, 32)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 129130 (504.41 KB)\n",
      "Trainable params: 129066 (504.16 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 16:22:18.933112: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 1.7270 - accuracy: 0.3563"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 16:22:38.732527: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 23s 194ms/step - loss: 1.7270 - accuracy: 0.3563 - val_loss: 2.5958 - val_accuracy: 0.1778\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - 16s 158ms/step - loss: 1.3537 - accuracy: 0.5037 - val_loss: 1.6845 - val_accuracy: 0.3908\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - 16s 163ms/step - loss: 1.1777 - accuracy: 0.5730 - val_loss: 1.4184 - val_accuracy: 0.4790\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - 16s 163ms/step - loss: 1.0465 - accuracy: 0.6216 - val_loss: 1.1898 - val_accuracy: 0.5696\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - 16s 160ms/step - loss: 0.9614 - accuracy: 0.6537 - val_loss: 1.0755 - val_accuracy: 0.6178\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.8872 - accuracy: 0.6815 - val_loss: 1.0424 - val_accuracy: 0.6327\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - 15s 156ms/step - loss: 0.8318 - accuracy: 0.7016 - val_loss: 0.9382 - val_accuracy: 0.6660\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - 15s 158ms/step - loss: 0.7757 - accuracy: 0.7223 - val_loss: 0.9095 - val_accuracy: 0.6837\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - 15s 152ms/step - loss: 0.7305 - accuracy: 0.7394 - val_loss: 0.8830 - val_accuracy: 0.6898\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - 15s 152ms/step - loss: 0.6990 - accuracy: 0.7500 - val_loss: 0.8733 - val_accuracy: 0.6980\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - 15s 152ms/step - loss: 0.6628 - accuracy: 0.7628 - val_loss: 0.9028 - val_accuracy: 0.6890\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - 16s 161ms/step - loss: 0.6260 - accuracy: 0.7758 - val_loss: 0.8919 - val_accuracy: 0.6962\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - 17s 173ms/step - loss: 0.5841 - accuracy: 0.7946 - val_loss: 0.9160 - val_accuracy: 0.6944\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - 17s 168ms/step - loss: 0.5669 - accuracy: 0.7985 - val_loss: 0.9117 - val_accuracy: 0.6928\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - 15s 152ms/step - loss: 0.5437 - accuracy: 0.8070 - val_loss: 0.9037 - val_accuracy: 0.6988\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - 15s 155ms/step - loss: 0.5035 - accuracy: 0.8212 - val_loss: 0.8777 - val_accuracy: 0.7093\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - 15s 156ms/step - loss: 0.4814 - accuracy: 0.8289 - val_loss: 0.8785 - val_accuracy: 0.7127\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - 15s 152ms/step - loss: 0.4431 - accuracy: 0.8425 - val_loss: 0.9214 - val_accuracy: 0.7051\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - 15s 151ms/step - loss: 0.4327 - accuracy: 0.8460 - val_loss: 0.9194 - val_accuracy: 0.7124\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - 15s 153ms/step - loss: 0.3976 - accuracy: 0.8596 - val_loss: 0.9220 - val_accuracy: 0.7135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3b0a50370>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_base_model_6_blocks(\n",
    "    input_shape=(32,32,3),\n",
    "    num_classes=10,\n",
    "    filters=FILTERS\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=20, batch_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

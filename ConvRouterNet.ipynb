{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CGANs - Conditional Generative Adversarial Nets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras import mixed_precision\n",
    "#mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, Dense, GlobalAveragePooling2D, LayerNormalization, Add, DepthwiseConv2D, MaxPool2D\n",
    ")\n",
    "from utils import (\n",
    "    PoolingLayer, ResidualBlock, ResidualBlock3x3, ResidualBlock5x5, ResidualBlock7x7, SpatialSE, ChannelSE, \n",
    "    ResidualBlockDepthwise3x3, ResidualBlockDepthwise5x5, ResidualBlockDepthwise7x7, ResidualBlockDepthwise9x9, \n",
    "    DummyBlock, Conv3x3PoolingLayer, Depthwise3x3ConvPoolingLayer, MaxPoolingLayer, AvgPoolingLayer,\n",
    "    Conv5x5PoolingLayer, Depthwise5x5ConvPoolingLayer, Depthwise7x7ConvPoolingLayer\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Softmax Router (no Gumbel). Optional hard mode at inference.\n",
    "# ---------------------------------------------------------\n",
    "class SoftmaxRouter(layers.Layer):\n",
    "    def __init__(self, num_choices, hard_at_inference=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_choices = num_choices\n",
    "        self.hard_at_inference = hard_at_inference\n",
    "        self.logits_layer = Dense(num_choices)\n",
    "\n",
    "    def call(self, features, training=None):\n",
    "        logits = self.logits_layer(GlobalAveragePooling2D()(features))  # (B, K)\n",
    "        if training or not self.hard_at_inference:\n",
    "            probs = tf.nn.softmax(logits, axis=-1)                      # (B, K)\n",
    "        else:\n",
    "            idx = tf.argmax(logits, axis=-1)\n",
    "            probs = tf.one_hot(idx, depth=self.num_choices, dtype=tf.float32)\n",
    "        return probs  # (B, K)\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Hazard-rate Halting Head (ST hard gate; good gradients)\n",
    "# ------------------------------------------------------------\n",
    "class HazardHaltingHead(layers.Layer):\n",
    "    \"\"\"\n",
    "    Predicts a hazard p_t in (0,1) from features; uses a straight-through\n",
    "    hard decision in forward, with gradients from the sigmoid (Concrete).\n",
    "    call(...) returns:\n",
    "      p_soft: [B,1]  (prob of halting at this step, for gradients/metrics)\n",
    "      h_st  : [B,1]  (hard 0/1 gate with ST gradient)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden=64, halt_temp=1.0, tau=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = int(hidden)\n",
    "        self.halt_temp = float(halt_temp)\n",
    "        self.tau = float(tau)\n",
    "        print(f\"HazardHaltingHead: hidden={self.hidden}, halt_temp={self.halt_temp}, tau={self.tau}\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input can be [B,H,W,C] or [B,C]; we GAP if rank-4\n",
    "        if len(input_shape) == 4:\n",
    "            in_dim = int(input_shape[-1])\n",
    "        else:\n",
    "            in_dim = int(input_shape[-1])\n",
    "        if self.hidden > 0:\n",
    "            self.mlp = keras.Sequential([\n",
    "                layers.Dense(self.hidden, activation=\"swish\", use_bias=False),\n",
    "                layers.Dense(1, use_bias=False),\n",
    "            ])\n",
    "        else:\n",
    "            self.mlp = layers.Dense(1, use_bias=False) #, bias_initializer=tf.keras.initializers.Constant(-0.2)\n",
    "\n",
    "    def call(self, feat, *, can_halt_mask, training=None):\n",
    "        # feat: [B,H,W,C] or [B,C]\n",
    "        if len(feat.shape) == 4:\n",
    "            pooled = tf.reduce_mean(feat, axis=[1,2])  # [B,C]\n",
    "        else:\n",
    "            pooled = feat                               # [B,C]\n",
    "\n",
    "        logits = self.mlp(pooled, training=training)    # [B,1]\n",
    "        p_soft = tf.nn.sigmoid(logits / self.halt_temp) # [B,1]\n",
    "        p_soft = p_soft * can_halt_mask                 # respect min_steps\n",
    "\n",
    "        # straight-through hard gate\n",
    "        h_hard = tf.cast(p_soft > self.tau, p_soft.dtype)    # [B,1]\n",
    "        h_st   = h_hard + tf.stop_gradient(p_soft - h_hard)\n",
    "        return p_soft, h_st\n",
    "\n",
    "# ---------------------------\n",
    "# Tiny conv stem\n",
    "# ---------------------------\n",
    "class ConvStem(layers.Layer):\n",
    "    def __init__(self, out_ch, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.conv = layers.Conv2D(out_ch, 3, padding=\"same\", use_bias=False)\n",
    "        self.norm = layers.LayerNormalization()\n",
    "        self.act  = layers.Activation(\"swish\")\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x, training=training)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Multi-head attention pooling router\n",
    "# Produces logits over K experts\n",
    "# ---------------------------\n",
    "class AttnPoolRouter(layers.Layer):\n",
    "    def __init__(self, K, heads=2, dim_head=64, mlp_hidden=64, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.K = int(K)\n",
    "        self.heads = int(heads)\n",
    "        self.dim_head = int(dim_head)\n",
    "        self.mlp_hidden = int(mlp_hidden)\n",
    "\n",
    "        self.q = self.add_weight(\n",
    "            name=\"queries\", shape=(self.heads, self.dim_head),\n",
    "            initializer=\"glorot_uniform\", trainable=True)\n",
    "\n",
    "        self.key_proj = layers.Conv2D(self.heads*self.dim_head, 1, use_bias=False)\n",
    "        self.val_proj = layers.Conv2D(self.heads*self.dim_head, 1, use_bias=False)\n",
    "\n",
    "        # Head aggregator -> K logits\n",
    "        if self.mlp_hidden > 0:\n",
    "            self.head_mlp = keras.Sequential([\n",
    "                layers.Dense(self.mlp_hidden, activation=\"swish\", use_bias=False),\n",
    "                layers.Dense(self.K, use_bias=False)\n",
    "            ])\n",
    "        else:\n",
    "            self.head_mlp = layers.Dense(self.K, use_bias=False)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        # x: [B,H,W,C]\n",
    "        B, H, W, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "        k = self.key_proj(x)  # [B,H,W,heads*dim]\n",
    "        v = self.val_proj(x)\n",
    "        k = tf.reshape(k, [B, H*W, self.heads, self.dim_head])  # [B,HW,Hd,D]\n",
    "        v = tf.reshape(v, [B, H*W, self.heads, self.dim_head])  # [B,HW,Hd,D]\n",
    "        k = tf.transpose(k, [0,2,1,3])  # [B,heads,HW,dim]\n",
    "        v = tf.transpose(v, [0,2,1,3])  # [B,heads,HW,dim]\n",
    "\n",
    "        # queries: [heads, dim] -> [B,heads,1,dim]\n",
    "        q = tf.expand_dims(self.q, axis=0)\n",
    "        q = tf.expand_dims(q, axis=2)\n",
    "\n",
    "        # attn: [B,heads,1,HW]\n",
    "        attn = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(self.dim_head, x.dtype))\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "\n",
    "        # pooled heads: [B,heads,1,dim]\n",
    "        pooled = tf.matmul(attn, v)  # [B,heads,1,dim]\n",
    "        pooled = tf.squeeze(pooled, axis=2)  # [B,heads,dim]\n",
    "\n",
    "        # flatten heads\n",
    "        pooled = tf.reshape(pooled, [B, self.heads*self.dim_head])  # [B, heads*dim]\n",
    "\n",
    "        logits = self.head_mlp(pooled, training=training)  # [B,K]\n",
    "        return logits, pooled  # pooled can be used as a feature if needed\n",
    "\n",
    "\n",
    "class HaltingClassifierHead(layers.Layer):\n",
    "    \"\"\"\n",
    "    Predicts class probabilities; halts when max prob > tau.\n",
    "    ST gating: forward uses hard threshold; backward uses a smooth sigmoid around tau.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, hidden=64, halt_temp=3.0, tau=0.8, bias_init=-2.5, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.num_classes = int(num_classes)\n",
    "        self.hidden = int(hidden)\n",
    "        self.halt_temp = float(halt_temp)\n",
    "        self.tau = float(tau)\n",
    "\n",
    "        if hidden > 0:\n",
    "            self.classifier = keras.Sequential([\n",
    "                layers.GlobalAveragePooling2D(),\n",
    "                layers.Dense(hidden, activation=\"swish\", use_bias=False),\n",
    "                layers.Dense(num_classes, use_bias=False)\n",
    "            ])\n",
    "        else:\n",
    "            self.classifier = keras.Sequential([\n",
    "                layers.GlobalAveragePooling2D(),\n",
    "                layers.Dense(num_classes, use_bias=False)\n",
    "            ])\n",
    "\n",
    "        # a tiny scalar bias we add to (max_prob - tau) before the sigmoid\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"halt_bias\", shape=(), initializer=tf.keras.initializers.Constant(bias_init),\n",
    "            trainable=True)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if len(x.shape) == 2:\n",
    "            x = tf.expand_dims(x, axis=1)\n",
    "            x = tf.expand_dims(x, axis=1)\n",
    "        elif len(x.shape) == 3:\n",
    "            x = tf.expand_dims(x, axis=1)\n",
    "        logits = self.classifier(x, training=training)         # [B,C]\n",
    "        probs  = tf.nn.softmax(logits, axis=-1)                 # [B,C]\n",
    "        maxp   = tf.reduce_max(probs, axis=-1, keepdims=True)   # [B,1]\n",
    "        tau = tf.cast(self.tau, maxp.dtype)\n",
    "        halt_temp = tf.cast(self.halt_temp, maxp.dtype)\n",
    "        z = (maxp - tau) / tf.maximum(tf.constant(1e-6, dtype=maxp.dtype), halt_temp)\n",
    "        p_soft = tf.nn.sigmoid(z + tf.cast(self.bias, maxp.dtype))                   # [B,1]\n",
    "        p_hard = tf.cast(maxp > tau, x.dtype)              # [B,1]\n",
    "        p_st = p_hard + tf.stop_gradient(p_soft - p_hard)\n",
    "        return probs, p_soft, p_hard, p_st   # class probs, soft gate, hard gate, ST gate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 10:08:47.722176: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-10-18 10:08:47.722207: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-10-18 10:08:47.722212: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-10-18 10:08:47.722445: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-10-18 10:08:47.722464: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# simple CIFAR-10 aug\n",
    "def cifar_preprocess(x, y):\n",
    "    x = tf.image.resize_with_crop_or_pad(x, 36, 36)\n",
    "    x = tf.image.random_crop(x, [tf.shape(x)[0], 32, 32, 3])\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    return x, y\n",
    "\n",
    "def make_dataset(x, y, batch=128, train=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    if train:\n",
    "        ds = ds.shuffle(5000).batch(batch).map(cifar_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        ds = ds.batch(batch)\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Usage\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype(\"float32\")/255.0; x_test = x_test.astype(\"float32\")/255.0\n",
    "y_train = y_train.flatten(); y_test = y_test.flatten()\n",
    "\n",
    "\n",
    "ds_train = make_dataset(x_train, y_train, batch=128, train=True)\n",
    "ds_val   = make_dataset(x_test, y_test, batch=256, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRouterBlockTop1Vectorized(layers.Layer):\n",
    "    \"\"\"\n",
    "    Deterministic Top-1 routing (same in train & infer) with ST grads.\n",
    "    Pooling is disabled: no pooling layers or events are used.\n",
    "    Allows selection of halting head: HazardHaltingHead or HaltingClassifierHead.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        branches,\n",
    "        min_steps=1,\n",
    "        max_steps=5,\n",
    "        route_temp=1.0,\n",
    "        router_settings={\n",
    "            \"heads\": 2,\n",
    "            \"dim_head\": 64,\n",
    "            \"mlp_hidden\": 0\n",
    "        },\n",
    "        halt_settings={\n",
    "            \"hidden\": 64,\n",
    "            \"temp\": 1.0,\n",
    "            \"ponder_lambda\": 1e-4,\n",
    "            \"type\": \"hazard\",  # \"hazard\" or \"classifier\"\n",
    "            \"num_classes\": None,  # Only needed for classifier head\n",
    "            \"tau\": 0.8,          # Only for classifier head\n",
    "            \"bias_init\": -2.5,   # Only for classifier head\n",
    "        },\n",
    "        name=None\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "        assert 1 <= min_steps <= max_steps\n",
    "        self.branches = branches\n",
    "        self.K = len(branches)\n",
    "        self.router = AttnPoolRouter(\n",
    "            K=self.K,\n",
    "            dim_head=router_settings.get(\"dim_head\", 64),\n",
    "            mlp_hidden=router_settings.get(\"mlp_hidden\", 0)\n",
    "        )\n",
    "        self._route_temp = float(route_temp)\n",
    "        self.ponder_lambda = float(halt_settings.get(\"ponder_lambda\", 0.5)) \n",
    "        self.min_steps = int(min_steps)\n",
    "        self.max_steps = int(max_steps)\n",
    "\n",
    "        # Select halting head type (unchanged)\n",
    "        halt_type = halt_settings.get(\"type\", \"hazard\")\n",
    "        if halt_type == \"classifier\":\n",
    "            num_classes = halt_settings.get(\"num_classes\", 10)\n",
    "            tau = halt_settings.get(\"tau\", 0.5)\n",
    "            bias_init = halt_settings.get(\"bias_init\", -2.5)\n",
    "            hidden = halt_settings.get(\"hidden\", 64)\n",
    "            halt_temp = float(halt_settings.get(\"temp\", 1.0))\n",
    "            self.halt = HaltingClassifierHead(\n",
    "                num_classes=num_classes,\n",
    "                hidden=hidden,\n",
    "                halt_temp=halt_temp,\n",
    "                tau=tau,\n",
    "                bias_init=bias_init\n",
    "            )\n",
    "        else:\n",
    "            hidden = halt_settings.get(\"hidden\", 64)\n",
    "            tau = halt_settings.get(\"tau\", 0.5)\n",
    "            halt_temp = float(halt_settings.get(\"temp\", 1.0))\n",
    "            self.halt = HazardHaltingHead(\n",
    "                hidden=hidden,\n",
    "                halt_temp=halt_temp,\n",
    "                tau=tau\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def route_temp(self):\n",
    "        return self._route_temp\n",
    "\n",
    "    @route_temp.setter\n",
    "    def route_temp(self, v: float):\n",
    "        self._route_temp = float(v)\n",
    "\n",
    "    @property\n",
    "    def halt_temp(self):\n",
    "        return self.halt.halt_temp\n",
    "\n",
    "    @halt_temp.setter\n",
    "    def halt_temp(self, v: float):\n",
    "        self.halt.halt_temp = float(v)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update(dict(\n",
    "            min_steps=self.min_steps,\n",
    "            max_steps=self.max_steps,\n",
    "            ponder_lambda=self.ponder_lambda,\n",
    "            route_temp=self.route_temp,\n",
    "            halt_temp=self.halt_temp,\n",
    "            K=self.K\n",
    "        ))\n",
    "        return cfg\n",
    "\n",
    "    def _diversity_from_means(self, V):\n",
    "        \"\"\"\n",
    "        Computes diversity among branch outputs V (shape [K, ...]).\n",
    "        Higher value means less diversity (more similar branches).\n",
    "        Can be used as a regularization loss to encourage diversity.\n",
    "        \"\"\"\n",
    "        V = tf.reshape(V, [tf.shape(V)[0], -1])  # flatten each branch output\n",
    "        V = tf.nn.l2_normalize(V, axis=-1)       # normalize\n",
    "        sims = tf.matmul(V, V, transpose_b=True) # [K, K] cosine similarity\n",
    "        K = tf.shape(V)[0]\n",
    "        mask = 1.0 - tf.eye(K, dtype=V.dtype)    # zero diagonal\n",
    "        denom = tf.reduce_sum(mask)\n",
    "        return tf.where(denom > 0, tf.reduce_sum(sims * mask) / denom, 0.0)\n",
    "    \n",
    "\n",
    "    def call(self, features, training=None):\n",
    "        x = features\n",
    "        div_loss = tf.constant(0.0, dtype=x.dtype)\n",
    "        B = tf.shape(x)[0]\n",
    "        dtype = x.dtype\n",
    "        halted = tf.zeros([B,1,1,1], dtype=dtype)\n",
    "        ponder_cost = tf.constant(0.0, dtype=dtype)\n",
    "        steps_taken = tf.zeros([B], dtype=dtype)\n",
    "\n",
    "        for t in range(self.max_steps):\n",
    "            router_logits, _ = self.router(x, training=training)              # [B,K]\n",
    "            probs  = tf.nn.softmax(router_logits / self.route_temp, axis=-1)  # [B,K]\n",
    "            top_idx = tf.argmax(probs, axis=-1, output_type=tf.int32)         # [B]\n",
    "            onehot  = tf.one_hot(top_idx, depth=self.K, dtype=dtype)          # [B,K]\n",
    "            onehot_st = onehot + tf.stop_gradient(probs - onehot)             # ST\n",
    "\n",
    "            y_list = [br(x, training=training) for br in self.branches]       # K x [B,H,W,C]\n",
    "            y_means = tf.stack([tf.reduce_mean(y, axis=[1,2,3]) for y in y_list], axis=0)  # [K,B]\n",
    "            div_loss += self._diversity_from_means(y_means)\n",
    "\n",
    "            y_stack = tf.stack(y_list, axis=1)                                 # [B,K,H,W,C]\n",
    "            mask = tf.reshape(onehot_st, [-1, self.K, 1, 1, 1])\n",
    "            y_sel = tf.reduce_sum(mask * y_stack, axis=1)                      # [B,H,W,C]\n",
    "\n",
    "            can_halt = tf.cast(t >= self.min_steps - 1, dtype) * tf.ones([B,1], dtype)\n",
    "            if isinstance(self.halt, HazardHaltingHead):\n",
    "                p_soft, h_st = self.halt(y_sel, can_halt_mask=can_halt, training=training)  # [B,1]\n",
    "            else:  # HaltingClassifierHead\n",
    "                _, p_soft, _, h_st = self.halt(y_sel, training=training)       # [B,1]\n",
    "                p_soft = p_soft * can_halt\n",
    "                h_st = h_st * can_halt\n",
    "\n",
    "            h_st4 = tf.reshape(h_st, [-1,1,1,1])\n",
    "            halted = tf.clip_by_value(halted + (1.0 - halted) * h_st4, 0.0, 1.0)\n",
    "            not_halted = 1.0 - tf.squeeze(halted, [1,2,3])  # [B]\n",
    "            steps_taken += not_halted\n",
    "\n",
    "            x = y_sel\n",
    "\n",
    "            if training and self.ponder_lambda > 0.0:\n",
    "                running_frac = tf.reduce_mean(1.0 - tf.squeeze(halted, [1,2,3]))\n",
    "                ponder_cost += tf.cast(running_frac, dtype)\n",
    "\n",
    "        if training and self.ponder_lambda > 0.0:\n",
    "            self.add_loss(self.ponder_lambda * ponder_cost)\n",
    "            self.add_loss(0.01 * div_loss / self.max_steps)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Cosine annealing learning rate scheduler.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_lr, min_lr, epochs, verbose=1):\n",
    "        super().__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        p = epoch / max(1, self.epochs - 1)\n",
    "        lr = self.min_lr + 0.5 * (self.base_lr - self.min_lr) * (1 + np.cos(np.pi * p))\n",
    "        keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose and (epoch < 1 or (epoch + 1) % 5 == 0):\n",
    "            print(f\"> [LR Scheduler] epoch {epoch+1}: lr={lr:.6f}\")\n",
    "\n",
    "\n",
    "class TempScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Linearly (or cosine) anneal route_temp and halt_temp over epochs.\n",
    "    route: 1.5 -> 0.7\n",
    "    halt:  3.0 -> 1.5\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_name=\"adaptive_router\",\n",
    "                 route_start=1.5, route_end=0.7,\n",
    "                 halt_start=3.0,  halt_end=0.5,\n",
    "                 epochs=150, mode=\"cosine\"):\n",
    "        super().__init__()\n",
    "        self.layer_name = layer_name\n",
    "        self.rs, self.re = float(route_start), float(route_end)\n",
    "        self.hs, self.he = float(halt_start),  float(halt_end)\n",
    "        self.E = int(epochs)\n",
    "        self.mode = mode\n",
    "\n",
    "    def _interp(self, e):\n",
    "        p = min(1.0, e / max(1, self.E-1))\n",
    "        if self.mode == \"cosine\":\n",
    "            p = 0.5*(1 - np.cos(np.pi*p))\n",
    "        return p\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        p = self._interp(epoch)\n",
    "        rtemp = self.rs + (self.re - self.rs)*p\n",
    "        htemp = self.hs + (self.he - self.hs)*p\n",
    "        layer = self.model.get_layer(self.layer_name)\n",
    "        layer.route_temp = rtemp\n",
    "        layer.halt_temp  = htemp\n",
    "        if epoch < 1 or (epoch + 1) % 5 == 0:\n",
    "            print(f\"> [TempScheduler] epoch {epoch+1}: route_temp={rtemp:.3f}, halt_temp={htemp:.3f}\")\n",
    "\n",
    "\n",
    "class RouterStatsCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, x_val, y_val, layer_name=\"adaptive_router\", batch_size=256):\n",
    "        super().__init__()\n",
    "        self.xv = x_val\n",
    "        self.yv = y_val\n",
    "        self.layer_name = layer_name\n",
    "        self.bs = batch_size\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch < 1 or (epoch + 1) % 5 == 0:\n",
    "            layer = self.model.get_layer(self.layer_name)\n",
    "            T, K = layer.max_steps, layer.K\n",
    "            steps_hist = np.zeros(T+1, np.int64)\n",
    "            expert_hist = np.zeros((T, K), np.int64)\n",
    "            n = len(self.xv)\n",
    "            for i in range(0, n, self.bs):\n",
    "                xb = self.xv[i:i+self.bs]\n",
    "                tb = trace_batch(self.model, xb, layer_name=self.layer_name, force_full=False)\n",
    "                t_used = tb[\"top_indices\"].shape[0]\n",
    "                running = tb[\"running_after\"]  # [t_used, B]\n",
    "                stopped = ~running\n",
    "                ever = stopped.any(axis=0)\n",
    "                first = np.argmax(stopped, axis=0)\n",
    "                used = np.where(ever, first+1, t_used)\n",
    "                for s in used:\n",
    "                    steps_hist[min(int(s), T)] += 1\n",
    "                for t in range(t_used):\n",
    "                    ch = tb[\"top_indices\"][t]\n",
    "                    cnt = np.bincount(ch, minlength=K)\n",
    "                    expert_hist[t] += cnt\n",
    "            avg_steps = np.sum(np.arange(T+1)*steps_hist)/max(1, steps_hist.sum())\n",
    "            print(f\"> [{self.layer_name}] epoch {epoch+1}: avg_steps={avg_steps:.2f}  steps_hist={steps_hist.tolist()} expert_hist={expert_hist.tolist()}\")\n",
    "\n",
    "\n",
    "class AdaptiveTauCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, layer_name, target_steps, update_rate=0.01):\n",
    "        super().__init__()\n",
    "        self.layer_name = layer_name\n",
    "        self.target_steps = target_steps\n",
    "        self.update_rate = update_rate\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch == 1 or (epoch + 1) % 5 == 0:\n",
    "            layer = self.model.get_layer(self.layer_name)\n",
    "            # Estimate average steps from validation set\n",
    "            stats = evaluate_with_router_stats(self.model, x_test, y_test, layer_name=self.layer_name)\n",
    "            avg_steps = np.sum(np.arange(stats[\"T\"]+1)*stats[\"steps_hist\"])/max(1, stats[\"steps_hist\"].sum())\n",
    "            # Adjust tau: increase tau if too many steps, decrease if too few\n",
    "            if hasattr(layer.halt, \"tau\"):\n",
    "                delta = self.update_rate * (avg_steps - self.target_steps) * abs(avg_steps - self.target_steps)\n",
    "                layer.halt.tau = np.clip(layer.halt.tau - delta, 0.01, 0.99)\n",
    "                print(f\"> [AdaptiveTau] epoch {epoch+1}: tau={layer.halt.tau:.3f}, avg_steps={avg_steps:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def trace_and_predict(\n",
    "    model,\n",
    "    x_input,\n",
    "    y_true=None,\n",
    "    layer_name=\"adaptive_router\",\n",
    "    force_full=False,   # set True to always loop max_steps, no early exit\n",
    "):\n",
    "    layer = model.get_layer(layer_name)\n",
    "    pre = keras.Model(model.input, layer.input)\n",
    "    x_in = tf.convert_to_tensor(x_input)\n",
    "    x = pre(x_in, training=False)\n",
    "    B = int(x.shape[0])\n",
    "    K, T = layer.K, layer.max_steps\n",
    "    dtype = x.dtype\n",
    "    running = np.ones((B,), dtype=bool)\n",
    "\n",
    "    top_indices, probs_list = [], []\n",
    "    halt_soft_list, halt_hard_list, running_after_list = [], [], []\n",
    "\n",
    "    for t in range(T):\n",
    "        router_logits, _ = layer.router(x, training=False)                         # [B,K]\n",
    "        probs = tf.nn.softmax(router_logits / layer.route_temp, axis=-1)           # [B,K]\n",
    "        top_idx = tf.argmax(probs, axis=-1, output_type=tf.int32)                  # [B]\n",
    "        onehot  = tf.one_hot(top_idx, depth=K, dtype=dtype)\n",
    "        onehot_st = onehot + tf.stop_gradient(probs - onehot)                      # ST\n",
    "\n",
    "        y_list = [br(x, training=False) for br in layer.branches]\n",
    "        y_stack = tf.stack(y_list, axis=1)                                         # [B,K,H,W,C]\n",
    "        mask = tf.reshape(onehot_st, [-1, K, 1, 1, 1])\n",
    "        y_sel = tf.reduce_sum(mask * y_stack, axis=1)                              # [B,H,W,C]\n",
    "\n",
    "        can_halt = (t >= layer.min_steps - 1)\n",
    "        can_mask = tf.ones([B,1], dtype) if can_halt else tf.zeros([B,1], dtype)\n",
    "\n",
    "        # --- FIX: handle halting head type ---\n",
    "        if isinstance(layer.halt, HazardHaltingHead):\n",
    "            p_soft, h_st = layer.halt(y_sel, can_halt_mask=can_mask, training=False)   # [B,1]\n",
    "        else:  # HaltingClassifierHead\n",
    "            _, p_soft, _, h_st = layer.halt(y_sel, training=False)                     # [B,1]\n",
    "            p_soft = p_soft * can_mask\n",
    "            h_st = h_st * can_mask\n",
    "\n",
    "        tau = float(layer.halt.tau)  # Always use the tau from halt_settings\n",
    "        p_np = tf.squeeze(p_soft, axis=1).numpy()                                  # [B]\n",
    "        h_st_np = tf.squeeze(h_st, axis=1).numpy()                                 # [B]\n",
    "        halt_this = (p_np > tau) & running & can_halt                              # [B] bool\n",
    "\n",
    "        running = running & (~halt_this)\n",
    "        x = y_sel  # no pooling\n",
    "\n",
    "        top_indices.append(top_idx.numpy())\n",
    "        probs_list.append(probs.numpy())\n",
    "        halt_soft_list.append(p_np.copy())\n",
    "        halt_hard_list.append(halt_this.astype(np.float32))\n",
    "        running_after_list.append(running.copy())\n",
    "\n",
    "        if (not force_full) and (not running.any()):\n",
    "            break\n",
    "\n",
    "    pred_probs = model(x_in, training=False).numpy()\n",
    "    pred_label = pred_probs.argmax(axis=-1).astype(np.int32)\n",
    "    if y_true is not None:\n",
    "        y_true_arr = np.asarray(y_true).reshape(-1)\n",
    "        correct = (pred_label == y_true_arr)\n",
    "    else:\n",
    "        correct = None\n",
    "\n",
    "    trace = {\n",
    "        \"top_indices\":   np.array(top_indices),                 # [t_used, B]\n",
    "        \"probs\":         np.array(probs_list),                  # [t_used, B, K]\n",
    "        \"halt_soft\":     np.array(halt_soft_list),              # [t_used, B]\n",
    "        \"halt_hard\":     np.array(halt_hard_list),              # [t_used, B]\n",
    "        \"running_after\": np.array(running_after_list, dtype=bool),  # [t_used, B]\n",
    "    }\n",
    "    return {\n",
    "        \"trace\": trace,\n",
    "        \"pred_probs\": pred_probs,\n",
    "        \"pred_label\": pred_label,\n",
    "        \"true_label\": None if y_true is None else np.asarray(y_true),\n",
    "        \"layer_info\": {\n",
    "            \"min_steps\": layer.min_steps,\n",
    "            \"max_steps\": layer.max_steps,\n",
    "            \"K\": layer.K,\n",
    "            \"route_temp\": getattr(layer, \"route_temp\", None),\n",
    "            \"halt_temp\": getattr(getattr(layer, \"halt\", None), \"halt_temp\", None),\n",
    "            \"unique_pools\": bool(getattr(layer, \"_pools\", []) not in (None, [])),\n",
    "            \"num_pools\": len(getattr(layer, \"_pools\", []) or []),\n",
    "        },\n",
    "        \"correct\": correct,\n",
    "    }\n",
    "\n",
    "\n",
    "def trace_batch(model, x_batch, layer_name=\"adaptive_router\", force_full=False):\n",
    "    layer = model.get_layer(layer_name)\n",
    "    pre = keras.Model(model.input, layer.input)\n",
    "\n",
    "    x_in = tf.convert_to_tensor(x_batch)\n",
    "    x = pre(x_in, training=False)\n",
    "\n",
    "    B = int(x.shape[0])\n",
    "    K, T = layer.K, layer.max_steps\n",
    "    running = np.ones((B,), dtype=bool)\n",
    "\n",
    "    top_indices, probs_list = [], []\n",
    "    halt_soft_list, halt_hard_list, running_after_list = [], [], []\n",
    "\n",
    "    for t in range(T):\n",
    "        router_logits, _ = layer.router(x, training=False)                         # [B,K]\n",
    "        probs = tf.nn.softmax(router_logits / layer.route_temp, axis=-1)           # [B,K]\n",
    "        top_idx = tf.argmax(probs, axis=-1, output_type=tf.int32)                  # [B]\n",
    "        onehot  = tf.one_hot(top_idx, depth=K, dtype=x.dtype)\n",
    "        onehot_st = onehot + tf.stop_gradient(probs - onehot)                      # ST\n",
    "\n",
    "        y_stack = tf.stack([br(x, training=False) for br in layer.branches], axis=1)  # [B,K,H,W,C]\n",
    "        y_sel = tf.reduce_sum(tf.reshape(onehot_st, [-1, K, 1, 1, 1]) * y_stack, axis=1)\n",
    "\n",
    "        can_halt = (t >= layer.min_steps - 1)\n",
    "        can_mask = tf.ones([B,1], x.dtype) if can_halt else tf.zeros([B,1], x.dtype)\n",
    "\n",
    "        # --- FIX: handle halting head type ---\n",
    "        if isinstance(layer.halt, HazardHaltingHead):\n",
    "            p_soft, h_st = layer.halt(y_sel, can_halt_mask=can_mask, training=False)      # [B,1]\n",
    "        else:  # HaltingClassifierHead\n",
    "            _, p_soft, _, h_st = layer.halt(y_sel, training=False)                        # [B,1]\n",
    "            p_soft = p_soft * can_mask\n",
    "            h_st = h_st * can_mask\n",
    "\n",
    "        tau = float(layer.halt.tau)  # Always use the tau from halt_settings\n",
    "        p_np = tf.squeeze(p_soft, axis=1).numpy()                                   # [B]\n",
    "        halt_this = (p_np > tau) & running & can_halt                               # [B] bool\n",
    "\n",
    "        running = running & (~halt_this)\n",
    "\n",
    "        x = y_sel  # no pooling\n",
    "\n",
    "        # ---- collect step data ----\n",
    "        top_indices.append(top_idx.numpy())\n",
    "        probs_list.append(probs.numpy())\n",
    "        halt_soft_list.append(p_np.copy())\n",
    "        halt_hard_list.append(halt_this.astype(np.float32))\n",
    "        running_after_list.append(running.copy())\n",
    "\n",
    "        # early exit only if no sample is still running\n",
    "        if (not force_full) and (not running.any()):\n",
    "            break\n",
    "\n",
    "    trace = {\n",
    "        \"top_indices\":   np.array(top_indices),                 # [t_used, B]\n",
    "        \"probs\":         np.array(probs_list),                  # [t_used, B, K]\n",
    "        \"halt_soft\":     np.array(halt_soft_list),              # [t_used, B]\n",
    "        \"halt_hard\":     np.array(halt_hard_list),              # [t_used, B]\n",
    "        \"running_after\": np.array(running_after_list, dtype=bool),  # [t_used, B]\n",
    "    }\n",
    "    return trace\n",
    "\n",
    "\n",
    "def steps_used_from_running(running_after):\n",
    "    \"\"\"\n",
    "    running_after: [t_used, B] bool (True = still running after that step)\n",
    "    Returns: [B] int steps used (first time running becomes False; else t_used)\n",
    "    \"\"\"\n",
    "    t_used, B = running_after.shape\n",
    "    # A sample stops running the step it halts; so steps_used is the first index\n",
    "    # where running becomes False, +1. If never False, it's t_used.\n",
    "    stopped = ~running_after\n",
    "    ever_stopped = stopped.any(axis=0)\n",
    "    first_stop = np.argmax(stopped, axis=0)  # undefined when never stopped, fine below\n",
    "    return np.where(ever_stopped, first_stop + 1, t_used)\n",
    "\n",
    "\n",
    "def evaluate_with_router_stats(model, x, y, layer_name=\"adaptive_router_top1\",\n",
    "                               batch_size=256, force_full=False):\n",
    "    # accuracy\n",
    "    loss, acc = model.evaluate(x, y, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # router stats\n",
    "    layer = model.get_layer(layer_name)\n",
    "    K, T = layer.K, layer.max_steps\n",
    "\n",
    "    steps_hist = np.zeros(T+1, dtype=np.int64)   # index t for steps=t, last bin for \"T or more\"\n",
    "    expert_hist = np.zeros((T, K), dtype=np.int64)\n",
    "    halt_rate   = np.zeros(T, dtype=np.float64)\n",
    "    n_seen = 0\n",
    "\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        xb = x[i:i+batch_size]\n",
    "        tb = trace_batch(model, xb, layer_name=layer_name, force_full=force_full)\n",
    "\n",
    "        t_used, B = tb[\"top_indices\"].shape[0], tb[\"top_indices\"].shape[1]\n",
    "        n_seen += B\n",
    "\n",
    "        # expert usage per step (only for steps that exist in this batch)\n",
    "        for t in range(t_used):\n",
    "            choices = tb[\"top_indices\"][t]  # [B]\n",
    "            counts = np.bincount(choices, minlength=K)\n",
    "            expert_hist[t, :] += counts\n",
    "            halt_rate[t] += tb[\"halt_hard\"][t].mean()\n",
    "\n",
    "        # steps used per sample\n",
    "        steps_used = steps_used_from_running(tb[\"running_after\"])  # [B]\n",
    "        # cap into histogram (if force_full=False, some batches may stop early)\n",
    "        for s in steps_used:\n",
    "            s_idx = min(int(s), T)  # put \"==T\" also into T bin\n",
    "            steps_hist[s_idx] += 1\n",
    "\n",
    "    # normalize\n",
    "    halt_rate[:t_used] = halt_rate[:t_used] / max(1, (len(x) + batch_size - 1) // batch_size)\n",
    "\n",
    "    return {\n",
    "        \"loss\": float(loss),\n",
    "        \"acc\": float(acc),\n",
    "        \"steps_hist\": steps_hist,    # length T+1\n",
    "        \"expert_hist\": expert_hist,  # [T, K]\n",
    "        \"halt_rate\": halt_rate,      # [T] avg hard halts at step t\n",
    "        \"seen\": n_seen,\n",
    "        \"K\": K,\n",
    "        \"T\": T,\n",
    "    }\n",
    "\n",
    "def print_router_stats(model, x, y, layer_name=\"adaptive_router_1\", batch_size=512, force_full=False):\n",
    "    stats = evaluate_with_router_stats(\n",
    "        model, x, y,\n",
    "        layer_name=layer_name,\n",
    "        batch_size=batch_size,\n",
    "        force_full=force_full\n",
    "    )\n",
    "    print(\"\")\n",
    "    print(f\"================== {layer_name} ====================\")\n",
    "    print(f\"Test acc: {stats['acc']*100:.2f}%  |  samples: {stats['seen']}\")\n",
    "    print(\"Steps histogram (0..T; last bin = T):\", stats[\"steps_hist\"])\n",
    "    # Only print halt rates for steps with expert usage\n",
    "    valid_steps = np.nonzero(stats['expert_hist'].sum(axis=1) > 0)[0]\n",
    "    if len(valid_steps) > 0:\n",
    "        last_step = valid_steps[-1] + 1\n",
    "        print(\"Halt rate per step:\", np.round(stats[\"halt_rate\"][:last_step], 3))\n",
    "    else:\n",
    "        print(\"Halt rate per step:\", stats[\"halt_rate\"])\n",
    "    for k in range(stats[\"K\"]):\n",
    "        total = stats[\"expert_hist\"][:,k].sum()\n",
    "        print(f\"Expert {k} total usage: {total} ({total/stats['seen']:.3f} per sample on avg)\")\n",
    "    for s in range(len(stats[\"expert_hist\"])):\n",
    "        step_total = stats[\"expert_hist\"][s].sum()\n",
    "        print(f\"Expert usage at step {s}:\", stats[\"expert_hist\"][s])\n",
    "\n",
    "\n",
    "def print_trace_for_samples(model, x, y, layer_name=\"adaptive_router_1\", start=1000, end=1020):\n",
    "    \"\"\"\n",
    "    Prints routing trace and prediction for samples in the given range.\n",
    "    \"\"\"\n",
    "    print(\"\")\n",
    "    print(f\"================== {layer_name} ====================\")\n",
    "    for i in range(start, end):\n",
    "        res = trace_and_predict(model, x[i:i+1], y_true=y[i:i+1], layer_name=layer_name)\n",
    "        if len(res[\"trace\"][\"top_indices\"][:, 0]) > 0:\n",
    "            trace = res[\"trace\"]\n",
    "            print(\" > pred label:\", res[\"pred_label\"][0], \"true label:\", int(res[\"true_label\"][0]))\n",
    "            print(\"   experts per step:\", trace[\"top_indices\"][:, 0])\n",
    "            print(\"   halt probs:\", trace[\"halt_soft\"][:, 0])\n",
    "                  \n",
    "\n",
    "\n",
    "def build_adaptive_model_sparse(branches, pool_branches, input_shape=(32,32,3), num_classes=10, filters=32,\n",
    "                                min_steps=1, max_steps=5,\n",
    "                                route_temp=1.0, router_settings={\"head\": 2, \"dim_head\": 64, \"mlp_hidden\": 0}, \n",
    "                                halt_settings={\"hidden\": 64, \"temp\": 1.0}):\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # For CNNs on GPU, BatchNorm is faster than LayerNorm:\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='swish')(inputs)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    x = MaxPoolingLayer(filters=filters)(x)\n",
    "\n",
    "    x = ResidualBlock3x3(filters=filters)(x)\n",
    "\n",
    "    x = AdaptiveRouterBlockTop1Vectorized(\n",
    "        branches=branches[0],\n",
    "        min_steps=min_steps,\n",
    "        max_steps=max_steps,\n",
    "        route_temp=route_temp,\n",
    "        router_settings=router_settings,\n",
    "        halt_settings=halt_settings[1],\n",
    "        name=\"adaptive_router_1\",\n",
    "    )(x)\n",
    "\n",
    "    x = AdaptiveRouterBlockTop1Vectorized(\n",
    "        branches=pool_branches[0],\n",
    "        min_steps=1,\n",
    "        max_steps=1,\n",
    "        route_temp=route_temp,\n",
    "        router_settings=router_settings,\n",
    "        halt_settings=halt_settings[0],\n",
    "        name=\"adaptive_router_pool_1\",\n",
    "    )(x)\n",
    "\n",
    "    x = AdaptiveRouterBlockTop1Vectorized(\n",
    "        branches=branches[1],\n",
    "        min_steps=min_steps,\n",
    "        max_steps=max_steps,\n",
    "        route_temp=route_temp,\n",
    "        router_settings=router_settings,\n",
    "        halt_settings=halt_settings[1],\n",
    "        name=\"adaptive_router_2\",\n",
    "    )(x)\n",
    "\n",
    "    x = AdaptiveRouterBlockTop1Vectorized(\n",
    "        branches=branches[2],\n",
    "        min_steps=min_steps,\n",
    "        max_steps=max_steps,\n",
    "        route_temp=route_temp,\n",
    "        router_settings=router_settings,\n",
    "        halt_settings=halt_settings[2],\n",
    "        name=\"adaptive_router_3\",\n",
    "    )(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # keep fp32 logits\n",
    "    return keras.Model(inputs, outputs, name=\"adaptive_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HazardHaltingHead: hidden=32, halt_temp=3.0, tau=0.55\n",
      "HazardHaltingHead: hidden=32, halt_temp=3.0, tau=0.55\n",
      "HazardHaltingHead: hidden=32, halt_temp=3.0, tau=0.55\n",
      "HazardHaltingHead: hidden=32, halt_temp=3.0, tau=0.55\n",
      "HazardHaltingHead: hidden=32, halt_temp=3.0, tau=0.55\n",
      "HazardHaltingHead: hidden=32, halt_temp=3.0, tau=0.55\n",
      "HazardHaltingHead: hidden=32, halt_temp=3.0, tau=0.55\n"
     ]
    }
   ],
   "source": [
    "FILTERS = 64\n",
    "MIN_STEPS = 1\n",
    "MAX_STEPS = 1\n",
    "ROUTE_TEMP = 15.0\n",
    "EPOCHS = 20\n",
    "HALT_TEMP = 3.0\n",
    "HALT_HEAD = \"hazard\"  # \"hazard\" or \"classifier\"\n",
    "\n",
    "target_steps = 4.0\n",
    "\n",
    "branches = [\n",
    "    [\n",
    "        ResidualBlock3x3(FILTERS),\n",
    "        ResidualBlockDepthwise7x7(FILTERS),\n",
    "        ChannelSE(FILTERS),\n",
    "        SpatialSE(),\n",
    "        DummyBlock()\n",
    "    ],\n",
    "    [\n",
    "        ResidualBlock3x3(FILTERS),\n",
    "        ResidualBlockDepthwise7x7(FILTERS),\n",
    "        ChannelSE(FILTERS),\n",
    "        SpatialSE(),\n",
    "        DummyBlock()\n",
    "    ],\n",
    "    [\n",
    "        ResidualBlock3x3(FILTERS),\n",
    "        ResidualBlockDepthwise7x7(FILTERS),\n",
    "        ChannelSE(FILTERS),\n",
    "        SpatialSE(),\n",
    "        DummyBlock()\n",
    "    ]\n",
    "]\n",
    "\n",
    "pool_branches = [\n",
    "    [\n",
    "        MaxPoolingLayer(filters=FILTERS),\n",
    "        AvgPoolingLayer(filters=FILTERS),\n",
    "        Conv3x3PoolingLayer(filters=FILTERS),\n",
    "        Depthwise5x5ConvPoolingLayer(filters=FILTERS)\n",
    "    ]\n",
    "]\n",
    "\n",
    "router_model = build_adaptive_model_sparse(\n",
    "    branches=branches,\n",
    "    pool_branches=pool_branches,\n",
    "    input_shape=(32,32,3),\n",
    "    num_classes=10,\n",
    "    filters=FILTERS,\n",
    "    min_steps=MIN_STEPS,\n",
    "    max_steps=MAX_STEPS,        # >0 encourages fewer steps; tune as needed\n",
    "    router_settings={\n",
    "        \"heads\": 2,\n",
    "        \"dim_head\": 32,\n",
    "        \"mlp_hidden\": 64,\n",
    "    },\n",
    "    halt_settings=[\n",
    "        {\n",
    "            \"hidden\": 32,\n",
    "            \"temp\": 3.0,\n",
    "            \"ponder_lambda\": 3e-4,\n",
    "            \"type\": HALT_HEAD,\n",
    "            \"num_classes\": 10, \n",
    "            \"tau\": 0.55,          \n",
    "            \"bias_init\": 0.0\n",
    "        },\n",
    "        {\n",
    "            \"hidden\": 32,\n",
    "            \"temp\": 3.0,\n",
    "            \"type\": HALT_HEAD,\n",
    "            \"ponder_lambda\": 3e-4,\n",
    "            \"num_classes\": 10, \n",
    "            \"tau\": 0.55,          \n",
    "            \"bias_init\": 0.0\n",
    "        },\n",
    "        {\n",
    "            \"hidden\": 32,\n",
    "            \"temp\": 3.0,\n",
    "            \"type\": HALT_HEAD,\n",
    "            \"ponder_lambda\": 3e-4,\n",
    "            \"num_classes\": 10, \n",
    "            \"tau\": 0.55,          \n",
    "            \"bias_init\": 0.0\n",
    "        }\n",
    "    ],\n",
    "    route_temp=ROUTE_TEMP\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"adaptive_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " layer_normalization_3 (Lay  (None, 32, 32, 32)        64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling_layer_7 (MaxPo  (None, 16, 16, 64)        2240      \n",
      " olingLayer)                                                     \n",
      "                                                                 \n",
      " residual_block3x3_15 (Resi  (None, 16, 16, 64)        82432     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " adaptive_router_1 (Adaptiv  (None, 16, 16, 64)        117024    \n",
      " eRouterBlockTop1Vectorized                                      \n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " layer_normalization_3 (Lay  (None, 32, 32, 32)        64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling_layer_7 (MaxPo  (None, 16, 16, 64)        2240      \n",
      " olingLayer)                                                     \n",
      "                                                                 \n",
      " residual_block3x3_15 (Resi  (None, 16, 16, 64)        82432     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " adaptive_router_1 (Adaptiv  (None, 16, 16, 64)        117024    \n",
      " eRouterBlockTop1Vectorized                                      \n",
      " )                                                               \n",
      "                                                                 \n",
      " adaptive_router_pool_1 (Ad  (None, 8, 8, 64)          66272     \n",
      " aptiveRouterBlockTop1Vecto                                      \n",
      " rized)                                                          \n",
      "                                                                 \n",
      " adaptive_router_2 (Adaptiv  (None, 8, 8, 64)          117024    \n",
      " eRouterBlockTop1Vectorized                                      \n",
      " )                                                               \n",
      "                                                                 \n",
      " adaptive_router_3 (Adaptiv  (None, 8, 8, 64)          117024    \n",
      " eRouterBlockTop1Vectorized                                      \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 64)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      " )                                                               \n",
      "                                                                 \n",
      " adaptive_router_pool_1 (Ad  (None, 8, 8, 64)          66272     \n",
      " aptiveRouterBlockTop1Vecto                                      \n",
      " rized)                                                          \n",
      "                                                                 \n",
      " adaptive_router_2 (Adaptiv  (None, 8, 8, 64)          117024    \n",
      " eRouterBlockTop1Vectorized                                      \n",
      " )                                                               \n",
      "                                                                 \n",
      " adaptive_router_3 (Adaptiv  (None, 8, 8, 64)          117024    \n",
      " eRouterBlockTop1Vectorized                                      \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 64)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 503626 (1.92 MB)\n",
      "Trainable params: 503626 (1.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "=================================================================\n",
      "Total params: 503626 (1.92 MB)\n",
      "Trainable params: 503626 (1.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    TempScheduler(layer_name=\"adaptive_router_2\", epochs=EPOCHS, mode=\"cosine\", route_start=ROUTE_TEMP, route_end=0.5, halt_start=HALT_TEMP, halt_end=1.0),\n",
    "    RouterStatsCallback(x_test, y_test, layer_name=\"adaptive_router_pool_1\"),\n",
    "    RouterStatsCallback(x_test, y_test, layer_name=\"adaptive_router_1\"),\n",
    "    RouterStatsCallback(x_test, y_test, layer_name=\"adaptive_router_2\"),\n",
    "    RouterStatsCallback(x_test, y_test, layer_name=\"adaptive_router_3\"),\n",
    "    CosineAnnealingScheduler(base_lr=3e-3, min_lr=1e-5, epochs=EPOCHS),\n",
    "    #AdaptiveTauCallback(layer_name=\"adaptive_router_1\", target_steps=target_steps, update_rate=0.001),\n",
    "    #AdaptiveTauCallback(layer_name=\"adaptive_router_2\", target_steps=target_steps, update_rate=0.001),\n",
    "]\n",
    "\n",
    "router_model.build(input_shape=(None, 32, 32, 3))\n",
    "router_model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "router_model.summary()\n",
    "\n",
    "#tf.keras.utils.plot_model(router_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [LR Scheduler] epoch 1: lr=0.003000\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['queries:0', 'adaptive_router_1/attn_pool_router_6/conv2d_16/kernel:0', 'adaptive_router_1/attn_pool_router_6/conv2d_17/kernel:0', 'dense_13/kernel:0', 'dense_14/kernel:0', 'dense_2/kernel:0', 'dense_3/kernel:0', 'queries:0', 'adaptive_router_pool_1/attn_pool_router_7/conv2d_18/kernel:0', 'adaptive_router_pool_1/attn_pool_router_7/conv2d_19/kernel:0', 'dense_15/kernel:0', 'dense_16/kernel:0', 'dense/kernel:0', 'dense_1/kernel:0', 'queries:0', 'adaptive_router_2/attn_pool_router_8/conv2d_20/kernel:0', 'adaptive_router_2/attn_pool_router_8/conv2d_21/kernel:0', 'dense_17/kernel:0', 'dense_18/kernel:0', 'dense_2/kernel:0', 'dense_3/kernel:0', 'queries:0', 'adaptive_router_3/attn_pool_router_9/conv2d_22/kernel:0', 'adaptive_router_3/attn_pool_router_9/conv2d_23/kernel:0', 'dense_19/kernel:0', 'dense_20/kernel:0', 'dense_2/kernel:0', 'dense_3/kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['queries:0', 'adaptive_router_1/attn_pool_router_6/conv2d_16/kernel:0', 'adaptive_router_1/attn_pool_router_6/conv2d_17/kernel:0', 'dense_13/kernel:0', 'dense_14/kernel:0', 'dense_2/kernel:0', 'dense_3/kernel:0', 'queries:0', 'adaptive_router_pool_1/attn_pool_router_7/conv2d_18/kernel:0', 'adaptive_router_pool_1/attn_pool_router_7/conv2d_19/kernel:0', 'dense_15/kernel:0', 'dense_16/kernel:0', 'dense/kernel:0', 'dense_1/kernel:0', 'queries:0', 'adaptive_router_2/attn_pool_router_8/conv2d_20/kernel:0', 'adaptive_router_2/attn_pool_router_8/conv2d_21/kernel:0', 'dense_17/kernel:0', 'dense_18/kernel:0', 'dense_2/kernel:0', 'dense_3/kernel:0', 'queries:0', 'adaptive_router_3/attn_pool_router_9/conv2d_22/kernel:0', 'adaptive_router_3/attn_pool_router_9/conv2d_23/kernel:0', 'dense_19/kernel:0', 'dense_20/kernel:0', 'dense_2/kernel:0', 'dense_3/kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    }
   ],
   "source": [
    "router_model.fit(ds_train, epochs=EPOCHS, validation_data=ds_val, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================== adaptive_router_2 ====================\n",
      "Test acc: 87.65%  |  samples: 10000\n",
      "Steps histogram (0..T; last bin = T): [   0 6155 1321  716  347 1461]\n",
      "Halt rate per step: [0.615 0.132 0.071 0.035 0.014]\n",
      "Expert 0 total usage: 416 (0.042 per sample on avg)\n",
      "Expert 1 total usage: 8928 (0.893 per sample on avg)\n",
      "Expert 2 total usage: 6270 (0.627 per sample on avg)\n",
      "Expert 3 total usage: 4221 (0.422 per sample on avg)\n",
      "Expert 4 total usage: 25188 (2.519 per sample on avg)\n",
      "Expert 5 total usage: 4977 (0.498 per sample on avg)\n",
      "Expert usage at step 0: [  23  652 1714  915 6413  283]\n",
      "Expert usage at step 1: [   1 1676  692 1116 5572  943]\n",
      "Expert usage at step 2: [  20 2133  780  981 4667 1419]\n",
      "Expert usage at step 3: [ 140 2261 1316  692 4289 1302]\n",
      "Expert usage at step 4: [ 232 2206 1768  517 4247 1030]\n"
     ]
    }
   ],
   "source": [
    "#print_router_stats(router_model, x_test, y_test, layer_name=\"adaptive_router_1\", batch_size=512)\n",
    "print_router_stats(router_model, x_test, y_test, layer_name=\"adaptive_router_2\", batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================== adaptive_router_2 ====================\n",
      " > pred label: 7 true label: 7\n",
      "   experts per step: [1 1 1]\n",
      "   halt probs: [0.54287356 0.57138026 0.6229602 ]\n",
      " > pred label: 7 true label: 7\n",
      "   experts per step: [1 1 1]\n",
      "   halt probs: [0.54287356 0.57138026 0.6229602 ]\n",
      " > pred label: 0 true label: 0\n",
      "   experts per step: [3 3 3]\n",
      "   halt probs: [0.5485479  0.57316166 0.59996545]\n",
      " > pred label: 0 true label: 0\n",
      "   experts per step: [3 3 3]\n",
      "   halt probs: [0.5485479  0.57316166 0.59996545]\n",
      " > pred label: 0 true label: 4\n",
      "   experts per step: [3]\n",
      "   halt probs: [0.5939134]\n",
      " > pred label: 0 true label: 4\n",
      "   experts per step: [3]\n",
      "   halt probs: [0.5939134]\n",
      " > pred label: 9 true label: 9\n",
      "   experts per step: [4]\n",
      "   halt probs: [0.5781599]\n",
      " > pred label: 9 true label: 9\n",
      "   experts per step: [4]\n",
      "   halt probs: [0.5781599]\n",
      " > pred label: 5 true label: 5\n",
      "   experts per step: [1]\n",
      "   halt probs: [0.5776754]\n",
      " > pred label: 5 true label: 5\n",
      "   experts per step: [1]\n",
      "   halt probs: [0.5776754]\n",
      " > pred label: 2 true label: 2\n",
      "   experts per step: [1]\n",
      "   halt probs: [0.59392893]\n",
      " > pred label: 2 true label: 2\n",
      "   experts per step: [1]\n",
      "   halt probs: [0.59392893]\n",
      " > pred label: 4 true label: 4\n",
      "   experts per step: [4 1 1]\n",
      "   halt probs: [0.56477916 0.57625836 0.58145684]\n",
      " > pred label: 4 true label: 4\n",
      "   experts per step: [4 1 1]\n",
      "   halt probs: [0.56477916 0.57625836 0.58145684]\n",
      " > pred label: 0 true label: 0\n",
      "   experts per step: [4 4 4 2]\n",
      "   halt probs: [0.5460836  0.55476713 0.5725925  0.5984294 ]\n",
      " > pred label: 0 true label: 0\n",
      "   experts per step: [4 4 4 2]\n",
      "   halt probs: [0.5460836  0.55476713 0.5725925  0.5984294 ]\n",
      " > pred label: 9 true label: 9\n",
      "   experts per step: [4 4]\n",
      "   halt probs: [0.5634865  0.58316123]\n",
      " > pred label: 9 true label: 9\n",
      "   experts per step: [4 4]\n",
      "   halt probs: [0.5634865  0.58316123]\n",
      " > pred label: 6 true label: 6\n",
      "   experts per step: [4]\n",
      "   halt probs: [0.6284362]\n",
      " > pred label: 6 true label: 6\n",
      "   experts per step: [4]\n",
      "   halt probs: [0.6284362]\n",
      " > pred label: 6 true label: 6\n",
      "   experts per step: [4]\n",
      "   halt probs: [0.60312146]\n",
      " > pred label: 6 true label: 6\n",
      "   experts per step: [4]\n",
      "   halt probs: [0.60312146]\n",
      " > pred label: 5 true label: 5\n",
      "   experts per step: [2]\n",
      "   halt probs: [0.59239763]\n",
      " > pred label: 5 true label: 5\n",
      "   experts per step: [2]\n",
      "   halt probs: [0.59239763]\n",
      " > pred label: 4 true label: 4\n",
      "   experts per step: [4]\n",
      "   halt probs: [0.61958176]\n",
      " > pred label: 4 true label: 4\n",
      "   experts per step: [4]\n",
      "   halt probs: [0.61958176]\n",
      " > pred label: 5 true label: 5\n",
      "   experts per step: [1 3 3]\n",
      "   halt probs: [0.5496357  0.55666095 0.5901303 ]\n",
      " > pred label: 5 true label: 5\n",
      "   experts per step: [1 3 3]\n",
      "   halt probs: [0.5496357  0.55666095 0.5901303 ]\n",
      " > pred label: 9 true label: 9\n",
      "   experts per step: [4]\n",
      "   halt probs: [0.5830648]\n",
      " > pred label: 9 true label: 9\n",
      "   experts per step: [4]\n",
      "   halt probs: [0.5830648]\n",
      " > pred label: 8 true label: 2\n",
      "   experts per step: [4]\n",
      "   halt probs: [0.5799484]\n",
      " > pred label: 8 true label: 2\n",
      "   experts per step: [4]\n",
      "   halt probs: [0.5799484]\n",
      " > pred label: 7 true label: 4\n",
      "   experts per step: [4 1 1]\n",
      "   halt probs: [0.5698778 0.5682596 0.5840002]\n",
      " > pred label: 7 true label: 4\n",
      "   experts per step: [4 1 1]\n",
      "   halt probs: [0.5698778 0.5682596 0.5840002]\n",
      " > pred label: 9 true label: 1\n",
      "   experts per step: [2]\n",
      "   halt probs: [0.6030896]\n",
      " > pred label: 9 true label: 1\n",
      "   experts per step: [2]\n",
      "   halt probs: [0.6030896]\n",
      " > pred label: 9 true label: 9\n",
      "   experts per step: [4]\n",
      "   halt probs: [0.59295934]\n",
      " > pred label: 9 true label: 9\n",
      "   experts per step: [4]\n",
      "   halt probs: [0.59295934]\n",
      " > pred label: 5 true label: 5\n",
      "   experts per step: [2]\n",
      "   halt probs: [0.6089387]\n",
      " > pred label: 5 true label: 5\n",
      "   experts per step: [2]\n",
      "   halt probs: [0.6089387]\n"
     ]
    }
   ],
   "source": [
    "#print_trace_for_samples(router_model, x_test, y_test, layer_name=\"adaptive_router_1\", start=0, end=5)\n",
    "print_trace_for_samples(router_model, x_test, y_test, layer_name=\"adaptive_router_2\", start=20, end=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#85 - 92 sec ==> accuracy: 0.8632 - val_loss: 0.5222 - val_accuracy: 0.8235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_model_blocks(input_shape=(32,32,3), num_classes=10, filters=32):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # For CNNs on GPU, BatchNorm is faster than LayerNorm:\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = MaxPoolingLayer(filters=filters)(x)\n",
    "    x = ResidualBlock3x3(filters)(x)\n",
    "    x = ResidualBlock3x3(filters)(x)\n",
    "    x = MaxPoolingLayer(filters=filters)(x)\n",
    "    x = ResidualBlock3x3(filters)(x)\n",
    "    x = ResidualBlock3x3(filters)(x)\n",
    "    x = ResidualBlock3x3(filters)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # keep fp32 logits\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "def build_big_base_model_blocks(input_shape=(32,32,3), num_classes=10, filters=32):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # For CNNs on GPU, BatchNorm is faster than LayerNorm:\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = ResidualBlock3x3(32)(x)\n",
    "    x = MaxPoolingLayer(filters=64)(x)\n",
    "    x = ResidualBlock3x3(filters=64)(x)\n",
    "    x = ResidualBlock3x3(filters=64)(x)\n",
    "    x = MaxPoolingLayer(filters=128)(x)\n",
    "    x = ResidualBlock3x3(filters=128)(x)\n",
    "    x = ResidualBlock3x3(filters=128)(x)\n",
    "    x = ResidualBlock3x3(filters=128)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # keep fp32 logits\n",
    "    return keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_800\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " layer_normalization_3 (Lay  (None, 32, 32, 32)        64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling_layer_9 (MaxPo  (None, 16, 16, 64)        2240      \n",
      " olingLayer)                                                     \n",
      "                                                                 \n",
      " residual_block3x3_15 (Resi  (None, 16, 16, 64)        82432     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " residual_block3x3_16 (Resi  (None, 16, 16, 64)        82432     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " max_pooling_layer_10 (MaxP  (None, 8, 8, 64)          4288      \n",
      " oolingLayer)                                                    \n",
      "                                                                 \n",
      " residual_block3x3_17 (Resi  (None, 8, 8, 64)          82432     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " residual_block3x3_18 (Resi  (None, 8, 8, 64)          82432     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " residual_block3x3_19 (Resi  (None, 8, 8, 64)          82432     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " layer_normalization_3 (Lay  (None, 32, 32, 32)        64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling_layer_9 (MaxPo  (None, 16, 16, 64)        2240      \n",
      " olingLayer)                                                     \n",
      "                                                                 \n",
      " residual_block3x3_15 (Resi  (None, 16, 16, 64)        82432     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " residual_block3x3_16 (Resi  (None, 16, 16, 64)        82432     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " max_pooling_layer_10 (MaxP  (None, 8, 8, 64)          4288      \n",
      " oolingLayer)                                                    \n",
      "                                                                 \n",
      " residual_block3x3_17 (Resi  (None, 8, 8, 64)          82432     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " residual_block3x3_18 (Resi  (None, 8, 8, 64)          82432     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " residual_block3x3_19 (Resi  (None, 8, 8, 64)          82432     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " global_average_pooling2d_3  (None, 64)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 420298 (1.60 MB)\n",
      "Trainable params: 420298 (1.60 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      " global_average_pooling2d_3  (None, 64)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 420298 (1.60 MB)\n",
      "Trainable params: 420298 (1.60 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    CosineAnnealingScheduler(base_lr=3e-3, min_lr=1e-5, epochs=EPOCHS),\n",
    "]\n",
    "\n",
    "model = build_base_model_blocks(\n",
    "    input_shape=(32,32,3),\n",
    "    num_classes=10,\n",
    "    filters=FILTERS\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [LR Scheduler] epoch 1: lr=0.003000\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 20:07:24.283751: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - ETA: 0s - loss: 1.9606 - accuracy: 0.2557"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 20:07:47.176519: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 26s 58ms/step - loss: 1.9606 - accuracy: 0.2557 - val_loss: 1.6372 - val_accuracy: 0.3931\n",
      "Epoch 2/150\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.5576 - accuracy: 0.4209 - val_loss: 1.3746 - val_accuracy: 0.4903\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.5576 - accuracy: 0.4209 - val_loss: 1.3746 - val_accuracy: 0.4903\n",
      "Epoch 3/150\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.2957 - accuracy: 0.5260 - val_loss: 1.1994 - val_accuracy: 0.5651\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.2957 - accuracy: 0.5260 - val_loss: 1.1994 - val_accuracy: 0.5651\n",
      "Epoch 4/150\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.1230 - accuracy: 0.5974 - val_loss: 1.0187 - val_accuracy: 0.6297\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.1230 - accuracy: 0.5974 - val_loss: 1.0187 - val_accuracy: 0.6297\n",
      "> [LR Scheduler] epoch 5: lr=0.002995\n",
      "Epoch 5/150\n",
      "> [LR Scheduler] epoch 5: lr=0.002995\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0123 - accuracy: 0.6385 - val_loss: 1.0294 - val_accuracy: 0.6311\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0123 - accuracy: 0.6385 - val_loss: 1.0294 - val_accuracy: 0.6311\n",
      "Epoch 6/150\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9255 - accuracy: 0.6684 - val_loss: 0.8779 - val_accuracy: 0.6860\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9255 - accuracy: 0.6684 - val_loss: 0.8779 - val_accuracy: 0.6860\n",
      "Epoch 7/150\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.8555 - accuracy: 0.6978 - val_loss: 0.8234 - val_accuracy: 0.7037\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.8555 - accuracy: 0.6978 - val_loss: 0.8234 - val_accuracy: 0.7037\n",
      "Epoch 8/150\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8066 - accuracy: 0.7145 - val_loss: 0.8154 - val_accuracy: 0.7154\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8066 - accuracy: 0.7145 - val_loss: 0.8154 - val_accuracy: 0.7154\n",
      "Epoch 9/150\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7680 - accuracy: 0.7311 - val_loss: 0.7723 - val_accuracy: 0.7288\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7680 - accuracy: 0.7311 - val_loss: 0.7723 - val_accuracy: 0.7288\n",
      "> [LR Scheduler] epoch 10: lr=0.002973\n",
      "Epoch 10/150\n",
      "> [LR Scheduler] epoch 10: lr=0.002973\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7300 - accuracy: 0.7437 - val_loss: 0.7837 - val_accuracy: 0.7265\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7300 - accuracy: 0.7437 - val_loss: 0.7837 - val_accuracy: 0.7265\n",
      "Epoch 11/150\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6975 - accuracy: 0.7565 - val_loss: 0.7708 - val_accuracy: 0.7301\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6975 - accuracy: 0.7565 - val_loss: 0.7708 - val_accuracy: 0.7301\n",
      "Epoch 12/150\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6720 - accuracy: 0.7671 - val_loss: 0.7111 - val_accuracy: 0.7527\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6720 - accuracy: 0.7671 - val_loss: 0.7111 - val_accuracy: 0.7527\n",
      "Epoch 13/150\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6437 - accuracy: 0.7737 - val_loss: 0.6912 - val_accuracy: 0.7652\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6437 - accuracy: 0.7737 - val_loss: 0.6912 - val_accuracy: 0.7652\n",
      "Epoch 14/150\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.6139 - accuracy: 0.7838 - val_loss: 0.6483 - val_accuracy: 0.7734\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.6139 - accuracy: 0.7838 - val_loss: 0.6483 - val_accuracy: 0.7734\n",
      "> [LR Scheduler] epoch 15: lr=0.002935\n",
      "Epoch 15/150\n",
      "> [LR Scheduler] epoch 15: lr=0.002935\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5962 - accuracy: 0.7915 - val_loss: 0.6529 - val_accuracy: 0.7741\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5962 - accuracy: 0.7915 - val_loss: 0.6529 - val_accuracy: 0.7741\n",
      "Epoch 16/150\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5614 - accuracy: 0.8028 - val_loss: 0.6163 - val_accuracy: 0.7884\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5614 - accuracy: 0.8028 - val_loss: 0.6163 - val_accuracy: 0.7884\n",
      "Epoch 17/150\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5584 - accuracy: 0.8060 - val_loss: 0.5980 - val_accuracy: 0.7961\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5584 - accuracy: 0.8060 - val_loss: 0.5980 - val_accuracy: 0.7961\n",
      "Epoch 18/150\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5298 - accuracy: 0.8150 - val_loss: 0.5759 - val_accuracy: 0.7956\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5298 - accuracy: 0.8150 - val_loss: 0.5759 - val_accuracy: 0.7956\n",
      "Epoch 19/150\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5159 - accuracy: 0.8207 - val_loss: 0.5828 - val_accuracy: 0.8043\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5159 - accuracy: 0.8207 - val_loss: 0.5828 - val_accuracy: 0.8043\n",
      "> [LR Scheduler] epoch 20: lr=0.002882\n",
      "Epoch 20/150\n",
      "> [LR Scheduler] epoch 20: lr=0.002882\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4947 - accuracy: 0.8268 - val_loss: 0.5577 - val_accuracy: 0.8048\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4947 - accuracy: 0.8268 - val_loss: 0.5577 - val_accuracy: 0.8048\n",
      "Epoch 21/150\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4797 - accuracy: 0.8320 - val_loss: 0.5474 - val_accuracy: 0.8151\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4797 - accuracy: 0.8320 - val_loss: 0.5474 - val_accuracy: 0.8151\n",
      "Epoch 22/150\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4648 - accuracy: 0.8380 - val_loss: 0.5304 - val_accuracy: 0.8145\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4648 - accuracy: 0.8380 - val_loss: 0.5304 - val_accuracy: 0.8145\n",
      "Epoch 23/150\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4507 - accuracy: 0.8446 - val_loss: 0.5060 - val_accuracy: 0.8289\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4507 - accuracy: 0.8446 - val_loss: 0.5060 - val_accuracy: 0.8289\n",
      "Epoch 24/150\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4339 - accuracy: 0.8480 - val_loss: 0.5504 - val_accuracy: 0.8150\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4339 - accuracy: 0.8480 - val_loss: 0.5504 - val_accuracy: 0.8150\n",
      "> [LR Scheduler] epoch 25: lr=0.002813\n",
      "Epoch 25/150\n",
      "> [LR Scheduler] epoch 25: lr=0.002813\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4276 - accuracy: 0.8507 - val_loss: 0.5207 - val_accuracy: 0.8215\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4276 - accuracy: 0.8507 - val_loss: 0.5207 - val_accuracy: 0.8215\n",
      "Epoch 26/150\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 0.4255 - accuracy: 0.8518 - val_loss: 0.5146 - val_accuracy: 0.8228\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 0.4255 - accuracy: 0.8518 - val_loss: 0.5146 - val_accuracy: 0.8228\n",
      "Epoch 27/150\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.4037 - accuracy: 0.8579 - val_loss: 0.4935 - val_accuracy: 0.8295\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.4037 - accuracy: 0.8579 - val_loss: 0.4935 - val_accuracy: 0.8295\n",
      "Epoch 28/150\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.3968 - accuracy: 0.8631 - val_loss: 0.4890 - val_accuracy: 0.8344\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.3968 - accuracy: 0.8631 - val_loss: 0.4890 - val_accuracy: 0.8344\n",
      "Epoch 29/150\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.3819 - accuracy: 0.8683 - val_loss: 0.4865 - val_accuracy: 0.8369\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.3819 - accuracy: 0.8683 - val_loss: 0.4865 - val_accuracy: 0.8369\n",
      "> [LR Scheduler] epoch 30: lr=0.002729\n",
      "Epoch 30/150\n",
      "> [LR Scheduler] epoch 30: lr=0.002729\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.3774 - accuracy: 0.8683 - val_loss: 0.4706 - val_accuracy: 0.8405\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.3774 - accuracy: 0.8683 - val_loss: 0.4706 - val_accuracy: 0.8405\n",
      "Epoch 31/150\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.3626 - accuracy: 0.8747 - val_loss: 0.4918 - val_accuracy: 0.8335\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.3626 - accuracy: 0.8747 - val_loss: 0.4918 - val_accuracy: 0.8335\n",
      "Epoch 32/150\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 0.3622 - accuracy: 0.8719 - val_loss: 0.4695 - val_accuracy: 0.8456\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 0.3622 - accuracy: 0.8719 - val_loss: 0.4695 - val_accuracy: 0.8456\n",
      "Epoch 33/150\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3499 - accuracy: 0.8779 - val_loss: 0.4646 - val_accuracy: 0.8459\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3499 - accuracy: 0.8779 - val_loss: 0.4646 - val_accuracy: 0.8459\n",
      "Epoch 34/150\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3414 - accuracy: 0.8807 - val_loss: 0.4608 - val_accuracy: 0.8447\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3414 - accuracy: 0.8807 - val_loss: 0.4608 - val_accuracy: 0.8447\n",
      "> [LR Scheduler] epoch 35: lr=0.002632\n",
      "Epoch 35/150\n",
      "> [LR Scheduler] epoch 35: lr=0.002632\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3307 - accuracy: 0.8832 - val_loss: 0.4741 - val_accuracy: 0.8380\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3307 - accuracy: 0.8832 - val_loss: 0.4741 - val_accuracy: 0.8380\n",
      "Epoch 36/150\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3312 - accuracy: 0.8835 - val_loss: 0.5007 - val_accuracy: 0.8386\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3312 - accuracy: 0.8835 - val_loss: 0.5007 - val_accuracy: 0.8386\n",
      "Epoch 37/150\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3117 - accuracy: 0.8900 - val_loss: 0.5319 - val_accuracy: 0.8268\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3117 - accuracy: 0.8900 - val_loss: 0.5319 - val_accuracy: 0.8268\n",
      "Epoch 38/150\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3104 - accuracy: 0.8909 - val_loss: 0.4777 - val_accuracy: 0.8416\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3104 - accuracy: 0.8909 - val_loss: 0.4777 - val_accuracy: 0.8416\n",
      "Epoch 39/150\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3041 - accuracy: 0.8940 - val_loss: 0.4622 - val_accuracy: 0.8476\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3041 - accuracy: 0.8940 - val_loss: 0.4622 - val_accuracy: 0.8476\n",
      "> [LR Scheduler] epoch 40: lr=0.002522\n",
      "Epoch 40/150\n",
      "> [LR Scheduler] epoch 40: lr=0.002522\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3005 - accuracy: 0.8945 - val_loss: 0.4640 - val_accuracy: 0.8475\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3005 - accuracy: 0.8945 - val_loss: 0.4640 - val_accuracy: 0.8475\n",
      "Epoch 41/150\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2904 - accuracy: 0.8974 - val_loss: 0.4430 - val_accuracy: 0.8518\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2904 - accuracy: 0.8974 - val_loss: 0.4430 - val_accuracy: 0.8518\n",
      "Epoch 42/150\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.2867 - accuracy: 0.8994 - val_loss: 0.4670 - val_accuracy: 0.8456\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.2867 - accuracy: 0.8994 - val_loss: 0.4670 - val_accuracy: 0.8456\n",
      "Epoch 43/150\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 0.2705 - accuracy: 0.9044 - val_loss: 0.4459 - val_accuracy: 0.8550\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 0.2705 - accuracy: 0.9044 - val_loss: 0.4459 - val_accuracy: 0.8550\n",
      "Epoch 44/150\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2777 - accuracy: 0.9029 - val_loss: 0.4631 - val_accuracy: 0.8524\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2777 - accuracy: 0.9029 - val_loss: 0.4631 - val_accuracy: 0.8524\n",
      "> [LR Scheduler] epoch 45: lr=0.002401\n",
      "Epoch 45/150\n",
      "> [LR Scheduler] epoch 45: lr=0.002401\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.2629 - accuracy: 0.9071 - val_loss: 0.4527 - val_accuracy: 0.8537\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.2629 - accuracy: 0.9071 - val_loss: 0.4527 - val_accuracy: 0.8537\n",
      "Epoch 46/150\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2600 - accuracy: 0.9094 - val_loss: 0.4598 - val_accuracy: 0.8521\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2600 - accuracy: 0.9094 - val_loss: 0.4598 - val_accuracy: 0.8521\n",
      "Epoch 47/150\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.2524 - accuracy: 0.9102 - val_loss: 0.4524 - val_accuracy: 0.8554\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.2524 - accuracy: 0.9102 - val_loss: 0.4524 - val_accuracy: 0.8554\n",
      "Epoch 48/150\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.2484 - accuracy: 0.9129 - val_loss: 0.4667 - val_accuracy: 0.8540\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.2484 - accuracy: 0.9129 - val_loss: 0.4667 - val_accuracy: 0.8540\n",
      "Epoch 49/150\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.2389 - accuracy: 0.9143 - val_loss: 0.4578 - val_accuracy: 0.8517\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.2389 - accuracy: 0.9143 - val_loss: 0.4578 - val_accuracy: 0.8517\n",
      "> [LR Scheduler] epoch 50: lr=0.002271\n",
      "Epoch 50/150\n",
      "> [LR Scheduler] epoch 50: lr=0.002271\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.2341 - accuracy: 0.9172 - val_loss: 0.4453 - val_accuracy: 0.8571\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.2341 - accuracy: 0.9172 - val_loss: 0.4453 - val_accuracy: 0.8571\n",
      "Epoch 51/150\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.2337 - accuracy: 0.9171 - val_loss: 0.4470 - val_accuracy: 0.8566\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.2337 - accuracy: 0.9171 - val_loss: 0.4470 - val_accuracy: 0.8566\n",
      "Epoch 52/150\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2273 - accuracy: 0.9193 - val_loss: 0.4538 - val_accuracy: 0.8595\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2273 - accuracy: 0.9193 - val_loss: 0.4538 - val_accuracy: 0.8595\n",
      "Epoch 53/150\n",
      "Epoch 53/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2254 - accuracy: 0.9193 - val_loss: 0.4564 - val_accuracy: 0.8532\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2254 - accuracy: 0.9193 - val_loss: 0.4564 - val_accuracy: 0.8532\n",
      "Epoch 54/150\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2123 - accuracy: 0.9257 - val_loss: 0.4547 - val_accuracy: 0.8592\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2123 - accuracy: 0.9257 - val_loss: 0.4547 - val_accuracy: 0.8592\n",
      "> [LR Scheduler] epoch 55: lr=0.002131\n",
      "Epoch 55/150\n",
      "> [LR Scheduler] epoch 55: lr=0.002131\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2189 - accuracy: 0.9222 - val_loss: 0.4657 - val_accuracy: 0.8551\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2189 - accuracy: 0.9222 - val_loss: 0.4657 - val_accuracy: 0.8551\n",
      "Epoch 56/150\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2082 - accuracy: 0.9252 - val_loss: 0.4677 - val_accuracy: 0.8541\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2082 - accuracy: 0.9252 - val_loss: 0.4677 - val_accuracy: 0.8541\n",
      "Epoch 57/150\n",
      "Epoch 57/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2060 - accuracy: 0.9272 - val_loss: 0.4606 - val_accuracy: 0.8562\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2060 - accuracy: 0.9272 - val_loss: 0.4606 - val_accuracy: 0.8562\n",
      "Epoch 58/150\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.1959 - accuracy: 0.9307 - val_loss: 0.4518 - val_accuracy: 0.8626\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.1959 - accuracy: 0.9307 - val_loss: 0.4518 - val_accuracy: 0.8626\n",
      "Epoch 59/150\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.1988 - accuracy: 0.9291 - val_loss: 0.4816 - val_accuracy: 0.8555\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.1988 - accuracy: 0.9291 - val_loss: 0.4816 - val_accuracy: 0.8555\n",
      "> [LR Scheduler] epoch 60: lr=0.001985\n",
      "Epoch 60/150\n",
      "> [LR Scheduler] epoch 60: lr=0.001985\n",
      "Epoch 60/150\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.1939 - accuracy: 0.9305 - val_loss: 0.4581 - val_accuracy: 0.8644\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.1939 - accuracy: 0.9305 - val_loss: 0.4581 - val_accuracy: 0.8644\n",
      "Epoch 61/150\n",
      "Epoch 61/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1875 - accuracy: 0.9329 - val_loss: 0.4550 - val_accuracy: 0.8616\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1875 - accuracy: 0.9329 - val_loss: 0.4550 - val_accuracy: 0.8616\n",
      "Epoch 62/150\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1810 - accuracy: 0.9346 - val_loss: 0.4363 - val_accuracy: 0.8672\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1810 - accuracy: 0.9346 - val_loss: 0.4363 - val_accuracy: 0.8672\n",
      "Epoch 63/150\n",
      "Epoch 63/150\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.1772 - accuracy: 0.9365 - val_loss: 0.4692 - val_accuracy: 0.8577\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.1772 - accuracy: 0.9365 - val_loss: 0.4692 - val_accuracy: 0.8577\n",
      "Epoch 64/150\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1691 - accuracy: 0.9396 - val_loss: 0.4782 - val_accuracy: 0.8603\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1691 - accuracy: 0.9396 - val_loss: 0.4782 - val_accuracy: 0.8603\n",
      "> [LR Scheduler] epoch 65: lr=0.001833\n",
      "Epoch 65/150\n",
      "> [LR Scheduler] epoch 65: lr=0.001833\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1639 - accuracy: 0.9421 - val_loss: 0.4406 - val_accuracy: 0.8709\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1639 - accuracy: 0.9421 - val_loss: 0.4406 - val_accuracy: 0.8709\n",
      "Epoch 66/150\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.1632 - accuracy: 0.9427 - val_loss: 0.4558 - val_accuracy: 0.8690\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.1632 - accuracy: 0.9427 - val_loss: 0.4558 - val_accuracy: 0.8690\n",
      "Epoch 67/150\n",
      "Epoch 67/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1542 - accuracy: 0.9438 - val_loss: 0.4602 - val_accuracy: 0.8657\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1542 - accuracy: 0.9438 - val_loss: 0.4602 - val_accuracy: 0.8657\n",
      "Epoch 68/150\n",
      "Epoch 68/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1602 - accuracy: 0.9422 - val_loss: 0.4726 - val_accuracy: 0.8635\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1602 - accuracy: 0.9422 - val_loss: 0.4726 - val_accuracy: 0.8635\n",
      "Epoch 69/150\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1568 - accuracy: 0.9447 - val_loss: 0.4731 - val_accuracy: 0.8644\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1568 - accuracy: 0.9447 - val_loss: 0.4731 - val_accuracy: 0.8644\n",
      "> [LR Scheduler] epoch 70: lr=0.001678\n",
      "Epoch 70/150\n",
      "> [LR Scheduler] epoch 70: lr=0.001678\n",
      "Epoch 70/150\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.1501 - accuracy: 0.9459 - val_loss: 0.4897 - val_accuracy: 0.8587\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.1501 - accuracy: 0.9459 - val_loss: 0.4897 - val_accuracy: 0.8587\n",
      "Epoch 71/150\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1412 - accuracy: 0.9494 - val_loss: 0.4618 - val_accuracy: 0.8682\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1412 - accuracy: 0.9494 - val_loss: 0.4618 - val_accuracy: 0.8682\n",
      "Epoch 72/150\n",
      "Epoch 72/150\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.1376 - accuracy: 0.9512 - val_loss: 0.4851 - val_accuracy: 0.8654\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.1376 - accuracy: 0.9512 - val_loss: 0.4851 - val_accuracy: 0.8654\n",
      "Epoch 73/150\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1346 - accuracy: 0.9520 - val_loss: 0.4778 - val_accuracy: 0.8652\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1346 - accuracy: 0.9520 - val_loss: 0.4778 - val_accuracy: 0.8652\n",
      "Epoch 74/150\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1377 - accuracy: 0.9512 - val_loss: 0.4866 - val_accuracy: 0.8677\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1377 - accuracy: 0.9512 - val_loss: 0.4866 - val_accuracy: 0.8677\n",
      "> [LR Scheduler] epoch 75: lr=0.001521\n",
      "Epoch 75/150\n",
      "> [LR Scheduler] epoch 75: lr=0.001521\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1293 - accuracy: 0.9528 - val_loss: 0.4658 - val_accuracy: 0.8717\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1293 - accuracy: 0.9528 - val_loss: 0.4658 - val_accuracy: 0.8717\n",
      "Epoch 76/150\n",
      "Epoch 76/150\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1257 - accuracy: 0.9545 - val_loss: 0.4915 - val_accuracy: 0.8673\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1257 - accuracy: 0.9545 - val_loss: 0.4915 - val_accuracy: 0.8673\n",
      "Epoch 77/150\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1195 - accuracy: 0.9567 - val_loss: 0.5131 - val_accuracy: 0.8662\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1195 - accuracy: 0.9567 - val_loss: 0.5131 - val_accuracy: 0.8662\n",
      "Epoch 78/150\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1192 - accuracy: 0.9565 - val_loss: 0.4765 - val_accuracy: 0.8714\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1192 - accuracy: 0.9565 - val_loss: 0.4765 - val_accuracy: 0.8714\n",
      "Epoch 79/150\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.1115 - accuracy: 0.9600 - val_loss: 0.4953 - val_accuracy: 0.8671\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.1115 - accuracy: 0.9600 - val_loss: 0.4953 - val_accuracy: 0.8671\n",
      "> [LR Scheduler] epoch 80: lr=0.001363\n",
      "Epoch 80/150\n",
      "> [LR Scheduler] epoch 80: lr=0.001363\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.1139 - accuracy: 0.9583 - val_loss: 0.5146 - val_accuracy: 0.8660\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.1139 - accuracy: 0.9583 - val_loss: 0.5146 - val_accuracy: 0.8660\n",
      "Epoch 81/150\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.1078 - accuracy: 0.9620 - val_loss: 0.4864 - val_accuracy: 0.8726\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.1078 - accuracy: 0.9620 - val_loss: 0.4864 - val_accuracy: 0.8726\n",
      "Epoch 82/150\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.1040 - accuracy: 0.9636 - val_loss: 0.5091 - val_accuracy: 0.8655\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.1040 - accuracy: 0.9636 - val_loss: 0.5091 - val_accuracy: 0.8655\n",
      "Epoch 83/150\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.0997 - accuracy: 0.9642 - val_loss: 0.5173 - val_accuracy: 0.8681\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.0997 - accuracy: 0.9642 - val_loss: 0.5173 - val_accuracy: 0.8681\n",
      "Epoch 84/150\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1003 - accuracy: 0.9646 - val_loss: 0.5400 - val_accuracy: 0.8684\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1003 - accuracy: 0.9646 - val_loss: 0.5400 - val_accuracy: 0.8684\n",
      "> [LR Scheduler] epoch 85: lr=0.001208\n",
      "Epoch 85/150\n",
      "> [LR Scheduler] epoch 85: lr=0.001208\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.0942 - accuracy: 0.9660 - val_loss: 0.5246 - val_accuracy: 0.8656\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.0942 - accuracy: 0.9660 - val_loss: 0.5246 - val_accuracy: 0.8656\n",
      "Epoch 86/150\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0919 - accuracy: 0.9668 - val_loss: 0.5230 - val_accuracy: 0.8697\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0919 - accuracy: 0.9668 - val_loss: 0.5230 - val_accuracy: 0.8697\n",
      "Epoch 87/150\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.0893 - accuracy: 0.9682 - val_loss: 0.5208 - val_accuracy: 0.8701\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.0893 - accuracy: 0.9682 - val_loss: 0.5208 - val_accuracy: 0.8701\n",
      "Epoch 88/150\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.0863 - accuracy: 0.9691 - val_loss: 0.5220 - val_accuracy: 0.8716\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.0863 - accuracy: 0.9691 - val_loss: 0.5220 - val_accuracy: 0.8716\n",
      "Epoch 89/150\n",
      "Epoch 89/150\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0842 - accuracy: 0.9697 - val_loss: 0.5397 - val_accuracy: 0.8669\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0842 - accuracy: 0.9697 - val_loss: 0.5397 - val_accuracy: 0.8669\n",
      "> [LR Scheduler] epoch 90: lr=0.001055\n",
      "Epoch 90/150\n",
      "> [LR Scheduler] epoch 90: lr=0.001055\n",
      "Epoch 90/150\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.0806 - accuracy: 0.9716 - val_loss: 0.5065 - val_accuracy: 0.8730\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.0806 - accuracy: 0.9716 - val_loss: 0.5065 - val_accuracy: 0.8730\n",
      "Epoch 91/150\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.0791 - accuracy: 0.9717 - val_loss: 0.5430 - val_accuracy: 0.8727\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.0791 - accuracy: 0.9717 - val_loss: 0.5430 - val_accuracy: 0.8727\n",
      "Epoch 92/150\n",
      "Epoch 92/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0723 - accuracy: 0.9743 - val_loss: 0.5568 - val_accuracy: 0.8696\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0723 - accuracy: 0.9743 - val_loss: 0.5568 - val_accuracy: 0.8696\n",
      "Epoch 93/150\n",
      "Epoch 93/150\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0722 - accuracy: 0.9740 - val_loss: 0.5420 - val_accuracy: 0.8722\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0722 - accuracy: 0.9740 - val_loss: 0.5420 - val_accuracy: 0.8722\n",
      "Epoch 94/150\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.0719 - accuracy: 0.9745 - val_loss: 0.5524 - val_accuracy: 0.8742\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.0719 - accuracy: 0.9745 - val_loss: 0.5524 - val_accuracy: 0.8742\n",
      "> [LR Scheduler] epoch 95: lr=0.000908\n",
      "Epoch 95/150\n",
      "> [LR Scheduler] epoch 95: lr=0.000908\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.0715 - accuracy: 0.9746 - val_loss: 0.5332 - val_accuracy: 0.8751\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.0715 - accuracy: 0.9746 - val_loss: 0.5332 - val_accuracy: 0.8751\n",
      "Epoch 96/150\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - 23s 60ms/step - loss: 0.0646 - accuracy: 0.9769 - val_loss: 0.5607 - val_accuracy: 0.8671\n",
      "391/391 [==============================] - 23s 60ms/step - loss: 0.0646 - accuracy: 0.9769 - val_loss: 0.5607 - val_accuracy: 0.8671\n",
      "Epoch 97/150\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.0631 - accuracy: 0.9770 - val_loss: 0.5482 - val_accuracy: 0.8728\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.0631 - accuracy: 0.9770 - val_loss: 0.5482 - val_accuracy: 0.8728\n",
      "Epoch 98/150\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.0636 - accuracy: 0.9773 - val_loss: 0.5508 - val_accuracy: 0.8749\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.0636 - accuracy: 0.9773 - val_loss: 0.5508 - val_accuracy: 0.8749\n",
      "Epoch 99/150\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0550 - accuracy: 0.9799 - val_loss: 0.5765 - val_accuracy: 0.8716\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0550 - accuracy: 0.9799 - val_loss: 0.5765 - val_accuracy: 0.8716\n",
      "> [LR Scheduler] epoch 100: lr=0.000767\n",
      "Epoch 100/150\n",
      "> [LR Scheduler] epoch 100: lr=0.000767\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.0561 - accuracy: 0.9801 - val_loss: 0.5693 - val_accuracy: 0.8718\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.0561 - accuracy: 0.9801 - val_loss: 0.5693 - val_accuracy: 0.8718\n",
      "Epoch 101/150\n",
      "Epoch 101/150\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.0526 - accuracy: 0.9817 - val_loss: 0.5572 - val_accuracy: 0.8783\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.0526 - accuracy: 0.9817 - val_loss: 0.5572 - val_accuracy: 0.8783\n",
      "Epoch 102/150\n",
      "Epoch 102/150\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.0546 - accuracy: 0.9805 - val_loss: 0.5692 - val_accuracy: 0.8741\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.0546 - accuracy: 0.9805 - val_loss: 0.5692 - val_accuracy: 0.8741\n",
      "Epoch 103/150\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.0508 - accuracy: 0.9818 - val_loss: 0.5715 - val_accuracy: 0.8743\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.0508 - accuracy: 0.9818 - val_loss: 0.5715 - val_accuracy: 0.8743\n",
      "Epoch 104/150\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0477 - accuracy: 0.9833 - val_loss: 0.5993 - val_accuracy: 0.8733\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0477 - accuracy: 0.9833 - val_loss: 0.5993 - val_accuracy: 0.8733\n",
      "> [LR Scheduler] epoch 105: lr=0.000634\n",
      "Epoch 105/150\n",
      "> [LR Scheduler] epoch 105: lr=0.000634\n",
      "Epoch 105/150\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0471 - accuracy: 0.9832 - val_loss: 0.5936 - val_accuracy: 0.8761\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0471 - accuracy: 0.9832 - val_loss: 0.5936 - val_accuracy: 0.8761\n",
      "Epoch 106/150\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0448 - accuracy: 0.9847 - val_loss: 0.5813 - val_accuracy: 0.8759\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0448 - accuracy: 0.9847 - val_loss: 0.5813 - val_accuracy: 0.8759\n",
      "Epoch 107/150\n",
      "Epoch 107/150\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 0.0440 - accuracy: 0.9849 - val_loss: 0.5683 - val_accuracy: 0.8765\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 0.0440 - accuracy: 0.9849 - val_loss: 0.5683 - val_accuracy: 0.8765\n",
      "Epoch 108/150\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0431 - accuracy: 0.9850 - val_loss: 0.6012 - val_accuracy: 0.8748\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0431 - accuracy: 0.9850 - val_loss: 0.6012 - val_accuracy: 0.8748\n",
      "Epoch 109/150\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0407 - accuracy: 0.9861 - val_loss: 0.5888 - val_accuracy: 0.8761\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0407 - accuracy: 0.9861 - val_loss: 0.5888 - val_accuracy: 0.8761\n",
      "> [LR Scheduler] epoch 110: lr=0.000511\n",
      "Epoch 110/150\n",
      "> [LR Scheduler] epoch 110: lr=0.000511\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0371 - accuracy: 0.9870 - val_loss: 0.5869 - val_accuracy: 0.8796\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0371 - accuracy: 0.9870 - val_loss: 0.5869 - val_accuracy: 0.8796\n",
      "Epoch 111/150\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0358 - accuracy: 0.9874 - val_loss: 0.5954 - val_accuracy: 0.8809\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0358 - accuracy: 0.9874 - val_loss: 0.5954 - val_accuracy: 0.8809\n",
      "Epoch 112/150\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0365 - accuracy: 0.9872 - val_loss: 0.5881 - val_accuracy: 0.8764\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0365 - accuracy: 0.9872 - val_loss: 0.5881 - val_accuracy: 0.8764\n",
      "Epoch 113/150\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0326 - accuracy: 0.9893 - val_loss: 0.6078 - val_accuracy: 0.8799\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0326 - accuracy: 0.9893 - val_loss: 0.6078 - val_accuracy: 0.8799\n",
      "Epoch 114/150\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0303 - accuracy: 0.9902 - val_loss: 0.6192 - val_accuracy: 0.8776\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0303 - accuracy: 0.9902 - val_loss: 0.6192 - val_accuracy: 0.8776\n",
      "> [LR Scheduler] epoch 115: lr=0.000399\n",
      "Epoch 115/150\n",
      "> [LR Scheduler] epoch 115: lr=0.000399\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0307 - accuracy: 0.9897 - val_loss: 0.6120 - val_accuracy: 0.8766\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0307 - accuracy: 0.9897 - val_loss: 0.6120 - val_accuracy: 0.8766\n",
      "Epoch 116/150\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0303 - accuracy: 0.9898 - val_loss: 0.6376 - val_accuracy: 0.8713\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0303 - accuracy: 0.9898 - val_loss: 0.6376 - val_accuracy: 0.8713\n",
      "Epoch 117/150\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.0286 - accuracy: 0.9904 - val_loss: 0.6185 - val_accuracy: 0.8787\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.0286 - accuracy: 0.9904 - val_loss: 0.6185 - val_accuracy: 0.8787\n",
      "Epoch 118/150\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0285 - accuracy: 0.9900 - val_loss: 0.6334 - val_accuracy: 0.8772\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0285 - accuracy: 0.9900 - val_loss: 0.6334 - val_accuracy: 0.8772\n",
      "Epoch 119/150\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0257 - accuracy: 0.9916 - val_loss: 0.6293 - val_accuracy: 0.8813\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0257 - accuracy: 0.9916 - val_loss: 0.6293 - val_accuracy: 0.8813\n",
      "> [LR Scheduler] epoch 120: lr=0.000299\n",
      "Epoch 120/150\n",
      "> [LR Scheduler] epoch 120: lr=0.000299\n",
      "Epoch 120/150\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.0245 - accuracy: 0.9916 - val_loss: 0.6422 - val_accuracy: 0.8779\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.0245 - accuracy: 0.9916 - val_loss: 0.6422 - val_accuracy: 0.8779\n",
      "Epoch 121/150\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.0227 - accuracy: 0.9925 - val_loss: 0.6407 - val_accuracy: 0.8778\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.0227 - accuracy: 0.9925 - val_loss: 0.6407 - val_accuracy: 0.8778\n",
      "Epoch 122/150\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0222 - accuracy: 0.9927 - val_loss: 0.6510 - val_accuracy: 0.8788\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0222 - accuracy: 0.9927 - val_loss: 0.6510 - val_accuracy: 0.8788\n",
      "Epoch 123/150\n",
      "Epoch 123/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0221 - accuracy: 0.9930 - val_loss: 0.6584 - val_accuracy: 0.8759\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0221 - accuracy: 0.9930 - val_loss: 0.6584 - val_accuracy: 0.8759\n",
      "Epoch 124/150\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0206 - accuracy: 0.9934 - val_loss: 0.6540 - val_accuracy: 0.8804\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0206 - accuracy: 0.9934 - val_loss: 0.6540 - val_accuracy: 0.8804\n",
      "> [LR Scheduler] epoch 125: lr=0.000213\n",
      "Epoch 125/150\n",
      "> [LR Scheduler] epoch 125: lr=0.000213\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0199 - accuracy: 0.9933 - val_loss: 0.6438 - val_accuracy: 0.8793\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0199 - accuracy: 0.9933 - val_loss: 0.6438 - val_accuracy: 0.8793\n",
      "Epoch 126/150\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0191 - accuracy: 0.9942 - val_loss: 0.6456 - val_accuracy: 0.8801\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0191 - accuracy: 0.9942 - val_loss: 0.6456 - val_accuracy: 0.8801\n",
      "Epoch 127/150\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.0176 - accuracy: 0.9948 - val_loss: 0.6568 - val_accuracy: 0.8789\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.0176 - accuracy: 0.9948 - val_loss: 0.6568 - val_accuracy: 0.8789\n",
      "Epoch 128/150\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0185 - accuracy: 0.9946 - val_loss: 0.6540 - val_accuracy: 0.8797\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0185 - accuracy: 0.9946 - val_loss: 0.6540 - val_accuracy: 0.8797\n",
      "Epoch 129/150\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0186 - accuracy: 0.9942 - val_loss: 0.6554 - val_accuracy: 0.8790\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0186 - accuracy: 0.9942 - val_loss: 0.6554 - val_accuracy: 0.8790\n",
      "> [LR Scheduler] epoch 130: lr=0.000141\n",
      "Epoch 130/150\n",
      "> [LR Scheduler] epoch 130: lr=0.000141\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0169 - accuracy: 0.9951 - val_loss: 0.6583 - val_accuracy: 0.8808\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0169 - accuracy: 0.9951 - val_loss: 0.6583 - val_accuracy: 0.8808\n",
      "Epoch 131/150\n",
      "Epoch 131/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0165 - accuracy: 0.9950 - val_loss: 0.6543 - val_accuracy: 0.8805\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0165 - accuracy: 0.9950 - val_loss: 0.6543 - val_accuracy: 0.8805\n",
      "Epoch 132/150\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0166 - accuracy: 0.9947 - val_loss: 0.6547 - val_accuracy: 0.8810\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0166 - accuracy: 0.9947 - val_loss: 0.6547 - val_accuracy: 0.8810\n",
      "Epoch 133/150\n",
      "Epoch 133/150\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.0151 - accuracy: 0.9959 - val_loss: 0.6591 - val_accuracy: 0.8817\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.0151 - accuracy: 0.9959 - val_loss: 0.6591 - val_accuracy: 0.8817\n",
      "Epoch 134/150\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.0146 - accuracy: 0.9958 - val_loss: 0.6650 - val_accuracy: 0.8795\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.0146 - accuracy: 0.9958 - val_loss: 0.6650 - val_accuracy: 0.8795\n",
      "> [LR Scheduler] epoch 135: lr=0.000084\n",
      "Epoch 135/150\n",
      "> [LR Scheduler] epoch 135: lr=0.000084\n",
      "Epoch 135/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0149 - accuracy: 0.9955 - val_loss: 0.6656 - val_accuracy: 0.8795\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0149 - accuracy: 0.9955 - val_loss: 0.6656 - val_accuracy: 0.8795\n",
      "Epoch 136/150\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.0140 - accuracy: 0.9960 - val_loss: 0.6675 - val_accuracy: 0.8819\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.0140 - accuracy: 0.9960 - val_loss: 0.6675 - val_accuracy: 0.8819\n",
      "Epoch 137/150\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.0134 - accuracy: 0.9961 - val_loss: 0.6655 - val_accuracy: 0.8824\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.0134 - accuracy: 0.9961 - val_loss: 0.6655 - val_accuracy: 0.8824\n",
      "Epoch 138/150\n",
      "Epoch 138/150\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.0133 - accuracy: 0.9960 - val_loss: 0.6695 - val_accuracy: 0.8824\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.0133 - accuracy: 0.9960 - val_loss: 0.6695 - val_accuracy: 0.8824\n",
      "Epoch 139/150\n",
      "Epoch 139/150\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.0118 - accuracy: 0.9971 - val_loss: 0.6785 - val_accuracy: 0.8794\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.0118 - accuracy: 0.9971 - val_loss: 0.6785 - val_accuracy: 0.8794\n",
      "> [LR Scheduler] epoch 140: lr=0.000043\n",
      "Epoch 140/150\n",
      "> [LR Scheduler] epoch 140: lr=0.000043\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.0124 - accuracy: 0.9964 - val_loss: 0.6741 - val_accuracy: 0.8802\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.0124 - accuracy: 0.9964 - val_loss: 0.6741 - val_accuracy: 0.8802\n",
      "Epoch 141/150\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0132 - accuracy: 0.9964 - val_loss: 0.6749 - val_accuracy: 0.8786\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0132 - accuracy: 0.9964 - val_loss: 0.6749 - val_accuracy: 0.8786\n",
      "Epoch 142/150\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0122 - accuracy: 0.9966 - val_loss: 0.6770 - val_accuracy: 0.8792\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.0122 - accuracy: 0.9966 - val_loss: 0.6770 - val_accuracy: 0.8792\n",
      "Epoch 143/150\n",
      "Epoch 143/150\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.0122 - accuracy: 0.9964 - val_loss: 0.6778 - val_accuracy: 0.8812\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.0122 - accuracy: 0.9964 - val_loss: 0.6778 - val_accuracy: 0.8812\n",
      "Epoch 144/150\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.0120 - accuracy: 0.9969 - val_loss: 0.6765 - val_accuracy: 0.8807\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.0120 - accuracy: 0.9969 - val_loss: 0.6765 - val_accuracy: 0.8807\n",
      "> [LR Scheduler] epoch 145: lr=0.000018\n",
      "Epoch 145/150\n",
      "> [LR Scheduler] epoch 145: lr=0.000018\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.0116 - accuracy: 0.9970 - val_loss: 0.6787 - val_accuracy: 0.8802\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.0116 - accuracy: 0.9970 - val_loss: 0.6787 - val_accuracy: 0.8802\n",
      "Epoch 146/150\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.0116 - accuracy: 0.9967 - val_loss: 0.6799 - val_accuracy: 0.8803\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.0116 - accuracy: 0.9967 - val_loss: 0.6799 - val_accuracy: 0.8803\n",
      "Epoch 147/150\n",
      "Epoch 147/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0114 - accuracy: 0.9971 - val_loss: 0.6809 - val_accuracy: 0.8797\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0114 - accuracy: 0.9971 - val_loss: 0.6809 - val_accuracy: 0.8797\n",
      "Epoch 148/150\n",
      "Epoch 148/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0117 - accuracy: 0.9969 - val_loss: 0.6795 - val_accuracy: 0.8802\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0117 - accuracy: 0.9969 - val_loss: 0.6795 - val_accuracy: 0.8802\n",
      "Epoch 149/150\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0120 - accuracy: 0.9969 - val_loss: 0.6813 - val_accuracy: 0.8806\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.0120 - accuracy: 0.9969 - val_loss: 0.6813 - val_accuracy: 0.8806\n",
      "> [LR Scheduler] epoch 150: lr=0.000010\n",
      "Epoch 150/150\n",
      "> [LR Scheduler] epoch 150: lr=0.000010\n",
      "Epoch 150/150\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.6816 - val_accuracy: 0.8809\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.6816 - val_accuracy: 0.8809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x67f984670>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ds_train, epochs=EPOCHS, validation_data=ds_val, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_801\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " layer_normalization_4 (Lay  (None, 32, 32, 32)        64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " residual_block3x3_20 (Resi  (None, 32, 32, 32)        20736     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " max_pooling_layer_11 (MaxP  (None, 16, 16, 64)        2240      \n",
      " oolingLayer)                                                    \n",
      "                                                                 \n",
      " residual_block3x3_21 (Resi  (None, 16, 16, 64)        82432     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " residual_block3x3_22 (Resi  (None, 16, 16, 64)        82432     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " max_pooling_layer_12 (MaxP  (None, 8, 8, 128)         8576      \n",
      " oolingLayer)                                                    \n",
      "                                                                 \n",
      " residual_block3x3_23 (Resi  (None, 8, 8, 128)         328704    \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " residual_block3x3_24 (Resi  (None, 8, 8, 128)         328704    \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " residual_block3x3_25 (Resi  (None, 8, 8, 128)         328704    \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " global_average_pooling2d_4  (None, 128)               0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " layer_normalization_4 (Lay  (None, 32, 32, 32)        64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " residual_block3x3_20 (Resi  (None, 32, 32, 32)        20736     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " max_pooling_layer_11 (MaxP  (None, 16, 16, 64)        2240      \n",
      " oolingLayer)                                                    \n",
      "                                                                 \n",
      " residual_block3x3_21 (Resi  (None, 16, 16, 64)        82432     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " residual_block3x3_22 (Resi  (None, 16, 16, 64)        82432     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " max_pooling_layer_12 (MaxP  (None, 8, 8, 128)         8576      \n",
      " oolingLayer)                                                    \n",
      "                                                                 \n",
      " residual_block3x3_23 (Resi  (None, 8, 8, 128)         328704    \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " residual_block3x3_24 (Resi  (None, 8, 8, 128)         328704    \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " residual_block3x3_25 (Resi  (None, 8, 8, 128)         328704    \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " global_average_pooling2d_4  (None, 128)               0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1184778 (4.52 MB)\n",
      "Trainable params: 1184778 (4.52 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Total params: 1184778 (4.52 MB)\n",
      "Trainable params: 1184778 (4.52 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    CosineAnnealingScheduler(base_lr=3e-3, min_lr=1e-5, epochs=EPOCHS),\n",
    "]\n",
    "\n",
    "model = build_big_base_model_blocks(\n",
    "    input_shape=(32,32,3),\n",
    "    num_classes=10,\n",
    "    filters=FILTERS\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [LR Scheduler] epoch 1: lr=0.003000\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 20:57:46.533806: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - ETA: 0s - loss: 2.1734 - accuracy: 0.1668"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 20:58:21.996903: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 40s 91ms/step - loss: 2.1734 - accuracy: 0.1668 - val_loss: 1.8677 - val_accuracy: 0.2623\n",
      "Epoch 2/150\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 1.7291 - accuracy: 0.3448 - val_loss: 1.5294 - val_accuracy: 0.4371\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 1.7291 - accuracy: 0.3448 - val_loss: 1.5294 - val_accuracy: 0.4371\n",
      "Epoch 3/150\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 1.4115 - accuracy: 0.4818 - val_loss: 1.2994 - val_accuracy: 0.5198\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 1.4115 - accuracy: 0.4818 - val_loss: 1.2994 - val_accuracy: 0.5198\n",
      "Epoch 4/150\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 1.1723 - accuracy: 0.5758 - val_loss: 1.1105 - val_accuracy: 0.5901\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 1.1723 - accuracy: 0.5758 - val_loss: 1.1105 - val_accuracy: 0.5901\n",
      "> [LR Scheduler] epoch 5: lr=0.002995\n",
      "Epoch 5/150\n",
      "> [LR Scheduler] epoch 5: lr=0.002995\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 1.0370 - accuracy: 0.6251 - val_loss: 0.9599 - val_accuracy: 0.6546\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 1.0370 - accuracy: 0.6251 - val_loss: 0.9599 - val_accuracy: 0.6546\n",
      "Epoch 6/150\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.9428 - accuracy: 0.6622 - val_loss: 0.8946 - val_accuracy: 0.6774\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.9428 - accuracy: 0.6622 - val_loss: 0.8946 - val_accuracy: 0.6774\n",
      "Epoch 7/150\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 36s 91ms/step - loss: 0.8614 - accuracy: 0.6925 - val_loss: 0.8283 - val_accuracy: 0.7062\n",
      "391/391 [==============================] - 36s 91ms/step - loss: 0.8614 - accuracy: 0.6925 - val_loss: 0.8283 - val_accuracy: 0.7062\n",
      "Epoch 8/150\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.8056 - accuracy: 0.7139 - val_loss: 0.8377 - val_accuracy: 0.7015\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.8056 - accuracy: 0.7139 - val_loss: 0.8377 - val_accuracy: 0.7015\n",
      "Epoch 9/150\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.7516 - accuracy: 0.7338 - val_loss: 0.7499 - val_accuracy: 0.7359\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.7516 - accuracy: 0.7338 - val_loss: 0.7499 - val_accuracy: 0.7359\n",
      "> [LR Scheduler] epoch 10: lr=0.002973\n",
      "Epoch 10/150\n",
      "> [LR Scheduler] epoch 10: lr=0.002973\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.7000 - accuracy: 0.7540 - val_loss: 0.7372 - val_accuracy: 0.7436\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.7000 - accuracy: 0.7540 - val_loss: 0.7372 - val_accuracy: 0.7436\n",
      "Epoch 11/150\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.6529 - accuracy: 0.7696 - val_loss: 0.6706 - val_accuracy: 0.7639\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.6529 - accuracy: 0.7696 - val_loss: 0.6706 - val_accuracy: 0.7639\n",
      "Epoch 12/150\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.6130 - accuracy: 0.7854 - val_loss: 0.5964 - val_accuracy: 0.7964\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.6130 - accuracy: 0.7854 - val_loss: 0.5964 - val_accuracy: 0.7964\n",
      "Epoch 13/150\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.5791 - accuracy: 0.7976 - val_loss: 0.5640 - val_accuracy: 0.8030\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.5791 - accuracy: 0.7976 - val_loss: 0.5640 - val_accuracy: 0.8030\n",
      "Epoch 14/150\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.5385 - accuracy: 0.8124 - val_loss: 0.5496 - val_accuracy: 0.8113\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.5385 - accuracy: 0.8124 - val_loss: 0.5496 - val_accuracy: 0.8113\n",
      "> [LR Scheduler] epoch 15: lr=0.002935\n",
      "Epoch 15/150\n",
      "> [LR Scheduler] epoch 15: lr=0.002935\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.5083 - accuracy: 0.8227 - val_loss: 0.5786 - val_accuracy: 0.8018\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.5083 - accuracy: 0.8227 - val_loss: 0.5786 - val_accuracy: 0.8018\n",
      "Epoch 16/150\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.4859 - accuracy: 0.8294 - val_loss: 0.5214 - val_accuracy: 0.8213\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.4859 - accuracy: 0.8294 - val_loss: 0.5214 - val_accuracy: 0.8213\n",
      "Epoch 17/150\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.4635 - accuracy: 0.8386 - val_loss: 0.5301 - val_accuracy: 0.8214\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.4635 - accuracy: 0.8386 - val_loss: 0.5301 - val_accuracy: 0.8214\n",
      "Epoch 18/150\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.4420 - accuracy: 0.8457 - val_loss: 0.4760 - val_accuracy: 0.8349\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.4420 - accuracy: 0.8457 - val_loss: 0.4760 - val_accuracy: 0.8349\n",
      "Epoch 19/150\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4266 - accuracy: 0.8507 - val_loss: 0.5047 - val_accuracy: 0.8271\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4266 - accuracy: 0.8507 - val_loss: 0.5047 - val_accuracy: 0.8271\n",
      "> [LR Scheduler] epoch 20: lr=0.002882\n",
      "Epoch 20/150\n",
      "> [LR Scheduler] epoch 20: lr=0.002882\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4084 - accuracy: 0.8574 - val_loss: 0.4668 - val_accuracy: 0.8444\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4084 - accuracy: 0.8574 - val_loss: 0.4668 - val_accuracy: 0.8444\n",
      "Epoch 21/150\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.3878 - accuracy: 0.8643 - val_loss: 0.4795 - val_accuracy: 0.8385\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.3878 - accuracy: 0.8643 - val_loss: 0.4795 - val_accuracy: 0.8385\n",
      "Epoch 22/150\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3768 - accuracy: 0.8673 - val_loss: 0.4607 - val_accuracy: 0.8413\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3768 - accuracy: 0.8673 - val_loss: 0.4607 - val_accuracy: 0.8413\n",
      "Epoch 23/150\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3571 - accuracy: 0.8752 - val_loss: 0.4636 - val_accuracy: 0.8464\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3571 - accuracy: 0.8752 - val_loss: 0.4636 - val_accuracy: 0.8464\n",
      "Epoch 24/150\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3470 - accuracy: 0.8780 - val_loss: 0.4546 - val_accuracy: 0.8486\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3470 - accuracy: 0.8780 - val_loss: 0.4546 - val_accuracy: 0.8486\n",
      "> [LR Scheduler] epoch 25: lr=0.002813\n",
      "Epoch 25/150\n",
      "> [LR Scheduler] epoch 25: lr=0.002813\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3324 - accuracy: 0.8836 - val_loss: 0.4959 - val_accuracy: 0.8382\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3324 - accuracy: 0.8836 - val_loss: 0.4959 - val_accuracy: 0.8382\n",
      "Epoch 26/150\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3225 - accuracy: 0.8882 - val_loss: 0.4313 - val_accuracy: 0.8544\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3225 - accuracy: 0.8882 - val_loss: 0.4313 - val_accuracy: 0.8544\n",
      "Epoch 27/150\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.3125 - accuracy: 0.8893 - val_loss: 0.4416 - val_accuracy: 0.8527\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.3125 - accuracy: 0.8893 - val_loss: 0.4416 - val_accuracy: 0.8527\n",
      "Epoch 28/150\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2923 - accuracy: 0.8980 - val_loss: 0.4446 - val_accuracy: 0.8542\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2923 - accuracy: 0.8980 - val_loss: 0.4446 - val_accuracy: 0.8542\n",
      "Epoch 29/150\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2922 - accuracy: 0.8971 - val_loss: 0.4264 - val_accuracy: 0.8583\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2922 - accuracy: 0.8971 - val_loss: 0.4264 - val_accuracy: 0.8583\n",
      "> [LR Scheduler] epoch 30: lr=0.002729\n",
      "Epoch 30/150\n",
      "> [LR Scheduler] epoch 30: lr=0.002729\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2796 - accuracy: 0.9012 - val_loss: 0.4474 - val_accuracy: 0.8536\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2796 - accuracy: 0.9012 - val_loss: 0.4474 - val_accuracy: 0.8536\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2719 - accuracy: 0.9033 - val_loss: 0.4111 - val_accuracy: 0.8640\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2719 - accuracy: 0.9033 - val_loss: 0.4111 - val_accuracy: 0.8640\n",
      "Epoch 32/150\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2633 - accuracy: 0.9063 - val_loss: 0.4275 - val_accuracy: 0.8618\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2633 - accuracy: 0.9063 - val_loss: 0.4275 - val_accuracy: 0.8618\n",
      "Epoch 33/150\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.2563 - accuracy: 0.9085 - val_loss: 0.4250 - val_accuracy: 0.8626\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.2563 - accuracy: 0.9085 - val_loss: 0.4250 - val_accuracy: 0.8626\n",
      "Epoch 34/150\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.2451 - accuracy: 0.9131 - val_loss: 0.4078 - val_accuracy: 0.8656\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.2451 - accuracy: 0.9131 - val_loss: 0.4078 - val_accuracy: 0.8656\n",
      "> [LR Scheduler] epoch 35: lr=0.002632\n",
      "Epoch 35/150\n",
      "> [LR Scheduler] epoch 35: lr=0.002632\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.2347 - accuracy: 0.9163 - val_loss: 0.4234 - val_accuracy: 0.8629\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.2347 - accuracy: 0.9163 - val_loss: 0.4234 - val_accuracy: 0.8629\n",
      "Epoch 36/150\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.2346 - accuracy: 0.9170 - val_loss: 0.5033 - val_accuracy: 0.8481\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.2346 - accuracy: 0.9170 - val_loss: 0.5033 - val_accuracy: 0.8481\n",
      "Epoch 37/150\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.2234 - accuracy: 0.9201 - val_loss: 0.4306 - val_accuracy: 0.8646\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.2234 - accuracy: 0.9201 - val_loss: 0.4306 - val_accuracy: 0.8646\n",
      "Epoch 38/150\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.2144 - accuracy: 0.9237 - val_loss: 0.4198 - val_accuracy: 0.8708\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.2144 - accuracy: 0.9237 - val_loss: 0.4198 - val_accuracy: 0.8708\n",
      "Epoch 39/150\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.2103 - accuracy: 0.9251 - val_loss: 0.4358 - val_accuracy: 0.8633\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.2103 - accuracy: 0.9251 - val_loss: 0.4358 - val_accuracy: 0.8633\n",
      "> [LR Scheduler] epoch 40: lr=0.002522\n",
      "Epoch 40/150\n",
      "> [LR Scheduler] epoch 40: lr=0.002522\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.1954 - accuracy: 0.9304 - val_loss: 0.4187 - val_accuracy: 0.8725\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.1954 - accuracy: 0.9304 - val_loss: 0.4187 - val_accuracy: 0.8725\n",
      "Epoch 41/150\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.1926 - accuracy: 0.9308 - val_loss: 0.4254 - val_accuracy: 0.8678\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.1926 - accuracy: 0.9308 - val_loss: 0.4254 - val_accuracy: 0.8678\n",
      "Epoch 42/150\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.1880 - accuracy: 0.9326 - val_loss: 0.4094 - val_accuracy: 0.8776\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.1880 - accuracy: 0.9326 - val_loss: 0.4094 - val_accuracy: 0.8776\n",
      "Epoch 43/150\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.1806 - accuracy: 0.9358 - val_loss: 0.4129 - val_accuracy: 0.8758\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.1806 - accuracy: 0.9358 - val_loss: 0.4129 - val_accuracy: 0.8758\n",
      "Epoch 44/150\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.1753 - accuracy: 0.9363 - val_loss: 0.4133 - val_accuracy: 0.8769\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.1753 - accuracy: 0.9363 - val_loss: 0.4133 - val_accuracy: 0.8769\n",
      "> [LR Scheduler] epoch 45: lr=0.002401\n",
      "Epoch 45/150\n",
      "> [LR Scheduler] epoch 45: lr=0.002401\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.1729 - accuracy: 0.9381 - val_loss: 0.4267 - val_accuracy: 0.8740\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.1729 - accuracy: 0.9381 - val_loss: 0.4267 - val_accuracy: 0.8740\n",
      "Epoch 46/150\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.1662 - accuracy: 0.9401 - val_loss: 0.4463 - val_accuracy: 0.8721\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.1662 - accuracy: 0.9401 - val_loss: 0.4463 - val_accuracy: 0.8721\n",
      "Epoch 47/150\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.1580 - accuracy: 0.9434 - val_loss: 0.4294 - val_accuracy: 0.8761\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.1580 - accuracy: 0.9434 - val_loss: 0.4294 - val_accuracy: 0.8761\n",
      "Epoch 48/150\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.1547 - accuracy: 0.9452 - val_loss: 0.4280 - val_accuracy: 0.8768\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.1547 - accuracy: 0.9452 - val_loss: 0.4280 - val_accuracy: 0.8768\n",
      "Epoch 49/150\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.1476 - accuracy: 0.9476 - val_loss: 0.4211 - val_accuracy: 0.8762\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.1476 - accuracy: 0.9476 - val_loss: 0.4211 - val_accuracy: 0.8762\n",
      "> [LR Scheduler] epoch 50: lr=0.002271\n",
      "Epoch 50/150\n",
      "> [LR Scheduler] epoch 50: lr=0.002271\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.1493 - accuracy: 0.9470 - val_loss: 0.4379 - val_accuracy: 0.8763\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.1493 - accuracy: 0.9470 - val_loss: 0.4379 - val_accuracy: 0.8763\n",
      "Epoch 51/150\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.1392 - accuracy: 0.9504 - val_loss: 0.4288 - val_accuracy: 0.8812\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.1392 - accuracy: 0.9504 - val_loss: 0.4288 - val_accuracy: 0.8812\n",
      "Epoch 52/150\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.1335 - accuracy: 0.9516 - val_loss: 0.4114 - val_accuracy: 0.8809\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.1335 - accuracy: 0.9516 - val_loss: 0.4114 - val_accuracy: 0.8809\n",
      "Epoch 53/150\n",
      "Epoch 53/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.1295 - accuracy: 0.9548 - val_loss: 0.4533 - val_accuracy: 0.8776\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.1295 - accuracy: 0.9548 - val_loss: 0.4533 - val_accuracy: 0.8776\n",
      "Epoch 54/150\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.1248 - accuracy: 0.9560 - val_loss: 0.4573 - val_accuracy: 0.8764\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.1248 - accuracy: 0.9560 - val_loss: 0.4573 - val_accuracy: 0.8764\n",
      "> [LR Scheduler] epoch 55: lr=0.002131\n",
      "Epoch 55/150\n",
      "> [LR Scheduler] epoch 55: lr=0.002131\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.1199 - accuracy: 0.9576 - val_loss: 0.4331 - val_accuracy: 0.8806\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.1199 - accuracy: 0.9576 - val_loss: 0.4331 - val_accuracy: 0.8806\n",
      "Epoch 56/150\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.1171 - accuracy: 0.9580 - val_loss: 0.4284 - val_accuracy: 0.8837\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.1171 - accuracy: 0.9580 - val_loss: 0.4284 - val_accuracy: 0.8837\n",
      "Epoch 57/150\n",
      "Epoch 57/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.1133 - accuracy: 0.9596 - val_loss: 0.4640 - val_accuracy: 0.8764\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.1133 - accuracy: 0.9596 - val_loss: 0.4640 - val_accuracy: 0.8764\n",
      "Epoch 58/150\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.1119 - accuracy: 0.9594 - val_loss: 0.4913 - val_accuracy: 0.8719\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.1119 - accuracy: 0.9594 - val_loss: 0.4913 - val_accuracy: 0.8719\n",
      "Epoch 59/150\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.1038 - accuracy: 0.9619 - val_loss: 0.4585 - val_accuracy: 0.8790\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.1038 - accuracy: 0.9619 - val_loss: 0.4585 - val_accuracy: 0.8790\n",
      "> [LR Scheduler] epoch 60: lr=0.001985\n",
      "Epoch 60/150\n",
      "> [LR Scheduler] epoch 60: lr=0.001985\n",
      "Epoch 60/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0960 - accuracy: 0.9649 - val_loss: 0.4531 - val_accuracy: 0.8866\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0960 - accuracy: 0.9649 - val_loss: 0.4531 - val_accuracy: 0.8866\n",
      "Epoch 61/150\n",
      "Epoch 61/150\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.0983 - accuracy: 0.9641 - val_loss: 0.4790 - val_accuracy: 0.8777\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.0983 - accuracy: 0.9641 - val_loss: 0.4790 - val_accuracy: 0.8777\n",
      "Epoch 62/150\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.0929 - accuracy: 0.9666 - val_loss: 0.4725 - val_accuracy: 0.8799\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.0929 - accuracy: 0.9666 - val_loss: 0.4725 - val_accuracy: 0.8799\n",
      "Epoch 63/150\n",
      "Epoch 63/150\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 0.0902 - accuracy: 0.9675 - val_loss: 0.4494 - val_accuracy: 0.8818\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 0.0902 - accuracy: 0.9675 - val_loss: 0.4494 - val_accuracy: 0.8818\n",
      "Epoch 64/150\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0897 - accuracy: 0.9674 - val_loss: 0.4328 - val_accuracy: 0.8875\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0897 - accuracy: 0.9674 - val_loss: 0.4328 - val_accuracy: 0.8875\n",
      "> [LR Scheduler] epoch 65: lr=0.001833\n",
      "Epoch 65/150\n",
      "> [LR Scheduler] epoch 65: lr=0.001833\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 0.0854 - accuracy: 0.9688 - val_loss: 0.4508 - val_accuracy: 0.8860\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 0.0854 - accuracy: 0.9688 - val_loss: 0.4508 - val_accuracy: 0.8860\n",
      "Epoch 66/150\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0763 - accuracy: 0.9727 - val_loss: 0.5151 - val_accuracy: 0.8801\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0763 - accuracy: 0.9727 - val_loss: 0.5151 - val_accuracy: 0.8801\n",
      "Epoch 67/150\n",
      "Epoch 67/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0768 - accuracy: 0.9724 - val_loss: 0.5040 - val_accuracy: 0.8790\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0768 - accuracy: 0.9724 - val_loss: 0.5040 - val_accuracy: 0.8790\n",
      "Epoch 68/150\n",
      "Epoch 68/150\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.0786 - accuracy: 0.9724 - val_loss: 0.4664 - val_accuracy: 0.8885\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.0786 - accuracy: 0.9724 - val_loss: 0.4664 - val_accuracy: 0.8885\n",
      "Epoch 69/150\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.0757 - accuracy: 0.9724 - val_loss: 0.4888 - val_accuracy: 0.8814\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.0757 - accuracy: 0.9724 - val_loss: 0.4888 - val_accuracy: 0.8814\n",
      "> [LR Scheduler] epoch 70: lr=0.001678\n",
      "Epoch 70/150\n",
      "> [LR Scheduler] epoch 70: lr=0.001678\n",
      "Epoch 70/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0669 - accuracy: 0.9762 - val_loss: 0.4858 - val_accuracy: 0.8864\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0669 - accuracy: 0.9762 - val_loss: 0.4858 - val_accuracy: 0.8864\n",
      "Epoch 71/150\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0705 - accuracy: 0.9748 - val_loss: 0.4971 - val_accuracy: 0.8859\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0705 - accuracy: 0.9748 - val_loss: 0.4971 - val_accuracy: 0.8859\n",
      "Epoch 72/150\n",
      "Epoch 72/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0601 - accuracy: 0.9784 - val_loss: 0.5006 - val_accuracy: 0.8803\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0601 - accuracy: 0.9784 - val_loss: 0.5006 - val_accuracy: 0.8803\n",
      "Epoch 73/150\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0643 - accuracy: 0.9763 - val_loss: 0.4935 - val_accuracy: 0.8862\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0643 - accuracy: 0.9763 - val_loss: 0.4935 - val_accuracy: 0.8862\n",
      "Epoch 74/150\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0614 - accuracy: 0.9772 - val_loss: 0.4893 - val_accuracy: 0.8862\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0614 - accuracy: 0.9772 - val_loss: 0.4893 - val_accuracy: 0.8862\n",
      "> [LR Scheduler] epoch 75: lr=0.001521\n",
      "Epoch 75/150\n",
      "> [LR Scheduler] epoch 75: lr=0.001521\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0548 - accuracy: 0.9803 - val_loss: 0.4755 - val_accuracy: 0.8908\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0548 - accuracy: 0.9803 - val_loss: 0.4755 - val_accuracy: 0.8908\n",
      "Epoch 76/150\n",
      "Epoch 76/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0528 - accuracy: 0.9810 - val_loss: 0.5032 - val_accuracy: 0.8891\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0528 - accuracy: 0.9810 - val_loss: 0.5032 - val_accuracy: 0.8891\n",
      "Epoch 77/150\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0528 - accuracy: 0.9811 - val_loss: 0.4971 - val_accuracy: 0.8885\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0528 - accuracy: 0.9811 - val_loss: 0.4971 - val_accuracy: 0.8885\n",
      "Epoch 78/150\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0492 - accuracy: 0.9820 - val_loss: 0.4963 - val_accuracy: 0.8901\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0492 - accuracy: 0.9820 - val_loss: 0.4963 - val_accuracy: 0.8901\n",
      "Epoch 79/150\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0509 - accuracy: 0.9821 - val_loss: 0.5075 - val_accuracy: 0.8860\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0509 - accuracy: 0.9821 - val_loss: 0.5075 - val_accuracy: 0.8860\n",
      "> [LR Scheduler] epoch 80: lr=0.001363\n",
      "Epoch 80/150\n",
      "> [LR Scheduler] epoch 80: lr=0.001363\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0425 - accuracy: 0.9844 - val_loss: 0.5148 - val_accuracy: 0.8893\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0425 - accuracy: 0.9844 - val_loss: 0.5148 - val_accuracy: 0.8893\n",
      "Epoch 81/150\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0446 - accuracy: 0.9840 - val_loss: 0.5129 - val_accuracy: 0.8875\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0446 - accuracy: 0.9840 - val_loss: 0.5129 - val_accuracy: 0.8875\n",
      "Epoch 82/150\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 0.0389 - accuracy: 0.9865 - val_loss: 0.5258 - val_accuracy: 0.8870\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 0.0389 - accuracy: 0.9865 - val_loss: 0.5258 - val_accuracy: 0.8870\n",
      "Epoch 83/150\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0425 - accuracy: 0.9849 - val_loss: 0.5512 - val_accuracy: 0.8874\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0425 - accuracy: 0.9849 - val_loss: 0.5512 - val_accuracy: 0.8874\n",
      "Epoch 84/150\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0387 - accuracy: 0.9868 - val_loss: 0.5435 - val_accuracy: 0.8904\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0387 - accuracy: 0.9868 - val_loss: 0.5435 - val_accuracy: 0.8904\n",
      "> [LR Scheduler] epoch 85: lr=0.001208\n",
      "Epoch 85/150\n",
      "> [LR Scheduler] epoch 85: lr=0.001208\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0419 - accuracy: 0.9856 - val_loss: 0.5138 - val_accuracy: 0.8908\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0419 - accuracy: 0.9856 - val_loss: 0.5138 - val_accuracy: 0.8908\n",
      "Epoch 86/150\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0373 - accuracy: 0.9871 - val_loss: 0.5576 - val_accuracy: 0.8857\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0373 - accuracy: 0.9871 - val_loss: 0.5576 - val_accuracy: 0.8857\n",
      "Epoch 87/150\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 0.0377 - accuracy: 0.9867 - val_loss: 0.5415 - val_accuracy: 0.8876\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 0.0377 - accuracy: 0.9867 - val_loss: 0.5415 - val_accuracy: 0.8876\n",
      "Epoch 88/150\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0310 - accuracy: 0.9898 - val_loss: 0.5361 - val_accuracy: 0.8897\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0310 - accuracy: 0.9898 - val_loss: 0.5361 - val_accuracy: 0.8897\n",
      "Epoch 89/150\n",
      "Epoch 89/150\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.0335 - accuracy: 0.9883 - val_loss: 0.5377 - val_accuracy: 0.8905\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.0335 - accuracy: 0.9883 - val_loss: 0.5377 - val_accuracy: 0.8905\n",
      "> [LR Scheduler] epoch 90: lr=0.001055\n",
      "Epoch 90/150\n",
      "> [LR Scheduler] epoch 90: lr=0.001055\n",
      "Epoch 90/150\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.0299 - accuracy: 0.9900 - val_loss: 0.5429 - val_accuracy: 0.8927\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.0299 - accuracy: 0.9900 - val_loss: 0.5429 - val_accuracy: 0.8927\n",
      "Epoch 91/150\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.0251 - accuracy: 0.9917 - val_loss: 0.5561 - val_accuracy: 0.8908\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.0251 - accuracy: 0.9917 - val_loss: 0.5561 - val_accuracy: 0.8908\n",
      "Epoch 92/150\n",
      "Epoch 92/150\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.0237 - accuracy: 0.9918 - val_loss: 0.5638 - val_accuracy: 0.8915\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.0237 - accuracy: 0.9918 - val_loss: 0.5638 - val_accuracy: 0.8915\n",
      "Epoch 93/150\n",
      "Epoch 93/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0271 - accuracy: 0.9904 - val_loss: 0.5708 - val_accuracy: 0.8877\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0271 - accuracy: 0.9904 - val_loss: 0.5708 - val_accuracy: 0.8877\n",
      "Epoch 94/150\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0247 - accuracy: 0.9916 - val_loss: 0.5486 - val_accuracy: 0.8915\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0247 - accuracy: 0.9916 - val_loss: 0.5486 - val_accuracy: 0.8915\n",
      "> [LR Scheduler] epoch 95: lr=0.000908\n",
      "Epoch 95/150\n",
      "> [LR Scheduler] epoch 95: lr=0.000908\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0227 - accuracy: 0.9924 - val_loss: 0.5697 - val_accuracy: 0.8891\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0227 - accuracy: 0.9924 - val_loss: 0.5697 - val_accuracy: 0.8891\n",
      "Epoch 96/150\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0232 - accuracy: 0.9920 - val_loss: 0.5586 - val_accuracy: 0.8914\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0232 - accuracy: 0.9920 - val_loss: 0.5586 - val_accuracy: 0.8914\n",
      "Epoch 97/150\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0222 - accuracy: 0.9922 - val_loss: 0.5626 - val_accuracy: 0.8926\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0222 - accuracy: 0.9922 - val_loss: 0.5626 - val_accuracy: 0.8926\n",
      "Epoch 98/150\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 0.0183 - accuracy: 0.9937 - val_loss: 0.5820 - val_accuracy: 0.8924\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 0.0183 - accuracy: 0.9937 - val_loss: 0.5820 - val_accuracy: 0.8924\n",
      "Epoch 99/150\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0164 - accuracy: 0.9947 - val_loss: 0.5774 - val_accuracy: 0.8934\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0164 - accuracy: 0.9947 - val_loss: 0.5774 - val_accuracy: 0.8934\n",
      "> [LR Scheduler] epoch 100: lr=0.000767\n",
      "Epoch 100/150\n",
      "> [LR Scheduler] epoch 100: lr=0.000767\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0188 - accuracy: 0.9937 - val_loss: 0.5954 - val_accuracy: 0.8902\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0188 - accuracy: 0.9937 - val_loss: 0.5954 - val_accuracy: 0.8902\n",
      "Epoch 101/150\n",
      "Epoch 101/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0154 - accuracy: 0.9949 - val_loss: 0.6058 - val_accuracy: 0.8924\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0154 - accuracy: 0.9949 - val_loss: 0.6058 - val_accuracy: 0.8924\n",
      "Epoch 102/150\n",
      "Epoch 102/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0158 - accuracy: 0.9948 - val_loss: 0.5762 - val_accuracy: 0.8955\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0158 - accuracy: 0.9948 - val_loss: 0.5762 - val_accuracy: 0.8955\n",
      "Epoch 103/150\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0137 - accuracy: 0.9955 - val_loss: 0.5922 - val_accuracy: 0.8957\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0137 - accuracy: 0.9955 - val_loss: 0.5922 - val_accuracy: 0.8957\n",
      "Epoch 104/150\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0141 - accuracy: 0.9953 - val_loss: 0.6098 - val_accuracy: 0.8915\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0141 - accuracy: 0.9953 - val_loss: 0.6098 - val_accuracy: 0.8915\n",
      "> [LR Scheduler] epoch 105: lr=0.000634\n",
      "Epoch 105/150\n",
      "> [LR Scheduler] epoch 105: lr=0.000634\n",
      "Epoch 105/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0106 - accuracy: 0.9968 - val_loss: 0.6062 - val_accuracy: 0.8923\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0106 - accuracy: 0.9968 - val_loss: 0.6062 - val_accuracy: 0.8923\n",
      "Epoch 106/150\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0136 - accuracy: 0.9956 - val_loss: 0.6090 - val_accuracy: 0.8937\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0136 - accuracy: 0.9956 - val_loss: 0.6090 - val_accuracy: 0.8937\n",
      "Epoch 107/150\n",
      "Epoch 107/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0128 - accuracy: 0.9956 - val_loss: 0.5908 - val_accuracy: 0.8932\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0128 - accuracy: 0.9956 - val_loss: 0.5908 - val_accuracy: 0.8932\n",
      "Epoch 108/150\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.6199 - val_accuracy: 0.8935\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.6199 - val_accuracy: 0.8935\n",
      "Epoch 109/150\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0099 - accuracy: 0.9971 - val_loss: 0.6071 - val_accuracy: 0.8951\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0099 - accuracy: 0.9971 - val_loss: 0.6071 - val_accuracy: 0.8951\n",
      "> [LR Scheduler] epoch 110: lr=0.000511\n",
      "Epoch 110/150\n",
      "> [LR Scheduler] epoch 110: lr=0.000511\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.6215 - val_accuracy: 0.8948\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.6215 - val_accuracy: 0.8948\n",
      "Epoch 111/150\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 0.0093 - accuracy: 0.9969 - val_loss: 0.6177 - val_accuracy: 0.8964\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 0.0093 - accuracy: 0.9969 - val_loss: 0.6177 - val_accuracy: 0.8964\n",
      "Epoch 112/150\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.6314 - val_accuracy: 0.8970\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.6314 - val_accuracy: 0.8970\n",
      "Epoch 113/150\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.6195 - val_accuracy: 0.8962\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.6195 - val_accuracy: 0.8962\n",
      "Epoch 114/150\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.6534 - val_accuracy: 0.8894\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.6534 - val_accuracy: 0.8894\n",
      "> [LR Scheduler] epoch 115: lr=0.000399\n",
      "Epoch 115/150\n",
      "> [LR Scheduler] epoch 115: lr=0.000399\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.6372 - val_accuracy: 0.8945\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.6372 - val_accuracy: 0.8945\n",
      "Epoch 116/150\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.6433 - val_accuracy: 0.8956\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.6433 - val_accuracy: 0.8956\n",
      "Epoch 117/150\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.6237 - val_accuracy: 0.8945\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.6237 - val_accuracy: 0.8945\n",
      "Epoch 118/150\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.6325 - val_accuracy: 0.8966\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.6325 - val_accuracy: 0.8966\n",
      "Epoch 119/150\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0056 - accuracy: 0.9985 - val_loss: 0.6445 - val_accuracy: 0.8956\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0056 - accuracy: 0.9985 - val_loss: 0.6445 - val_accuracy: 0.8956\n",
      "> [LR Scheduler] epoch 120: lr=0.000299\n",
      "Epoch 120/150\n",
      "> [LR Scheduler] epoch 120: lr=0.000299\n",
      "Epoch 120/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.6482 - val_accuracy: 0.8957\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.6482 - val_accuracy: 0.8957\n",
      "Epoch 121/150\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.6370 - val_accuracy: 0.8973\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.6370 - val_accuracy: 0.8973\n",
      "Epoch 122/150\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.6344 - val_accuracy: 0.8956\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.6344 - val_accuracy: 0.8956\n",
      "Epoch 123/150\n",
      "Epoch 123/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.6526 - val_accuracy: 0.8969\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.6526 - val_accuracy: 0.8969\n",
      "Epoch 124/150\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.6396 - val_accuracy: 0.8961\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.6396 - val_accuracy: 0.8961\n",
      "> [LR Scheduler] epoch 125: lr=0.000213\n",
      "Epoch 125/150\n",
      "> [LR Scheduler] epoch 125: lr=0.000213\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.6530 - val_accuracy: 0.8951\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.6530 - val_accuracy: 0.8951\n",
      "Epoch 126/150\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.6669 - val_accuracy: 0.8951\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.6669 - val_accuracy: 0.8951\n",
      "Epoch 127/150\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.6593 - val_accuracy: 0.8957\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.6593 - val_accuracy: 0.8957\n",
      "Epoch 128/150\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.6632 - val_accuracy: 0.8971\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.6632 - val_accuracy: 0.8971\n",
      "Epoch 129/150\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.6709 - val_accuracy: 0.8961\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.6709 - val_accuracy: 0.8961\n",
      "> [LR Scheduler] epoch 130: lr=0.000141\n",
      "Epoch 130/150\n",
      "> [LR Scheduler] epoch 130: lr=0.000141\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.6609 - val_accuracy: 0.8984\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.6609 - val_accuracy: 0.8984\n",
      "Epoch 131/150\n",
      "Epoch 131/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.6638 - val_accuracy: 0.8959\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.6638 - val_accuracy: 0.8959\n",
      "Epoch 132/150\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.6761 - val_accuracy: 0.8967\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.6761 - val_accuracy: 0.8967\n",
      "Epoch 133/150\n",
      "Epoch 133/150\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.6775 - val_accuracy: 0.8971\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.6775 - val_accuracy: 0.8971\n",
      "Epoch 134/150\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.6775 - val_accuracy: 0.8982\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.6775 - val_accuracy: 0.8982\n",
      "> [LR Scheduler] epoch 135: lr=0.000084\n",
      "Epoch 135/150\n",
      "> [LR Scheduler] epoch 135: lr=0.000084\n",
      "Epoch 135/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.6836 - val_accuracy: 0.8985\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.6836 - val_accuracy: 0.8985\n",
      "Epoch 136/150\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.6786 - val_accuracy: 0.8977\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.6786 - val_accuracy: 0.8977\n",
      "Epoch 137/150\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.6802 - val_accuracy: 0.8971\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.6802 - val_accuracy: 0.8971\n",
      "Epoch 138/150\n",
      "Epoch 138/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.6805 - val_accuracy: 0.8962\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.6805 - val_accuracy: 0.8962\n",
      "Epoch 139/150\n",
      "Epoch 139/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.6891 - val_accuracy: 0.8975\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.6891 - val_accuracy: 0.8975\n",
      "> [LR Scheduler] epoch 140: lr=0.000043\n",
      "Epoch 140/150\n",
      "> [LR Scheduler] epoch 140: lr=0.000043\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.6893 - val_accuracy: 0.8975\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.6893 - val_accuracy: 0.8975\n",
      "Epoch 141/150\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.6865 - val_accuracy: 0.8980\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.6865 - val_accuracy: 0.8980\n",
      "Epoch 142/150\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.6842 - val_accuracy: 0.8972\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.6842 - val_accuracy: 0.8972\n",
      "Epoch 143/150\n",
      "Epoch 143/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.6875 - val_accuracy: 0.8974\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.6875 - val_accuracy: 0.8974\n",
      "Epoch 144/150\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.6858 - val_accuracy: 0.8977\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.6858 - val_accuracy: 0.8977\n",
      "> [LR Scheduler] epoch 145: lr=0.000018\n",
      "Epoch 145/150\n",
      "> [LR Scheduler] epoch 145: lr=0.000018\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 9.8422e-04 - accuracy: 0.9998 - val_loss: 0.6851 - val_accuracy: 0.8982\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 9.8422e-04 - accuracy: 0.9998 - val_loss: 0.6851 - val_accuracy: 0.8982\n",
      "Epoch 146/150\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.6844 - val_accuracy: 0.8978\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.6844 - val_accuracy: 0.8978\n",
      "Epoch 147/150\n",
      "Epoch 147/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.6859 - val_accuracy: 0.8984\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.6859 - val_accuracy: 0.8984\n",
      "Epoch 148/150\n",
      "Epoch 148/150\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.6848 - val_accuracy: 0.8973\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.6848 - val_accuracy: 0.8973\n",
      "Epoch 149/150\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.6858 - val_accuracy: 0.8978\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.6858 - val_accuracy: 0.8978\n",
      "> [LR Scheduler] epoch 150: lr=0.000010\n",
      "Epoch 150/150\n",
      "> [LR Scheduler] epoch 150: lr=0.000010\n",
      "Epoch 150/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.6870 - val_accuracy: 0.8972\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.6870 - val_accuracy: 0.8972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x397ba5100>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ds_train, epochs=EPOCHS, validation_data=ds_val, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

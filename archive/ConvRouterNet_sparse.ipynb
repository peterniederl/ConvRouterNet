{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CGANs - Conditional Generative Adversarial Nets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, Dense, GlobalAveragePooling2D, LayerNormalization, Add, DepthwiseConv2D, MaxPool2D\n",
    ")\n",
    "from utils import PoolingLayer, ResidualBlock, ResidualBlock3x3, ResidualBlock5x5, ResidualBlock7x7, SpatialSE, ChannelSE, ResidualBlockDepthwise3x3, ResidualBlockDepthwise5x5, ResidualBlockDepthwise7x7\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Softmax Router (no Gumbel). Optional hard mode at inference.\n",
    "# ---------------------------------------------------------\n",
    "class SoftmaxRouter(layers.Layer):\n",
    "    def __init__(self, num_choices, hard_at_inference=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_choices = num_choices\n",
    "        self.hard_at_inference = hard_at_inference\n",
    "        self.logits_layer = Dense(num_choices)\n",
    "\n",
    "    def call(self, features, training=None):\n",
    "        logits = self.logits_layer(GlobalAveragePooling2D()(features))  # (B, K)\n",
    "        if training or not self.hard_at_inference:\n",
    "            probs = tf.nn.softmax(logits, axis=-1)                      # (B, K)\n",
    "        else:\n",
    "            idx = tf.argmax(logits, axis=-1)\n",
    "            probs = tf.one_hot(idx, depth=self.num_choices, dtype=tf.float32)\n",
    "        return probs  # (B, K)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Halting Head: outputs probability to halt at current step\n",
    "# ---------------------------------------------------------\n",
    "class HaltingHead(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.halt_dense = Dense(1)\n",
    "\n",
    "    def call(self, features):\n",
    "        h = GlobalAveragePooling2D()(features)\n",
    "        p_halt = tf.nn.sigmoid(self.halt_dense(h))  # (B, 1)\n",
    "        return p_halt\n",
    "    \n",
    "def match_mask(mask, ref):\n",
    "    # mask: [B,1,1,1] or [B,1]\n",
    "    # ref : [B,H,W,C]\n",
    "    mask = tf.reshape(mask, (-1, 1, 1, 1))          # ensure rank-4\n",
    "    return mask * tf.ones_like(ref[..., :1])        # broadcast to [B,H,W,1]\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Tiny conv stem\n",
    "# ---------------------------\n",
    "class ConvStem(layers.Layer):\n",
    "    def __init__(self, out_ch, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.conv = layers.Conv2D(out_ch, 3, padding=\"same\", use_bias=False)\n",
    "        self.norm = layers.LayerNormalization()\n",
    "        self.act  = layers.Activation(\"swish\")\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x, training=training)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Multi-head attention pooling router\n",
    "# Produces logits over K experts\n",
    "# ---------------------------\n",
    "class AttnPoolRouter(layers.Layer):\n",
    "    def __init__(self, K, heads=2, dim_head=64, mlp_hidden=64, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.K = int(K)\n",
    "        self.heads = int(heads)\n",
    "        self.dim_head = int(dim_head)\n",
    "        self.mlp_hidden = int(mlp_hidden)\n",
    "\n",
    "        self.q = self.add_weight(\n",
    "            name=\"queries\", shape=(self.heads, self.dim_head),\n",
    "            initializer=\"glorot_uniform\", trainable=True)\n",
    "\n",
    "        self.key_proj = layers.Conv2D(self.heads*self.dim_head, 1, use_bias=False)\n",
    "        self.val_proj = layers.Conv2D(self.heads*self.dim_head, 1, use_bias=False)\n",
    "\n",
    "        # Head aggregator -> K logits\n",
    "        if self.mlp_hidden > 0:\n",
    "            self.head_mlp = keras.Sequential([\n",
    "                layers.Dense(self.mlp_hidden, activation=\"swish\"),\n",
    "                layers.Dense(self.K)\n",
    "            ])\n",
    "        else:\n",
    "            self.head_mlp = layers.Dense(self.K)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        # x: [B,H,W,C]\n",
    "        B, H, W, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "        k = self.key_proj(x)  # [B,H,W,heads*dim]\n",
    "        v = self.val_proj(x)\n",
    "        k = tf.reshape(k, [B, H*W, self.heads, self.dim_head])  # [B,HW,Hd,D]\n",
    "        v = tf.reshape(v, [B, H*W, self.heads, self.dim_head])  # [B,HW,Hd,D]\n",
    "        k = tf.transpose(k, [0,2,1,3])  # [B,heads,HW,dim]\n",
    "        v = tf.transpose(v, [0,2,1,3])  # [B,heads,HW,dim]\n",
    "\n",
    "        # queries: [heads, dim] -> [B,heads,1,dim]\n",
    "        q = tf.expand_dims(self.q, axis=0)\n",
    "        q = tf.expand_dims(q, axis=2)\n",
    "\n",
    "        # attn: [B,heads,1,HW]\n",
    "        attn = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(self.dim_head, x.dtype))\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "\n",
    "        # pooled heads: [B,heads,1,dim]\n",
    "        pooled = tf.matmul(attn, v)  # [B,heads,1,dim]\n",
    "        pooled = tf.squeeze(pooled, axis=2)  # [B,heads,dim]\n",
    "\n",
    "        # flatten heads\n",
    "        pooled = tf.reshape(pooled, [B, self.heads*self.dim_head])  # [B, heads*dim]\n",
    "\n",
    "        logits = self.head_mlp(pooled, training=training)  # [B,K]\n",
    "        return logits, pooled  # pooled can be used as a feature if needed\n",
    "\n",
    "\n",
    "class HaltingClassifierHead(layers.Layer):\n",
    "    \"\"\"\n",
    "    Predicts class probabilities; halts when max prob > tau.\n",
    "    ST gating: forward uses hard threshold; backward uses a smooth sigmoid around tau.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, hidden=64, halt_temp=3.0, tau=0.8, bias_init=-2.5, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.num_classes = int(num_classes)\n",
    "        self.hidden = int(hidden)\n",
    "        self.halt_temp = float(halt_temp)\n",
    "        self.tau = float(tau)\n",
    "\n",
    "        if hidden > 0:\n",
    "            self.classifier = keras.Sequential([\n",
    "                layers.GlobalAveragePooling2D(),\n",
    "                layers.Dense(hidden, activation=\"swish\"),\n",
    "                layers.Dense(num_classes)\n",
    "            ])\n",
    "        else:\n",
    "            self.classifier = keras.Sequential([\n",
    "                layers.GlobalAveragePooling2D(),\n",
    "                layers.Dense(num_classes)\n",
    "            ])\n",
    "\n",
    "        # a tiny scalar bias we add to (max_prob - tau) before the sigmoid\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"halt_bias\", shape=(), initializer=tf.keras.initializers.Constant(bias_init),\n",
    "            trainable=True)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        logits = self.classifier(x, training=training)          # [B,C]\n",
    "        probs  = tf.nn.softmax(logits, axis=-1)                 # [B,C]\n",
    "        maxp   = tf.reduce_max(probs, axis=-1, keepdims=True)   # [B,1]\n",
    "        tau = tf.cast(self.tau, maxp.dtype)\n",
    "        halt_temp = tf.cast(self.halt_temp, maxp.dtype)\n",
    "        z = (maxp - tau) / tf.maximum(tf.constant(1e-6, dtype=maxp.dtype), halt_temp)\n",
    "        p_soft = tf.nn.sigmoid(z + tf.cast(self.bias, maxp.dtype))                   # [B,1]\n",
    "        p_hard = tf.cast(maxp > tau, x.dtype)              # [B,1]\n",
    "        p_st = p_hard + tf.stop_gradient(p_soft - p_hard)\n",
    "        return probs, p_soft, p_hard, p_st   # class probs, soft gate, hard gate, ST gate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdaptiveRouterBlockTop1Sparse(layers.Layer):\n",
    "    \"\"\"\n",
    "    Top-1 only, faster variant of the original block.\n",
    "\n",
    "    - Uses AttnPoolRouter to pick a single expert per running sample (argmax).\n",
    "    - For each unique selected expert we gather its subset once, run the expert,\n",
    "      scale by the soft routing probability for that expert and scatter back.\n",
    "    - Removes TensorArrays and the Top-2 logic for a simpler, faster graph.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 branches,\n",
    "                 num_classes,\n",
    "                 pooling_layer=None,\n",
    "                 pooling_layers=None,\n",
    "                 unique_pools=False,\n",
    "                 min_steps=1, max_steps=6, pool_every_n=1,\n",
    "                 router_heads=2, router_dim=64, router_mlp=64,\n",
    "                 route_temp=1.5, halt_temp=3.0, halt_tau=0.8,\n",
    "                 lb_alpha=1e-3, div_alpha=1e-4,\n",
    "                 top_k=1,                     # force top1\n",
    "                 name=None):\n",
    "        super().__init__(name=name)\n",
    "        assert 1 <= min_steps <= max_steps\n",
    "        # top_k kept for compatibility but only 1 is supported here\n",
    "        assert top_k == 1, \"This optimized block supports top_k==1 only\"\n",
    "        self.branches = branches\n",
    "        self.K = len(branches)\n",
    "        self.num_classes = int(num_classes)\n",
    "\n",
    "        self.min_steps = int(min_steps)\n",
    "        self.max_steps = int(max_steps)\n",
    "        self.pool_every_n = int(pool_every_n)\n",
    "\n",
    "        self._single_pool      = pooling_layer\n",
    "        self._pooling_layersIn = pooling_layers\n",
    "        self.unique_pools      = bool(unique_pools)\n",
    "        self._pools = None\n",
    "\n",
    "        self.stem = ConvStem(out_ch=None)\n",
    "\n",
    "        self.router = AttnPoolRouter(K=self.K, heads=router_heads,\n",
    "                                     dim_head=router_dim, mlp_hidden=router_mlp)\n",
    "        self.halt   = HaltingClassifierHead(num_classes=self.num_classes,\n",
    "                                            hidden=32, halt_temp=halt_temp,\n",
    "                                            tau=halt_tau, bias_init=-1.0)\n",
    "\n",
    "        self._route_temp = float(route_temp)\n",
    "        self.lb_alpha  = float(lb_alpha)\n",
    "        self.div_alpha = float(div_alpha)\n",
    "\n",
    "        self.top_k = 1\n",
    "\n",
    "    @property\n",
    "    def route_temp(self): return self._route_temp\n",
    "    @route_temp.setter\n",
    "    def route_temp(self, v): self._route_temp = float(v)\n",
    "    @property\n",
    "    def halt_temp(self): return self.halt.halt_temp\n",
    "    @halt_temp.setter\n",
    "    def halt_temp(self, v): self.halt.halt_temp = float(v)\n",
    "\n",
    "    def _pooling_events(self):\n",
    "        return 0 if self.pool_every_n <= 0 else self.max_steps // self.pool_every_n\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        C_in = int(input_shape[-1])\n",
    "        self.stem.conv = layers.Conv2D(C_in, 3, padding=\"same\", use_bias=False)\n",
    "\n",
    "        num_pools = self._pooling_events()\n",
    "        if num_pools == 0:\n",
    "            self._pools = []\n",
    "        else:\n",
    "            if self._pooling_layersIn is not None:\n",
    "                assert len(self._pooling_layersIn) == num_pools\n",
    "                self._pools = self._pooling_layersIn\n",
    "            elif self.unique_pools:\n",
    "                assert self._single_pool is not None, \"unique_pools=True requires pooling_layer\"\n",
    "                base, cfg, cls = self._single_pool, self._single_pool.get_config(), type(self._single_pool)\n",
    "                self._pools = [cls(**cfg) for _ in range(num_pools)]\n",
    "            else:\n",
    "                assert self._single_pool is not None, \"Provide pooling_layer or pooling_layers\"\n",
    "                self._pools = [self._single_pool] * num_pools\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def _apply_pool(self, x, pool_count, training=None):\n",
    "        if self._pools and len(self._pools) > 0:\n",
    "            n = len(self._pools)\n",
    "            idx = tf.minimum(tf.convert_to_tensor(pool_count, tf.int32),\n",
    "                             tf.constant(n - 1, tf.int32))\n",
    "            fns = [(lambda i=i: self._pools[i](x, training=training)) for i in range(n)]\n",
    "            return tf.switch_case(idx, branch_fns=fns, default=lambda: self._pools[-1](x, training=training))\n",
    "        if self._single_pool is not None:\n",
    "            return self._single_pool(x, training=training)\n",
    "        return x\n",
    "\n",
    "    def _diversity_from_means(self, V):\n",
    "        V = tf.nn.l2_normalize(V, axis=-1)\n",
    "        sims = tf.matmul(V, V, transpose_b=True)\n",
    "        E = tf.shape(V)[0]\n",
    "        mask = 1.0 - tf.eye(E, dtype=V.dtype)\n",
    "        denom = tf.reduce_sum(mask)\n",
    "        return tf.where(denom > 0, tf.reduce_sum(sims * mask) / denom, 0.0)\n",
    "\n",
    "    def _call_branch(self, k_int32, x_k, training=None):\n",
    "        # Graph-safe expert selection\n",
    "        fns = [(lambda i=i: self.branches[i](x_k, training=training)) for i in range(self.K)]\n",
    "        return tf.switch_case(k_int32, branch_fns=fns, default=lambda: self.branches[-1](x_k, training=training))\n",
    "\n",
    "    @tf.function\n",
    "    def call_core(self, features, training=None):\n",
    "        x = self.stem(features, training=training)\n",
    "        B = tf.shape(x)[0]\n",
    "        dtype = x.dtype\n",
    "        halted = tf.zeros([B,1,1,1], dtype=dtype)\n",
    "        pool_count = tf.constant(0, tf.int32)\n",
    "        lb_loss_total = tf.constant(0.0, tf.float32)\n",
    "        div_loss_total = tf.constant(0.0, tf.float32)\n",
    "        C = tf.shape(x)[-1]\n",
    "        for t in range(self.max_steps):\n",
    "            running_mask = tf.squeeze(1.0 - halted, [1,2,3]) > 0.5\n",
    "            run_idx = tf.cast(tf.reshape(tf.where(running_mask), [-1]), tf.int32)\n",
    "            Br = tf.shape(run_idx)[0]\n",
    "            def no_running_path():\n",
    "                return x, halted, tf.constant(0.0, tf.float32), tf.constant(0.0, tf.float32), pool_count\n",
    "            def route_running_path():\n",
    "                x_run = tf.gather(x, run_idx)\n",
    "                r_logits, _ = self.router(x_run, training=training)\n",
    "                probs = tf.nn.softmax(r_logits / self.route_temp, axis=-1)\n",
    "                idx1 = tf.argmax(probs, axis=-1, output_type=tf.int32)\n",
    "                y_contrib_run = tf.zeros_like(x_run)\n",
    "                g_all = tf.zeros([Br, C], dtype=x_run.dtype)\n",
    "                count_all = tf.zeros([Br, 1], dtype=x_run.dtype)\n",
    "                for u in tf.range(self.K):\n",
    "                    sel = tf.equal(idx1, tf.cast(u, idx1.dtype))\n",
    "                    ids = tf.reshape(tf.where(sel), [-1])\n",
    "                    Bk = tf.shape(ids)[0]\n",
    "                    def do_u():\n",
    "                        x_k = tf.gather(x_run, ids)\n",
    "                        y_k = self._call_branch(tf.cast(u, tf.int32), x_k, training=training)\n",
    "                        prob_rows = tf.gather(probs, ids)\n",
    "                        w_for_k = tf.reshape(tf.gather(prob_rows, u, axis=-1), [-1,1,1,1])\n",
    "                        y_scaled = y_k * w_for_k\n",
    "                        scatter_idx = tf.expand_dims(ids, axis=-1)\n",
    "                        y_contrib_updated = tf.tensor_scatter_nd_add(y_contrib_run, scatter_idx, y_scaled)\n",
    "                        g_k = tf.reduce_mean(y_k, axis=[1,2])\n",
    "                        g_all_updated = tf.tensor_scatter_nd_add(g_all, scatter_idx, g_k)\n",
    "                        ones = tf.ones([Bk,1], dtype=x_run.dtype)\n",
    "                        count_all_updated = tf.tensor_scatter_nd_add(count_all, scatter_idx, ones)\n",
    "                        return y_contrib_updated, g_all_updated, count_all_updated\n",
    "                    def skip_u():\n",
    "                        return y_contrib_run, g_all, count_all\n",
    "                    y_contrib_run, g_all, count_all = tf.cond(Bk > 0, do_u, skip_u)\n",
    "                feat_sums = tf.math.unsorted_segment_sum(g_all, idx1, num_segments=self.K)\n",
    "                counts = tf.math.unsorted_segment_sum(count_all, idx1, num_segments=self.K)\n",
    "                y_sel_run = tf.where(tf.broadcast_to(tf.reduce_any(tf.not_equal(y_contrib_run, 0.0), axis=[1,2,3])[:,None,None,None], tf.shape(y_contrib_run)), y_contrib_run, x_run)\n",
    "                class_probs_run, p_soft_run, p_hard_run, p_st_run = self.halt(y_sel_run, training=training)\n",
    "                if self.min_steps > 1:\n",
    "                    can = tf.cast(t >= self.min_steps - 1, y_sel_run.dtype)\n",
    "                    p_st_run = p_st_run * can\n",
    "                is_pool_step = tf.logical_and(tf.greater(self.pool_every_n, 0), tf.equal(tf.math.mod(t+1, self.pool_every_n), 0))\n",
    "                def do_pool():\n",
    "                    x_pooled_all = self._apply_pool(x, pool_count, training=training)\n",
    "                    y_next_run = self._apply_pool(y_sel_run, pool_count, training=training)\n",
    "                    return x_pooled_all, y_next_run, pool_count + 1\n",
    "                def no_pool():\n",
    "                    return x, y_sel_run, pool_count\n",
    "                base_x_next, y_next_run, new_pool_count = tf.cond(is_pool_step, do_pool, no_pool)\n",
    "                run_idx_i32 = tf.cast(run_idx, tf.int32)\n",
    "                x_next = tf.tensor_scatter_nd_update(base_x_next, tf.expand_dims(run_idx_i32, axis=-1), y_next_run)\n",
    "                p_st4_run = tf.reshape(p_st_run, [-1,1,1,1])\n",
    "                old_h_run = tf.gather(halted, run_idx_i32)\n",
    "                new_h_run = tf.clip_by_value(old_h_run + (1.0 - old_h_run) * p_st4_run, 0.0, 1.0)\n",
    "                halted_next = tf.tensor_scatter_nd_update(halted, tf.expand_dims(run_idx_i32, -1), new_h_run)\n",
    "                lb_add = tf.constant(0.0, tf.float32)\n",
    "                if training and self.lb_alpha > 0.0:\n",
    "                    p_mean = tf.reduce_mean(probs, axis=0)\n",
    "                    uniform = tf.fill([self.K], tf.constant(1.0/self.K, dtype=p_mean.dtype))\n",
    "                    kl = tf.reduce_sum(p_mean * (tf.math.log(p_mean + tf.constant(1e-8, dtype=p_mean.dtype)) - tf.math.log(uniform + tf.constant(1e-8, dtype=p_mean.dtype))))\n",
    "                    lb_add = tf.cast(kl, tf.float32)\n",
    "                div_add = tf.constant(0.0, tf.float32)\n",
    "                if training and self.div_alpha > 0.0:\n",
    "                    counts_safe = tf.maximum(counts, tf.constant(1e-6, dtype=counts.dtype))\n",
    "                    V = feat_sums / counts_safe\n",
    "                    valid = tf.squeeze(counts > 0.5, axis=-1)\n",
    "                    V_sel = tf.boolean_mask(V, valid)\n",
    "                    def div_from_V(Vs):\n",
    "                        Vs = tf.nn.l2_normalize(Vs, axis=-1)\n",
    "                        sims = tf.matmul(Vs, Vs, transpose_b=True)\n",
    "                        E2 = tf.shape(Vs)[0]\n",
    "                        mask = 1.0 - tf.eye(E2, dtype=Vs.dtype)\n",
    "                        denom = tf.reduce_sum(mask)\n",
    "                        return tf.where(denom > 0, tf.reduce_sum(sims * mask) / denom, 0.0)\n",
    "                    div_add = tf.cond(tf.shape(V_sel)[0] > 1, lambda: tf.cast(div_from_V(V_sel), tf.float32), lambda: tf.constant(0.0, tf.float32))\n",
    "                return x_next, halted_next, lb_add, div_add, new_pool_count\n",
    "            x, halted, lb_add, div_add, pool_count = tf.cond(Br > 0, route_running_path, no_running_path)\n",
    "            if training:\n",
    "                if self.lb_alpha > 0.0:  lb_loss_total  += lb_add\n",
    "                if self.div_alpha > 0.0: div_loss_total += div_add\n",
    "            if not training and tf.executing_eagerly():\n",
    "                if bool(tf.reduce_all(tf.squeeze(halted, [1,2,3]) > 0.5).numpy()):\n",
    "                    break\n",
    "        steps = tf.cast(self.max_steps, tf.float32)\n",
    "        return x, lb_loss_total, div_loss_total, steps\n",
    "\n",
    "    def call(self, features, training=None):\n",
    "        x, lb_loss_total, div_loss_total, steps = self.call_core(features, training=training)\n",
    "        if training:\n",
    "            if self.lb_alpha > 0.0:\n",
    "                self.add_loss(self.lb_alpha * lb_loss_total / steps)\n",
    "            if self.div_alpha > 0.0:\n",
    "                self.add_loss(self.div_alpha * div_loss_total / steps)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Linearly (or cosine) anneal route_temp and halt_temp over epochs.\n",
    "    route: 1.5 -> 0.7\n",
    "    halt:  3.0 -> 1.5\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_name=\"adaptive_router\",\n",
    "                 route_start=1.5, route_end=0.7,\n",
    "                 halt_start=3.0,  halt_end=1.5,\n",
    "                 epochs=150, mode=\"cosine\"):\n",
    "        super().__init__()\n",
    "        self.layer_name = layer_name\n",
    "        self.rs, self.re = float(route_start), float(route_end)\n",
    "        self.hs, self.he = float(halt_start),  float(halt_end)\n",
    "        self.E = int(epochs)\n",
    "        self.mode = mode\n",
    "\n",
    "    def _interp(self, e):\n",
    "        p = min(1.0, e / max(1, self.E-1))\n",
    "        if self.mode == \"cosine\":\n",
    "            p = 0.5*(1 - np.cos(np.pi*p))\n",
    "        return p\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        p = self._interp(epoch)\n",
    "        rtemp = self.rs + (self.re - self.rs)*p\n",
    "        htemp = self.hs + (self.he - self.hs)*p\n",
    "        layer = self.model.get_layer(self.layer_name)\n",
    "        layer.route_temp = rtemp\n",
    "        layer.halt_temp  = htemp\n",
    "        print(f\" [TempScheduler] epoch {epoch+1}: route_temp={rtemp:.3f}, halt_temp={htemp:.3f}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def trace_batch(model, x_batch, layer_name=\"adaptive_router\", force_full=False):\n",
    "    \"\"\"\n",
    "    Trace routing/halting for a batch for AdaptiveRouterBlockTop2Sparse (top-1 only).\n",
    "\n",
    "    Returns dict (arrays shaped [t_used, B, ...] unless noted):\n",
    "      - top_indices:      [t_used, B]       Top-1 expert (argmax over K)\n",
    "      - probs:            [t_used, B, K]    soft routing probs over experts\n",
    "      - class_maxprob:    [t_used, B]       halting classifier max class prob\n",
    "      - class_pred:       [t_used, B]       halting classifier argmax class\n",
    "      - halt_soft:        [t_used, B]       soft halting gate (sigmoid around Ï„)\n",
    "      - halt_hard:        [t_used, B]       hard halting gate (0/1)\n",
    "      - running_after:    [t_used, B] bool  True if still running after step t\n",
    "    \"\"\"\n",
    "    layer = model.get_layer(layer_name)\n",
    "\n",
    "    # features entering the router block (before stem)\n",
    "    pre = keras.Model(model.input, layer.input)\n",
    "    x_in = tf.convert_to_tensor(x_batch)\n",
    "    x = pre(x_in, training=False)\n",
    "\n",
    "    B = int(x.shape[0])\n",
    "    K, T = layer.K, layer.max_steps\n",
    "\n",
    "    # apply the same conv stem as the block\n",
    "    x = layer.stem(x, training=False)\n",
    "\n",
    "    have_pool_list = hasattr(layer, \"_pools\") and isinstance(layer._pools, (list, tuple)) and len(layer._pools) > 0\n",
    "    pool_count = 0\n",
    "    running = np.ones((B,), dtype=bool)\n",
    "\n",
    "    top1_list, probs_list = [], []\n",
    "    class_max_list, class_pred_list = [], []\n",
    "    halt_soft_list, halt_hard_list, running_after_list = [], [], []\n",
    "\n",
    "    for t in range(T):\n",
    "        r_logits, _ = layer.router(x, training=False)                         # [B,K]\n",
    "        probs = tf.nn.softmax(r_logits / layer.route_temp, axis=-1)           # [B,K]\n",
    "        probs_np = probs.numpy()\n",
    "        probs_list.append(probs_np)\n",
    "\n",
    "        top1 = tf.argmax(probs, axis=-1, output_type=tf.int32).numpy()        # [B]\n",
    "        top1_list.append(top1)\n",
    "\n",
    "        # Only top-1 expert is used in the block\n",
    "        y_contrib = None\n",
    "        for k in range(K):\n",
    "            sel = (top1 == k)\n",
    "            if not np.any(sel):\n",
    "                continue\n",
    "            x_k = tf.boolean_mask(x, sel)\n",
    "            y_k = layer.branches[k](x_k, training=False)\n",
    "            w_for_k = probs_np[sel, k].reshape(-1,1,1,1)\n",
    "            y_scaled = y_k * w_for_k\n",
    "            scatter_idx = np.where(sel)[0]\n",
    "            scatter_idx = tf.expand_dims(tf.convert_to_tensor(scatter_idx, dtype=tf.int32), axis=-1)\n",
    "            if y_contrib is None:\n",
    "                y_contrib = tf.zeros_like(x)\n",
    "            y_contrib = tf.tensor_scatter_nd_add(y_contrib, scatter_idx, y_scaled)\n",
    "        y_sel = x if y_contrib is None else y_contrib\n",
    "\n",
    "        class_probs, p_soft, p_hard, _ = layer.halt(y_sel, training=False)    # [B,C], [B,1], [B,1], [B,1]\n",
    "        class_probs_np = class_probs.numpy()\n",
    "        class_max = class_probs_np.max(axis=-1)                                # [B]\n",
    "        class_pred = class_probs_np.argmax(axis=-1)                            # [B]\n",
    "\n",
    "        if t < layer.min_steps - 1:\n",
    "            halt_this = np.zeros((B,), dtype=bool)\n",
    "            p_soft_np = np.squeeze(p_soft.numpy(), axis=1)\n",
    "            p_soft_np *= 0.0\n",
    "            p_hard_np = np.zeros((B,), dtype=np.float32)\n",
    "        else:\n",
    "            p_soft_np = np.squeeze(p_soft.numpy(), axis=1)                     # [B]\n",
    "            p_hard_np = np.squeeze(p_hard.numpy(), axis=1).astype(np.float32)  # [B]\n",
    "            halt_this = (p_hard_np > 0.5) & running\n",
    "\n",
    "        running = running & (~halt_this)\n",
    "\n",
    "        if layer.pool_every_n > 0 and ((t + 1) % layer.pool_every_n == 0):\n",
    "            if have_pool_list:\n",
    "                pool_layer = layer._pools[min(pool_count, len(layer._pools)-1)]\n",
    "                x = pool_layer(y_sel)\n",
    "                pool_count += 1\n",
    "            else:\n",
    "                x = layer.pooling_layer(y_sel)\n",
    "        else:\n",
    "            x = y_sel\n",
    "\n",
    "        class_max_list.append(class_max.copy())\n",
    "        class_pred_list.append(class_pred.copy())\n",
    "        halt_soft_list.append(p_soft_np.copy())\n",
    "        halt_hard_list.append(p_hard_np.copy())\n",
    "        running_after_list.append(running.copy())\n",
    "\n",
    "        if (not force_full) and (not running.any()):\n",
    "            break\n",
    "\n",
    "    trace = {\n",
    "        \"top_indices\":   np.asarray(top1_list),             # [t_used, B]\n",
    "        \"probs\":         np.asarray(probs_list),            # [t_used, B, K]\n",
    "        \"class_maxprob\": np.asarray(class_max_list),        # [t_used, B]\n",
    "        \"class_pred\":    np.asarray(class_pred_list),       # [t_used, B]\n",
    "        \"halt_soft\":     np.asarray(halt_soft_list),        # [t_used, B]\n",
    "        \"halt_hard\":     np.asarray(halt_hard_list),        # [t_used, B]\n",
    "        \"running_after\": np.asarray(running_after_list, dtype=bool),  # [t_used, B]\n",
    "    }\n",
    "    return trace\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_model(input_shape=(32,32,3), num_classes=10,\n",
    "                filters=32,\n",
    "                min_steps=2, max_steps=6, pool_every_n=3,\n",
    "                unique_pools=True):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(filters, 3, padding='same', activation='swish')(inputs)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = PoolingLayer(filters=filters, frac_ratio=2.0)(x)\n",
    "    x = ResidualBlock3x3(filters)(x)\n",
    "    x = PoolingLayer(filters=filters, frac_ratio=2.0)(x)\n",
    "\n",
    "    # Heterogeneous experts\n",
    "    branches = [\n",
    "        ResidualBlock3x3(filters),\n",
    "        ResidualBlockDepthwise5x5(filters),\n",
    "        #ResidualBlockDepthwise7x7(filters),\n",
    "        #ChannelSE(filters)\n",
    "    ]\n",
    "\n",
    "    # Pooling template (or supply a list for explicit per-event pools)\n",
    "    pooling_template = PoolingLayer(filters=filters, frac_ratio=2.0)\n",
    "\n",
    "    x = AdaptiveRouterBlockTop1Sparse(\n",
    "        branches=branches,\n",
    "        num_classes=num_classes,\n",
    "        pooling_layer=pooling_template,\n",
    "        pooling_layers=None,           # or a list with len = max_steps//pool_every_n\n",
    "        unique_pools=unique_pools,\n",
    "        min_steps=min_steps,\n",
    "        max_steps=max_steps,\n",
    "        pool_every_n=pool_every_n,\n",
    "        router_heads=1, router_dim=32, router_mlp=32,\n",
    "        route_temp=1.5, halt_temp=1.5, halt_tau=0.7,\n",
    "        lb_alpha=1e-3, div_alpha=1e-4,\n",
    "        name=\"adaptive_router\"\n",
    "    )(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(num_classes, dtype=\"float32\", activation='softmax')(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " layer_normalization_12 (La  (None, 32, 32, 32)        64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " pooling_layer_18 (PoolingL  (None, 16, 16, 32)        1120      \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " residual_block3x3_12 (Resi  (None, 16, 16, 32)        20736     \n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " layer_normalization_12 (La  (None, 32, 32, 32)        64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " pooling_layer_18 (PoolingL  (None, 16, 16, 32)        1120      \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " residual_block3x3_12 (Resi  (None, 16, 16, 32)        20736     \n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " pooling_layer_19 (PoolingL  (None, 8, 8, 32)          1120      \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " adaptive_router (AdaptiveR  (None, None, None, 32)    40749     \n",
      " outerBlockTop1Sparse)                                           \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 32)                0         \n",
      " 0 (GlobalAveragePooling2D)                                      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65015 (253.96 KB)\n",
      "Trainable params: 65015 (253.96 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      " dualBlock3x3)                                                   \n",
      "                                                                 \n",
      " pooling_layer_19 (PoolingL  (None, 8, 8, 32)          1120      \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " adaptive_router (AdaptiveR  (None, None, None, 32)    40749     \n",
      " outerBlockTop1Sparse)                                           \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 32)                0         \n",
      " 0 (GlobalAveragePooling2D)                                      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65015 (253.96 KB)\n",
      "Trainable params: 65015 (253.96 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# simple CIFAR-10 aug\n",
    "def cifar_preprocess(x, y):\n",
    "    x = tf.image.resize_with_crop_or_pad(x, 36, 36)\n",
    "    x = tf.image.random_crop(x, [tf.shape(x)[0], 32, 32, 3])\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    return x, y\n",
    "\n",
    "def make_dataset(x, y, batch=128, train=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    if train:\n",
    "        ds = ds.shuffle(5000).batch(batch).map(cifar_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        ds = ds.batch(batch)\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "class RouterStatsCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, x_val, y_val, layer_name=\"adaptive_router_top1\", batch_size=256):\n",
    "        super().__init__()\n",
    "        self.xv = x_val; self.yv = y_val\n",
    "        self.layer_name = layer_name\n",
    "        self.bs = batch_size\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # accuracy\n",
    "        loss, acc = self.model.evaluate(self.xv, self.yv, batch_size=self.bs, verbose=0)\n",
    "        # quick router stats (avg steps + histogram)\n",
    "        from math import ceil\n",
    "        layer = self.model.get_layer(self.layer_name)\n",
    "        T, K = layer.max_steps, layer.K\n",
    "        steps_hist = np.zeros(T+1, np.int64)\n",
    "        expert_hist = np.zeros((T, K), np.int64)\n",
    "        n = len(self.xv)\n",
    "        for i in range(0, n, self.bs):\n",
    "            xb = self.xv[i:i+self.bs]\n",
    "            tb = trace_batch(self.model, xb, layer_name=self.layer_name, force_full=False)\n",
    "            t_used = tb[\"top_indices\"].shape[0]\n",
    "            running = tb[\"running_after\"]  # [t_used, B]\n",
    "            # per-sample steps\n",
    "            stopped = ~running\n",
    "            ever = stopped.any(axis=0)\n",
    "            first = np.argmax(stopped, axis=0)\n",
    "            used = np.where(ever, first+1, t_used)\n",
    "            for s in used:\n",
    "                steps_hist[min(int(s), T)] += 1\n",
    "            # experts\n",
    "            for t in range(t_used):\n",
    "                ch = tb[\"top_indices\"][t]\n",
    "                cnt = np.bincount(ch, minlength=K)\n",
    "                expert_hist[t] += cnt\n",
    "        avg_steps = np.sum(np.arange(T+1)*steps_hist)/max(1, steps_hist.sum())\n",
    "        print(f\"[Stats] epoch {epoch+1}: val_acc={acc*100:.2f}%  avg_steps={avg_steps:.2f}  steps_hist={steps_hist.tolist()} expert_hist={expert_hist.tolist()}\")\n",
    "        # you can also print expert_hist[0] etc.\n",
    "\n",
    "# Usage\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype(\"float32\")/255.0; x_test = x_test.astype(\"float32\")/255.0\n",
    "y_train = y_train.flatten(); y_test = y_test.flatten()\n",
    "\n",
    "model = build_model()\n",
    "opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(optimizer=opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "lr_sched = keras.optimizers.schedules.CosineDecay(initial_learning_rate=1e-3, decay_steps=150*len(x_train)//128)\n",
    "model.optimizer.learning_rate = lr_sched\n",
    "\n",
    "callbacks = [\n",
    "    TempScheduler(layer_name=\"adaptive_router\", epochs=50, mode=\"cosine\"),\n",
    "    RouterStatsCallback(x_test, y_test, layer_name=\"adaptive_router\")\n",
    "]\n",
    "ds_train = make_dataset(x_train, y_train, batch=128, train=True)\n",
    "ds_val   = make_dataset(x_test, y_test, batch=256, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [TempScheduler] epoch 1: route_temp=1.500, halt_temp=3.000\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['halt_bias:0', 'dense_29/kernel:0', 'dense_29/bias:0', 'dense_30/kernel:0', 'dense_30/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['halt_bias:0', 'dense_29/kernel:0', 'dense_29/bias:0', 'dense_30/kernel:0', 'dense_30/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['halt_bias:0', 'dense_29/kernel:0', 'dense_29/bias:0', 'dense_30/kernel:0', 'dense_30/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['halt_bias:0', 'dense_29/kernel:0', 'dense_29/bias:0', 'dense_30/kernel:0', 'dense_30/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['halt_bias:0', 'dense_29/kernel:0', 'dense_29/bias:0', 'dense_30/kernel:0', 'dense_30/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "2025-10-14 17:08:00.331888: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-10-14 17:08:00.331888: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-10-14 17:08:04.361553: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:933] Skipping loop optimization for Merge node with control input: model_3/adaptive_router/StatefulPartitionedCall/cond_5_163/then/_3606/cond_5/cond/branch_executed/_14093\n",
      "2025-10-14 17:08:04.361553: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:933] Skipping loop optimization for Merge node with control input: model_3/adaptive_router/StatefulPartitionedCall/cond_5_163/then/_3606/cond_5/cond/branch_executed/_14093\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.fit(ds_train, epochs=50, validation_data=ds_val, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base model with 4 blocks or 6 blocks of 32 filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_model_4_blocks(input_shape=(32,32,3), num_classes=10, filters=32):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # For CNNs on GPU, BatchNorm is faster than LayerNorm:\n",
    "    x = layers.Conv2D(filters, 3, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = ResidualBlock(filters)(x)\n",
    "    x = PoolingLayer(filters=filters, frac_ratio=2.0)(x)\n",
    "    x = ResidualBlock(filters)(x)\n",
    "    x = ResidualBlock(filters)(x)\n",
    "    x = PoolingLayer(filters=filters, frac_ratio=2.0)(x)\n",
    "    x = ResidualBlock(filters)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # keep fp32 logits\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "def build_base_model_6_blocks(input_shape=(32,32,3), num_classes=10, filters=32):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # For CNNs on GPU, BatchNorm is faster than LayerNorm:\n",
    "    x = layers.Conv2D(filters, 3, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = ResidualBlock(filters)(x)\n",
    "    x = PoolingLayer(filters=filters, frac_ratio=2.0)(x)\n",
    "    x = ResidualBlock(filters)(x)\n",
    "    x = ResidualBlock(filters)(x)\n",
    "    x = PoolingLayer(filters=filters, frac_ratio=2.0)(x)\n",
    "    x = ResidualBlock(filters)(x)\n",
    "    x = ResidualBlock(filters)(x)\n",
    "    x = PoolingLayer(filters=filters, frac_ratio=2.0)(x)\n",
    "    x = ResidualBlock(filters)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # keep fp32 logits\n",
    "    return keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_61\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 32, 32, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " residual_block_4 (Residual  (None, 32, 32, 32)        20736     \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " pooling_layer_1 (PoolingLa  (None, 16, 16, 32)        1120      \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " residual_block_5 (Residual  (None, 16, 16, 32)        20736     \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " residual_block_6 (Residual  (None, 16, 16, 32)        20736     \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " pooling_layer_2 (PoolingLa  (None, 8, 8, 32)          1120      \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " residual_block_7 (Residual  (None, 8, 8, 32)          20736     \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 32)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86538 (338.04 KB)\n",
      "Trainable params: 86474 (337.79 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 16:17:22.540030: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 1.8282 - accuracy: 0.3055"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 16:17:42.444025: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 23s 189ms/step - loss: 1.8282 - accuracy: 0.3055 - val_loss: 2.3907 - val_accuracy: 0.1831\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - 15s 151ms/step - loss: 1.4866 - accuracy: 0.4441 - val_loss: 1.9441 - val_accuracy: 0.3035\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - 15s 148ms/step - loss: 1.2755 - accuracy: 0.5356 - val_loss: 1.9825 - val_accuracy: 0.3222\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - 14s 143ms/step - loss: 1.1392 - accuracy: 0.5867 - val_loss: 1.7945 - val_accuracy: 0.3760\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 1.0311 - accuracy: 0.6264 - val_loss: 1.2198 - val_accuracy: 0.5541\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.9601 - accuracy: 0.6540 - val_loss: 1.0745 - val_accuracy: 0.6036\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - 15s 148ms/step - loss: 0.9133 - accuracy: 0.6695 - val_loss: 1.0224 - val_accuracy: 0.6342\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.8544 - accuracy: 0.6936 - val_loss: 1.0272 - val_accuracy: 0.6400\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - 14s 146ms/step - loss: 0.8335 - accuracy: 0.6998 - val_loss: 0.9523 - val_accuracy: 0.6620\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - 14s 143ms/step - loss: 0.7940 - accuracy: 0.7143 - val_loss: 0.9144 - val_accuracy: 0.6762\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - 14s 145ms/step - loss: 0.7706 - accuracy: 0.7247 - val_loss: 0.8708 - val_accuracy: 0.6916\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.7407 - accuracy: 0.7330 - val_loss: 0.9664 - val_accuracy: 0.6532\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.7240 - accuracy: 0.7395 - val_loss: 0.9708 - val_accuracy: 0.6591\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - 14s 148ms/step - loss: 0.6902 - accuracy: 0.7525 - val_loss: 0.8552 - val_accuracy: 0.6935\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - 15s 149ms/step - loss: 0.6881 - accuracy: 0.7530 - val_loss: 0.8673 - val_accuracy: 0.6926\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.6585 - accuracy: 0.7642 - val_loss: 0.8753 - val_accuracy: 0.6875\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.6431 - accuracy: 0.7707 - val_loss: 0.7975 - val_accuracy: 0.7194\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.6122 - accuracy: 0.7807 - val_loss: 0.8450 - val_accuracy: 0.7071\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.5958 - accuracy: 0.7866 - val_loss: 0.8561 - val_accuracy: 0.7045\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - 14s 144ms/step - loss: 0.5752 - accuracy: 0.7955 - val_loss: 0.8019 - val_accuracy: 0.7204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x158b70f40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_base_model_4_blocks(\n",
    "    input_shape=(32,32,3),\n",
    "    num_classes=10,\n",
    "    filters=FILTERS\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=20, batch_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_62\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 32, 32, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " residual_block_8 (Residual  (None, 32, 32, 32)        20736     \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " pooling_layer_3 (PoolingLa  (None, 16, 16, 32)        1120      \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " residual_block_9 (Residual  (None, 16, 16, 32)        20736     \n",
      " Block)                                                          \n",
      "                                                                 \n",
      " residual_block_10 (Residua  (None, 16, 16, 32)        20736     \n",
      " lBlock)                                                         \n",
      "                                                                 \n",
      " pooling_layer_4 (PoolingLa  (None, 8, 8, 32)          1120      \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " residual_block_11 (Residua  (None, 8, 8, 32)          20736     \n",
      " lBlock)                                                         \n",
      "                                                                 \n",
      " residual_block_12 (Residua  (None, 8, 8, 32)          20736     \n",
      " lBlock)                                                         \n",
      "                                                                 \n",
      " pooling_layer_5 (PoolingLa  (None, 4, 4, 32)          1120      \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " residual_block_13 (Residua  (None, 4, 4, 32)          20736     \n",
      " lBlock)                                                         \n",
      "                                                                 \n",
      " global_average_pooling2d_2  (None, 32)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 129130 (504.41 KB)\n",
      "Trainable params: 129066 (504.16 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 16:22:18.933112: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 1.7270 - accuracy: 0.3563"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 16:22:38.732527: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 23s 194ms/step - loss: 1.7270 - accuracy: 0.3563 - val_loss: 2.5958 - val_accuracy: 0.1778\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - 16s 158ms/step - loss: 1.3537 - accuracy: 0.5037 - val_loss: 1.6845 - val_accuracy: 0.3908\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - 16s 163ms/step - loss: 1.1777 - accuracy: 0.5730 - val_loss: 1.4184 - val_accuracy: 0.4790\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - 16s 163ms/step - loss: 1.0465 - accuracy: 0.6216 - val_loss: 1.1898 - val_accuracy: 0.5696\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - 16s 160ms/step - loss: 0.9614 - accuracy: 0.6537 - val_loss: 1.0755 - val_accuracy: 0.6178\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.8872 - accuracy: 0.6815 - val_loss: 1.0424 - val_accuracy: 0.6327\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - 15s 156ms/step - loss: 0.8318 - accuracy: 0.7016 - val_loss: 0.9382 - val_accuracy: 0.6660\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - 15s 158ms/step - loss: 0.7757 - accuracy: 0.7223 - val_loss: 0.9095 - val_accuracy: 0.6837\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - 15s 152ms/step - loss: 0.7305 - accuracy: 0.7394 - val_loss: 0.8830 - val_accuracy: 0.6898\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - 15s 152ms/step - loss: 0.6990 - accuracy: 0.7500 - val_loss: 0.8733 - val_accuracy: 0.6980\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - 15s 152ms/step - loss: 0.6628 - accuracy: 0.7628 - val_loss: 0.9028 - val_accuracy: 0.6890\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - 16s 161ms/step - loss: 0.6260 - accuracy: 0.7758 - val_loss: 0.8919 - val_accuracy: 0.6962\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - 17s 173ms/step - loss: 0.5841 - accuracy: 0.7946 - val_loss: 0.9160 - val_accuracy: 0.6944\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - 17s 168ms/step - loss: 0.5669 - accuracy: 0.7985 - val_loss: 0.9117 - val_accuracy: 0.6928\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - 15s 152ms/step - loss: 0.5437 - accuracy: 0.8070 - val_loss: 0.9037 - val_accuracy: 0.6988\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - 15s 155ms/step - loss: 0.5035 - accuracy: 0.8212 - val_loss: 0.8777 - val_accuracy: 0.7093\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - 15s 156ms/step - loss: 0.4814 - accuracy: 0.8289 - val_loss: 0.8785 - val_accuracy: 0.7127\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - 15s 152ms/step - loss: 0.4431 - accuracy: 0.8425 - val_loss: 0.9214 - val_accuracy: 0.7051\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - 15s 151ms/step - loss: 0.4327 - accuracy: 0.8460 - val_loss: 0.9194 - val_accuracy: 0.7124\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - 15s 153ms/step - loss: 0.3976 - accuracy: 0.8596 - val_loss: 0.9220 - val_accuracy: 0.7135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3b0a50370>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_base_model_6_blocks(\n",
    "    input_shape=(32,32,3),\n",
    "    num_classes=10,\n",
    "    filters=FILTERS\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=20, batch_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

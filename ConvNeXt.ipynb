{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af992c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterniederl/opt/miniconda3/envs/tf-mlp/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2025-10-22 09:02:42.374587: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-10-22 09:02:42.374622: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-10-22 09:02:42.374629: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-10-22 09:02:42.374892: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-10-22 09:02:42.374909: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# ConvNeXt Small (CIFAR-10) — full model + optimizer\n",
    "import os, math, tensorflow as tf, tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers as L, models as M\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Optional determinism / seeds\n",
    "# ----------------------------\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "\n",
    "# Data pipeline (CIFAR-10) — replace with your own input pipeline if needed\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "num_classes = 10\n",
    "y_train = tf.squeeze(tf.one_hot(y_train, num_classes), axis=1)\n",
    "y_test  = tf.squeeze(tf.one_hot(y_test,  num_classes), axis=1)\n",
    "\n",
    "def preprocess(images, labels):\n",
    "    images = tf.cast(images, tf.float32) / 255.0\n",
    "    return images, labels\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "ds_test = (tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "            .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "def augment(image, label):\n",
    "    # pad by 4 and random crop back to 32x32\n",
    "    image = tf.image.resize_with_crop_or_pad(image, 36, 36)\n",
    "    image = tf.image.random_crop(image, size=(32, 32, 3), seed=SEED)\n",
    "    image = tf.image.random_flip_left_right(image, seed=SEED)\n",
    "    return image, label\n",
    "\n",
    "ds_train = (tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            .shuffle(50000, seed=SEED)\n",
    "            .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .map(augment, num_parallel_calls=tf.data.AUTOTUNE)  # per-example\n",
    "            .batch(batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "\n",
    "\n",
    "steps_per_epoch = math.ceil(len(x_train) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cca8799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "class DropPath(L.Layer):\n",
    "    \"\"\"Stochastic Depth per sample.\"\"\"\n",
    "    def __init__(self, drop_prob=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.drop_prob = float(drop_prob)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if (not training) or self.drop_prob == 0.0:\n",
    "            return x\n",
    "        keep = 1.0 - self.drop_prob\n",
    "        # shape: [B, 1, 1, 1] to broadcast over HWC\n",
    "        shape = (tf.shape(x)[0],) + (1, 1, 1)\n",
    "        rnd = tf.random.uniform(shape, dtype=x.dtype)\n",
    "        mask = tf.cast(rnd < keep, x.dtype)\n",
    "        return x * mask / keep\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\"drop_prob\": self.drop_prob})\n",
    "        return cfg\n",
    "\n",
    "\n",
    "class ConvNeXtBlock(L.Layer):\n",
    "    \"\"\"ConvNeXt block (channels-last). DWConv-7x7 → LN → 1x1 MLP (4x expand) → residual (+ DropPath).\"\"\"\n",
    "    def __init__(self, dim, drop_path=0.0, mlp_ratio=4, se_ratio=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.drop_path_rate = drop_path\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.se_ratio = se_ratio\n",
    "\n",
    "        self.dw = L.DepthwiseConv2D(\n",
    "            kernel_size=7, padding=\"same\", use_bias=True, name=\"dw7\"\n",
    "        )\n",
    "        self.ln = L.LayerNormalization(epsilon=1e-6, name=\"ln\")  # channels-last\n",
    "        self.pw1 = L.Dense(int(mlp_ratio * dim), name=\"pw1\")    # 1x1 via Dense on last dim\n",
    "        self.act = L.Activation(\"gelu\")\n",
    "        self.pw2 = L.Dense(dim, name=\"pw2\")\n",
    "        self.drop_path = DropPath(drop_prob=drop_path)\n",
    "\n",
    "        if se_ratio and se_ratio > 0.0:\n",
    "            mid = max(1, int(dim * se_ratio))\n",
    "            self.se = M.Sequential([\n",
    "                L.GlobalAveragePooling2D(keepdims=True),\n",
    "                L.Conv2D(mid, 1, activation=\"gelu\", use_bias=True),\n",
    "                L.Conv2D(dim, 1, activation=\"sigmoid\", use_bias=True)\n",
    "            ], name=\"se\")\n",
    "        else:\n",
    "            self.se = None\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        shortcut = x\n",
    "        y = self.dw(x)\n",
    "        # NHWC → keep NHWC, LayerNorm with axis=-1 works channels-last\n",
    "        y = self.ln(y)\n",
    "        y = self.pw2(self.act(self.pw1(y)))\n",
    "        if self.se is not None:\n",
    "            y = y * self.se(y)  # lightweight SE (optional)\n",
    "        y = self.drop_path(y, training=training)\n",
    "        return shortcut + y\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update(dict(dim=self.dim, drop_path=self.drop_path_rate,\n",
    "                        mlp_ratio=self.mlp_ratio, se_ratio=self.se_ratio))\n",
    "        return cfg\n",
    "\n",
    "\n",
    "def Downsample(in_ch, out_ch, name):\n",
    "    # Conv 2x2 s=2 to shrink spatial size\n",
    "    return M.Sequential([\n",
    "        L.Conv2D(out_ch, kernel_size=2, strides=2, padding=\"valid\", use_bias=True)\n",
    "    ], name=name)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Model builder\n",
    "# ----------------------------\n",
    "def build_convnext_small_cifar10(\n",
    "    num_classes=10,\n",
    "    depths=(3, 6, 6),\n",
    "    dims=(96, 192, 384),\n",
    "    drop_path_rate=0.1,\n",
    "    se_ratio=0.25  # set 0.0 to disable SE\n",
    "):\n",
    "    \"\"\"\n",
    "    ConvNeXt Small tailored for CIFAR-10.\n",
    "    depths: blocks per stage\n",
    "    dims:   channels per stage\n",
    "    drop_path_rate: max stochastic depth across blocks (linearly scaled)\n",
    "    \"\"\"\n",
    "    assert len(depths) == 3 and len(dims) == 3\n",
    "\n",
    "    inputs = L.Input(shape=(32, 32, 3))\n",
    "\n",
    "    # Stem: 4x4 conv stride 4 → 8x8 tokens, C = dims[0]\n",
    "    x = L.Conv2D(dims[0], kernel_size=4, strides=4, padding=\"valid\", use_bias=True, name=\"stem\")(inputs)\n",
    "    x = L.LayerNormalization(epsilon=1e-6, name=\"stem_ln\")(x)\n",
    "\n",
    "    # Compute per-block drop_path schedule (linear from 0 → drop_path_rate)\n",
    "    total_blocks = sum(depths)\n",
    "    dpr = [i * drop_path_rate / max(1, total_blocks - 1) for i in range(total_blocks)]\n",
    "\n",
    "    idx = 0\n",
    "    # Stage 1 (8x8)\n",
    "    for b in range(depths[0]):\n",
    "        x = ConvNeXtBlock(dims[0], drop_path=dpr[idx], se_ratio=se_ratio, name=f\"stage1_block{b}\")(x)\n",
    "        idx += 1\n",
    "\n",
    "    # Downsample to 4x4\n",
    "    x = Downsample(dims[0], dims[1], name=\"down1\")(x)\n",
    "    x = L.LayerNormalization(epsilon=1e-6, name=\"down1_ln\")(x)\n",
    "\n",
    "    # Stage 2 (4x4)\n",
    "    for b in range(depths[1]):\n",
    "        x = ConvNeXtBlock(dims[1], drop_path=dpr[idx], se_ratio=se_ratio, name=f\"stage2_block{b}\")(x)\n",
    "        idx += 1\n",
    "\n",
    "    # Downsample to 2x2\n",
    "    x = Downsample(dims[1], dims[2], name=\"down2\")(x)\n",
    "    x = L.LayerNormalization(epsilon=1e-6, name=\"down2_ln\")(x)\n",
    "\n",
    "    # Stage 3 (2x2)\n",
    "    for b in range(depths[2]):\n",
    "        x = ConvNeXtBlock(dims[2], drop_path=dpr[idx], se_ratio=se_ratio, name=f\"stage3_block{b}\")(x)\n",
    "        idx += 1\n",
    "\n",
    "    # Head\n",
    "    x = L.LayerNormalization(epsilon=1e-6, name=\"head_ln\")(x)\n",
    "    x = L.GlobalAveragePooling2D(name=\"gap\")(x)\n",
    "    outputs = L.Dense(num_classes, name=\"logits\")(x)\n",
    "\n",
    "    return M.Model(inputs, outputs, name=\"ConvNeXtSmall_CIFAR10\")\n",
    "\n",
    "\n",
    "\n",
    "import math, tensorflow as tf\n",
    "\n",
    "# --- Serializable warmup+cosine schedule ---\n",
    "class WarmupCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, base_lr, warmup_steps, total_steps, name=None):\n",
    "        super().__init__()\n",
    "        self._base_lr = float(base_lr)\n",
    "        self._warmup_steps = int(warmup_steps)\n",
    "        self._total_steps = int(total_steps)\n",
    "        self._name = name or \"WarmupCosine\"\n",
    "    def __call__(self, step):\n",
    "        import math as _m\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        base = tf.cast(self._base_lr, tf.float32)\n",
    "        w = tf.cast(self._warmup_steps, tf.float32)\n",
    "        T = tf.cast(self._total_steps, tf.float32)\n",
    "        warm = (step / tf.maximum(1.0, w)) * base\n",
    "        prog = (step - w) / tf.maximum(1.0, T - w)\n",
    "        prog = tf.clip_by_value(prog, 0.0, 1.0)\n",
    "        cos = 0.5 * (1.0 + tf.cos(tf.constant(_m.pi, tf.float32) * prog))\n",
    "        decayed = base * cos\n",
    "        return tf.where(step < w, warm, decayed, name=self._name)\n",
    "    def get_config(self):\n",
    "        return {\"base_lr\": self._base_lr, \"warmup_steps\": self._warmup_steps,\n",
    "                \"total_steps\": self._total_steps, \"name\": self._name}\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg): return cls(**cfg)\n",
    "\n",
    "def make_optimizer(steps_per_epoch, epochs, base_lr=3e-3, weight_decay=0.05, warmup_epochs=10):\n",
    "    total_steps = int(steps_per_epoch * epochs)\n",
    "    warmup_steps = int(steps_per_epoch * warmup_epochs)\n",
    "    lr_sched = WarmupCosine(base_lr, warmup_steps, total_steps)\n",
    "    # Prefer native; on M1/M2 this will auto-fallback to legacy.AdamW (fast) if needed\n",
    "    try:\n",
    "        opt = tfa.optimizers.AdamW(\n",
    "            learning_rate=lr_sched, weight_decay=weight_decay,\n",
    "            beta_1=0.9, beta_2=0.999, epsilon=1e-8\n",
    "        )\n",
    "    except Exception:\n",
    "        opt = tf.keras.optimizers.legacy.AdamW(\n",
    "            learning_rate=lr_sched, weight_decay=weight_decay,\n",
    "            beta_1=0.9, beta_2=0.999, epsilon=1e-8\n",
    "        )\n",
    "    return opt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5438e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, tensorflow as tf\n",
    "\n",
    "# --- LR schedule (serializable) ---\n",
    "class WarmupCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, base_lr, warmup_steps, total_steps, name=None):\n",
    "        super().__init__()\n",
    "        self._base_lr = float(base_lr)\n",
    "        self._warmup_steps = int(warmup_steps)\n",
    "        self._total_steps = int(total_steps)\n",
    "        self._name = name or \"WarmupCosine\"\n",
    "    def __call__(self, step):\n",
    "        import math as _m\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        base = tf.cast(self._base_lr, tf.float32)\n",
    "        w = tf.cast(self._warmup_steps, tf.float32)\n",
    "        T = tf.cast(self._total_steps, tf.float32)\n",
    "        warm = (step / tf.maximum(1.0, w)) * base\n",
    "        prog = (step - w) / tf.maximum(1.0, T - w)\n",
    "        prog = tf.clip_by_value(prog, 0.0, 1.0)\n",
    "        cos = 0.5 * (1.0 + tf.cos(tf.constant(_m.pi, tf.float32) * prog))\n",
    "        decayed = base * cos\n",
    "        return tf.where(step < w, warm, decayed, name=self._name)\n",
    "    def get_config(self):\n",
    "        return {\"base_lr\": self._base_lr, \"warmup_steps\": self._warmup_steps,\n",
    "                \"total_steps\": self._total_steps, \"name\": self._name}\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg): return cls(**cfg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c845de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ConvNeXtSmall_CIFAR10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " stem (Conv2D)               (None, 8, 8, 96)          4704      \n",
      "                                                                 \n",
      " stem_ln (LayerNormalizatio  (None, 8, 8, 96)          192       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " stage1_block0 (ConvNeXtBlo  (None, 8, 8, 96)          83928     \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " stage1_block1 (ConvNeXtBlo  (None, 8, 8, 96)          83928     \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " stage1_block2 (ConvNeXtBlo  (None, 8, 8, 96)          83928     \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " down1 (Sequential)          (None, 4, 4, 192)         73920     \n",
      "                                                                 \n",
      " down1_ln (LayerNormalizati  (None, 4, 4, 192)         384       \n",
      " on)                                                             \n",
      "                                                                 \n",
      " stage2_block0 (ConvNeXtBlo  (None, 4, 4, 192)         324528    \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " stage2_block1 (ConvNeXtBlo  (None, 4, 4, 192)         324528    \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " stage2_block2 (ConvNeXtBlo  (None, 4, 4, 192)         324528    \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " stage2_block3 (ConvNeXtBlo  (None, 4, 4, 192)         324528    \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " stage2_block4 (ConvNeXtBlo  (None, 4, 4, 192)         324528    \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " stage2_block5 (ConvNeXtBlo  (None, 4, 4, 192)         324528    \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " down2 (Sequential)          (None, 2, 2, 384)         295296    \n",
      "                                                                 \n",
      " down2_ln (LayerNormalizati  (None, 2, 2, 384)         768       \n",
      " on)                                                             \n",
      "                                                                 \n",
      " stage3_block0 (ConvNeXtBlo  (None, 2, 2, 384)         1275744   \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " stage3_block1 (ConvNeXtBlo  (None, 2, 2, 384)         1275744   \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " stage3_block2 (ConvNeXtBlo  (None, 2, 2, 384)         1275744   \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " stage3_block3 (ConvNeXtBlo  (None, 2, 2, 384)         1275744   \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " stage3_block4 (ConvNeXtBlo  (None, 2, 2, 384)         1275744   \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " stage3_block5 (ConvNeXtBlo  (None, 2, 2, 384)         1275744   \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " head_ln (LayerNormalizatio  (None, 2, 2, 384)         768       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " gap (GlobalAveragePooling2  (None, 384)               0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " logits (Dense)              (None, 10)                3850      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10233298 (39.04 MB)\n",
      "Trainable params: 10233298 (39.04 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set steps_per_epoch explicitly (avoid Dataset materialization)\n",
    "batch_size = 128\n",
    "steps_per_epoch = math.ceil(50000 / batch_size)  # CIFAR-10 train size\n",
    "epochs = 150\n",
    "base_lr = 3e-3\n",
    "weight_decay=1e-3\n",
    "\n",
    "total_steps  = int(steps_per_epoch * epochs)\n",
    "warmup_steps = int(steps_per_epoch * 5)\n",
    "lr_sched = WarmupCosine(base_lr, warmup_steps, total_steps)\n",
    "# Faster on M1/M2 when TF falls back\n",
    "opt = tfa.optimizers.AdamW(\n",
    "    learning_rate=lr_sched, weight_decay=weight_decay,\n",
    "    beta_1=0.9, beta_2=0.999, epsilon=1e-8\n",
    ")\n",
    "\n",
    "model = build_convnext_small_cifar10(\n",
    "    num_classes=10,\n",
    "    depths=(3, 6, 6),      # total 15 blocks\n",
    "    dims=(96, 192, 384),   # widths\n",
    "    drop_path_rate=0.1,    # linear 0..0.1 across blocks\n",
    "    se_ratio=0.25          # set 0.0 to disable SE\n",
    ")\n",
    "\n",
    "# Recompile with the OBJECT, not a string\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"acc\")]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e79dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 09:02:47.861843: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - ETA: 0s - loss: 1.9872 - acc: 0.3092"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 09:04:41.344615: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 129s 297ms/step - loss: 1.9872 - acc: 0.3092 - val_loss: 1.8154 - val_acc: 0.4019\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 107s 274ms/step - loss: 1.7639 - acc: 0.4152 - val_loss: 1.6520 - val_acc: 0.4753\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 106s 270ms/step - loss: 1.7181 - acc: 0.4366 - val_loss: 1.6026 - val_acc: 0.4941\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 102s 262ms/step - loss: 1.6937 - acc: 0.4497 - val_loss: 1.6530 - val_acc: 0.4530\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 104s 265ms/step - loss: 1.6631 - acc: 0.4629 - val_loss: 1.6341 - val_acc: 0.4792\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 101s 258ms/step - loss: 1.6233 - acc: 0.4825 - val_loss: 1.5604 - val_acc: 0.5216\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 99s 253ms/step - loss: 1.5976 - acc: 0.4949 - val_loss: 1.5534 - val_acc: 0.5163\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 100s 257ms/step - loss: 1.5879 - acc: 0.5008 - val_loss: 1.5090 - val_acc: 0.5380\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 134s 342ms/step - loss: 1.5646 - acc: 0.5099 - val_loss: 1.4741 - val_acc: 0.5611\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 104s 266ms/step - loss: 1.5512 - acc: 0.5169 - val_loss: 1.4823 - val_acc: 0.5569\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 101s 257ms/step - loss: 1.5420 - acc: 0.5233 - val_loss: 1.4356 - val_acc: 0.5705\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 101s 259ms/step - loss: 1.5306 - acc: 0.5278 - val_loss: 1.4866 - val_acc: 0.5495\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 102s 262ms/step - loss: 1.5269 - acc: 0.5298 - val_loss: 1.4676 - val_acc: 0.5602\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 102s 260ms/step - loss: 1.5152 - acc: 0.5365 - val_loss: 1.4526 - val_acc: 0.5667\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 101s 259ms/step - loss: 1.5135 - acc: 0.5321 - val_loss: 1.4698 - val_acc: 0.5551\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 102s 261ms/step - loss: 1.5078 - acc: 0.5397 - val_loss: 1.4560 - val_acc: 0.5614\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 104s 265ms/step - loss: 1.5115 - acc: 0.5382 - val_loss: 1.4347 - val_acc: 0.5807\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 103s 264ms/step - loss: 1.5013 - acc: 0.5425 - val_loss: 1.4488 - val_acc: 0.5705\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 104s 264ms/step - loss: 1.4914 - acc: 0.5467 - val_loss: 1.4911 - val_acc: 0.5507\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 102s 261ms/step - loss: 1.5097 - acc: 0.5366 - val_loss: 1.4631 - val_acc: 0.5594\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 101s 258ms/step - loss: 1.4843 - acc: 0.5504 - val_loss: 1.4278 - val_acc: 0.5765\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 103s 262ms/step - loss: 1.4860 - acc: 0.5492 - val_loss: 1.3865 - val_acc: 0.6010\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 106s 272ms/step - loss: 2.2238 - acc: 0.1482 - val_loss: 2.3032 - val_acc: 0.1000\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 108s 277ms/step - loss: 2.3032 - acc: 0.1034 - val_loss: 2.3032 - val_acc: 0.1000\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 107s 273ms/step - loss: 2.3031 - acc: 0.1003 - val_loss: 2.3027 - val_acc: 0.1069\n",
      "Epoch 26/150\n",
      "  1/391 [..............................] - ETA: 2:44 - loss: 2.3024 - acc: 0.0781"
     ]
    }
   ],
   "source": [
    "history = model.fit(ds_train, validation_data=ds_test, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f94b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
